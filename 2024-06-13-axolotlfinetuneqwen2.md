---
layout: single
title: "axolotlå¾®è°ƒQwen2"
sidebar:
  nav: "docs"
date: 2024-06-13 00:00:00 +0800
categories: [AI-Agents, Fine-Tuning, LLM]
tags: [AI, AutoGen, Fine-Tuning, GPT, HuggingFace, LLM, Llama-3, Ollama, PyTorch, Qwen]
classes: wide
author_profile: true
---


#  axolotlå¾®è°ƒQwen2 

###  ollamaéƒ¨ç½²Qwen2 
    
    
```
    #ollamaä¸‹è½½åœ°å€https://www.ollama.com/
    ollama run qwen2:7b
    ollama run qwen2:72b
```
    
    ##ä¸ºollamaå¼€å¯å…¬ç½‘ip
    
    sudo mkdir -p /etc/systemd/system/ollama.service.d/
    
    sudo nano /etc/systemd/system/ollama.service.d/override.conf
    
    sudo nano /etc/systemd/system/ollama.service
    
```
    ##è¾“å…¥
    [Service]
    Environment="OLLAMA_HOST=0.0.0.0"
```
    
    #ç„¶åCtrl+O æŒ‰å›è½¦ å†æŒ‰Ctrl+X
    
    sudo systemctl daemon-reload
    
    sudo systemctl restart ollama.service
    
    åœ¨æµè§ˆå™¨è®¿é—® http://64.247.196.27:11434/v1

###  vLLMéƒ¨ç½²Qwen 7b 
    
    
```
    conda create -n myenv python=3.9 -y
    conda activate myenv
    pip install vllm
```
    
    
    
    #å¼€å¯API
    
    ##é€šç”¨æ¨¡å‹localhost
    python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2-7B --dtype auto --api-key token-abc123
    
    ##é€šç”¨æ¨¡å‹å…¬ç½‘è®¿é—®
    python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2-7B --dtype auto --api-key token-abc123 --host 0.0.0.0
    
```
    ##glm-4-9bçš„å…¬ç½‘è®¿é—® 
    ##éœ€è¦å°† trust_remote_code é€‰é¡¹è®¾ç½®ä¸º True
    python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2-7B --dtype auto --api-key token-abc123 --host 0.0.0.0 --trust-remote-code
```
    
```
    #æ¥å£ä¿¡æ¯
            {
                "model": "Qwen/Qwen2-7B",
                "base_url": "http://64.247.196.36:8000/v1",
                "api_key": "token-abc123",
            },
```
    

###  autogen studio å®‰è£…å’Œå¯åŠ¨ 
    
    
    pip install autogenstudio
    
    autogenstudio ui --port 8081 --host 0.0.

###  colabè„šæœ¬ [ https://colab.research.google.com/drive/1Vq_4i2AEaYqXVWdLt6HpGL-1hVdfWxQh#scrollTo=at5ei-2J8VZt ](<https://colab.research.google.com/drive/1Vq_4i2AEaYqXVWdLt6HpGL-1hVdfWxQh#scrollTo=at5ei-2J8VZt>)

### 

###  pdfå¤„ç† 

[ https://pypi.org/project/marker-pdf/ ](<https://pypi.org/project/marker-pdf/>)
    
    
     pip install marker-pdf 
     marker_single GPT.pdf ./folder --batch_multiplier 2 --max_pages 52 --langs English

###  é…ç½® 

Axolotl æ˜¯ä¸€ç§æ—¨åœ¨ç®€åŒ–å„ç§ AI æ¨¡å‹å¾®è°ƒçš„å·¥å…·ï¼Œä¸ºå¤šç§é…ç½®å’Œæ¶æ„æä¾›æ”¯æŒã€‚ 

Python >=3.10 and Pytorch >=2.1.1 
    
    
    # ä» GitHub å…‹éš† axolotl ä»“åº“
    git clone https://github.com/OpenAccess-AI-Collective/axolotl
    
    # åˆ‡æ¢åˆ° axolotl ç›®å½•
    cd axolotl
    
    # å°†å½“å‰ç”¨æˆ·æ·»åŠ åˆ° docker ç»„,ä»¥ä¾¿åœ¨æ²¡æœ‰ sudo çš„æƒ…å†µä¸‹è¿è¡Œ docker å‘½ä»¤
    sudo usermod -aG docker $USER
    
    # æ›´æ–°ç»„ä¿¡æ¯,ä½¿æ›´æ”¹ç”Ÿæ•ˆ
    newgrp docker
    
```
    # è¿è¡Œ axolotl docker å®¹å™¨,ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ GPU,å¹¶åœ¨é€€å‡ºæ—¶è‡ªåŠ¨åˆ é™¤å®¹å™¨
    # docker hub https://hub.docker.com/r/winglian/axolotl/tags
    docker run --gpus '"all"' --rm -it winglian/axolotl:main-latest
```
    
    
    # ä½¿ç”¨ Accelerate åº“å¯åŠ¨ axolotl è®­ç»ƒè„šæœ¬,ä½¿ç”¨ examples/openllama-3b/qlora.yml é…ç½®æ–‡ä»¶
    accelerate launch -m axolotl.cli.train examples/qwen2/qlora-fsdp.yaml

æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼š   
  


https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/qwen2/qlora-fsdp.yaml 

##  **ğŸ”¥** å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng 

##  **ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡**

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>) ** ****

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>) **

æ³¨æ„: 

é…ç½®æ–‡ä»¶ä¸­fp16è®¾ä¸ºfalse bfp16è®¾ä¸ºtrue 

å¦‚æœæ•°æ®é›†å†…å®¹å¤ªå°‘ï¼Œéœ€è¦å°† ` eval_sample_packing ` è®¾ç½®ä¸º ` False `

ä½¿ç”¨ Nano åœ¨ç»ˆç«¯ç¼–è¾‘ Qlora.yml è®­ç»ƒæ–‡ä»¶ 
    
    
```
    nano examples/qwen2/qlora-fsdp.yaml
    (Ctrl+o) = Save
    (Ctrl+x) = Exit
```
    

ä¸Šä¼ è‡³hugging face 
    
    
    pip install huggingface-cli
    
    huggingface-cli login
    
    huggingface-cli upload leo009/Qwen2-finetuned qlora-out
    

æ•°æ®é›† [ https://huggingface.co/datasets/tatsu-lab/alpaca ](<https://huggingface.co/datasets/tatsu-lab/alpaca>)

###  qlora-fsdp.yamlé…ç½®æ–‡ä»¶ 
    
    
```
    base_model: Qwen/Qwen2-7B
    trust_remote_code: true
    load_in_8bit: false
    load_in_4bit: true
    strict: false
    datasets:
      - path: tatsu-lab/alpaca
        type: alpaca
    dataset_prepared_path:
    val_set_size: 0.05
    output_dir: ./outputs/out
    sequence_len: 2048
    sample_packing: true
    eval_sample_packing: true
    pad_to_sequence_len: true
    adapter: qlora
    lora_model_dir:
    lora_r: 32
    lora_alpha: 64
    lora_dropout: 0.05
    lora_target_linear: true
    lora_fan_in_fan_out:
    wandb_project:
    wandb_entity:
    wandb_watch:
    wandb_name:
    wandb_log_model:
    gradient_accumulation_steps: 8
    micro_batch_size: 1
    num_epochs: 4
    optimizer: adamw_torch
    lr_scheduler: cosine
    learning_rate: 0.0002
    train_on_inputs: false
    group_by_length: false
    bf16: auto
    fp16:
    tf32: true
    gradient_checkpointing: false
    gradient_checkpointing_kwargs:
      use_reentrant: false
    early_stopping_patience:
    resume_from_checkpoint:
    local_rank:
    logging_steps: 1
    xformers_attention:
    flash_attention: false
    warmup_steps: 10
    evals_per_epoch: 4
    saves_per_epoch: 1
    debug:
    deepspeed:
    weight_decay: 0.0
    special_tokens:
```
