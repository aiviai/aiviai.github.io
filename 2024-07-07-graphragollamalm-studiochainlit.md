---
layout: single
title: "GraphRAG+ollama+LM Studio+chainlité«˜çº§ç”¨æ³•"
sidebar:
  nav: "docs"
date: 2024-07-07 00:00:00 +0800
categories: [AI-Agents, Fine-Tuning, LLM, RAG]
tags: [AI, AI-Agents, Chainlit, GPT, GraphRAG, LLM, Ollama, RAG]
classes: wide
author_profile: true
---


#  GraphRAG+ollama+LM Studio+chainlité«˜çº§ç”¨æ³• 

###  ollamaä¸‹è½½æ¨¡å‹ 
    
    
    ollama run gemma2:9b

###  graphragè®¾ç½® 
    
    
    pip install graphrag
    
    python -m graphrag.index --init --root ./ragpdf
    
    python -m graphrag.index --root ./ragpdf
    

###  æœ¬åœ°æ¨¡å‹é…ç½® 
    
    
```
    encoding_model: cl100k_base
    skip_workflows: []
    llm:
      api_key: ollama
      type: openai_chat # or azure_openai_chat
      model: gemma2:latest
      model_supports_json: true # recommended if this is available for your model.
      api_base: http://localhost:11434/v1
```
    
    
```
    embeddings:
      ## parallelization: override the global parallelization settings for embeddings
      async_mode: threaded # or asyncio
      llm:
        api_key: lm-studio
        type: openai_embedding # or azure_openai_embedding
        model: nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q5_K_M.gguf
        api_base:  http://localhost:1234/v1
```

###  pdfè½¬markdownï¼Œmarkdownè½¬txt 
    
    
    #æµ‹è¯•æ–‡æ¡£ https://github.com/win4r/mytest/blob/main/book.pdf
    
    pip install marker-pdf
    
    marker_single ./book.pdf ./pdftxt --batch_multiplier 2 --max_pages 60 --langs English
    
    #markdownè½¬txt
    python markdown_to_text.py book.md book.txt
    

##  **ğŸ‘‰ğŸ‘‰ğŸ‘‰å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng**

##  **ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡**

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰æˆ‘çš„å¼€æºé¡¹ç›®** **[ https://github.com/win4r/AISuperDomain ](<https://github.com/win4r/AISuperDomain>) **

###  è®¾ç½®API keyå’Œæ¨¡å‹åç§° 
    
    
```
    export GRAPHRAG_API_KEY="sk-xggd2443fg"
    export GRAPHRAG_LLM_MODEL="gpt-3.5-turbo"
    export GRAPHRAG_EMBEDDING_MODEL="text-embedding-ada-002"
```
    

###  global search 
    
    
```
    import os
    import asyncio
    import pandas as pd
    import tiktoken
```
    
```
    from graphrag.query.indexer_adapters import read_indexer_entities, read_indexer_reports
    from graphrag.query.llm.oai.chat_openai import ChatOpenAI
    from graphrag.query.llm.oai.typing import OpenaiApiType
    from graphrag.query.structured_search.global_search.community_context import (
        GlobalCommunityContext,
    )
    from graphrag.query.structured_search.global_search.search import GlobalSearch
```
    
```
    async def main():
        # è®¾ç½®è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰
        # ä»ç¯å¢ƒå˜é‡ä¸­è·å–APIå¯†é’¥å’Œæ¨¡å‹åç§°
        api_key = os.environ.get("GRAPHRAG_API_KEY")
        llm_model = os.environ.get("GRAPHRAG_LLM_MODEL")
```
    
```
        # åˆå§‹åŒ–ChatOpenAIå®ä¾‹
        llm = ChatOpenAI(
            api_key=api_key,
            model=llm_model,
            api_type=OpenaiApiType.OpenAI,  # ä½¿ç”¨OpenAI API
            max_retries=20,  # æœ€å¤§é‡è¯•æ¬¡æ•°
        )
```
    
        # åˆå§‹åŒ–tokenç¼–ç å™¨
        token_encoder = tiktoken.get_encoding("cl100k_base")
    
```
        # åŠ è½½ç¤¾åŒºæŠ¥å‘Šä½œä¸ºå…¨å±€æœç´¢çš„ä¸Šä¸‹æ–‡
        INPUT_DIR = "./inputs/operation dulce"  # è¾“å…¥ç›®å½•
        COMMUNITY_REPORT_TABLE = "create_final_community_reports"  # ç¤¾åŒºæŠ¥å‘Šè¡¨å
        ENTITY_TABLE = "create_final_nodes"  # å®ä½“è¡¨å
        ENTITY_EMBEDDING_TABLE = "create_final_entities"  # å®ä½“åµŒå…¥è¡¨å
```
    
```
        COMMUNITY_LEVEL = 2  # Leidenç¤¾åŒºå±‚æ¬¡ç»“æ„ä¸­çš„ç¤¾åŒºçº§åˆ«
        # è¯»å–parquetæ–‡ä»¶
        entity_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_TABLE}.parquet")
        report_df = pd.read_parquet(f"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet")
        entity_embedding_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet")
```
    
```
        # è¯»å–ç´¢å¼•å™¨æŠ¥å‘Šå’Œå®ä½“
        reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)
        entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)
        print(f"æŠ¥å‘Šè®°å½•æ•°: {len(report_df)}")
        print(report_df.head())
```
    
```
        # åŸºäºç¤¾åŒºæŠ¥å‘Šæ„å»ºå…¨å±€ä¸Šä¸‹æ–‡
        context_builder = GlobalCommunityContext(
            community_reports=reports,
            entities=entities,  # ç”¨äºè®¡ç®—ä¸Šä¸‹æ–‡æ’åºçš„ç¤¾åŒºæƒé‡
            token_encoder=token_encoder,
        )
```
    
```
        # æ‰§è¡Œå…¨å±€æœç´¢
        # è®¾ç½®ä¸Šä¸‹æ–‡æ„å»ºå™¨å‚æ•°
        context_builder_params = {
            "use_community_summary": False,  # ä½¿ç”¨å®Œæ•´çš„ç¤¾åŒºæŠ¥å‘Šï¼Œè€Œä¸æ˜¯æ‘˜è¦
            "shuffle_data": True,  # éšæœºæ‰“ä¹±æ•°æ®
            "include_community_rank": True,  # åŒ…æ‹¬ç¤¾åŒºæ’å
            "min_community_rank": 0,  # æœ€å°ç¤¾åŒºæ’å
            "community_rank_name": "rank",  # ç¤¾åŒºæ’åçš„åç§°
            "include_community_weight": True,  # åŒ…æ‹¬ç¤¾åŒºæƒé‡
            "community_weight_name": "occurrence weight",  # ç¤¾åŒºæƒé‡çš„åç§°
            "normalize_community_weight": True,  # æ ‡å‡†åŒ–ç¤¾åŒºæƒé‡
            "max_tokens": 12_000,  # æœ€å¤§tokenæ•°
            "context_name": "Reports",  # ä¸Šä¸‹æ–‡åç§°
        }
```
    
```
        # è®¾ç½®map LLMå‚æ•°
        map_llm_params = {
            "max_tokens": 1000,  # æœ€å¤§ç”Ÿæˆtokenæ•°
            "temperature": 0.0,  # æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§
            "response_format": {"type": "json_object"},  # å“åº”æ ¼å¼ä¸ºJSONå¯¹è±¡
        }
```
    
```
        # è®¾ç½®reduce LLMå‚æ•°
        reduce_llm_params = {
            "max_tokens": 2000,  # æœ€å¤§ç”Ÿæˆtokenæ•°
            "temperature": 0.0,  # æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§
        }
```
    
```
        # åˆå§‹åŒ–å…¨å±€æœç´¢å¼•æ“
        search_engine = GlobalSearch(
            llm=llm,
            context_builder=context_builder,
            token_encoder=token_encoder,
            max_data_tokens=12_000,  # æœ€å¤§æ•°æ®tokenæ•°
            map_llm_params=map_llm_params,
            reduce_llm_params=reduce_llm_params,
            allow_general_knowledge=False,  # ä¸å…è®¸ä½¿ç”¨é€šç”¨çŸ¥è¯†
            json_mode=True,  # ä½¿ç”¨JSONæ¨¡å¼
            context_builder_params=context_builder_params,
            concurrent_coroutines=32,  # å¹¶å‘åç¨‹æ•°
            response_type="multiple paragraphs",  # å“åº”ç±»å‹ä¸ºå¤šä¸ªæ®µè½
        )
```
    
```
        # æ‰§è¡Œå¼‚æ­¥æœç´¢
        result = await search_engine.asearch(
            "è¿™ä¸ªæ•…äº‹ä¸­çš„ä¸»è¦å†²çªæ˜¯ä»€ä¹ˆï¼Œè°æ˜¯ä¸»è§’å’Œåæ´¾ï¼Ÿ"
        )
```
    
```
        # æ‰“å°æœç´¢ç»“æœ
        print(result.response)
        print("ä¸Šä¸‹æ–‡æ•°æ®æŠ¥å‘Š:")
        print(result.context_data["reports"])
        print(f"LLMè°ƒç”¨æ¬¡æ•°: {result.llm_calls}. LLM tokensæ•°: {result.prompt_tokens}")
```
    
    if __name__ == "__main__":
        asyncio.run(main())  # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°

###  local search 
    
    
```
    import os
    import asyncio
    import pandas as pd
    import tiktoken
```
    
```
    from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
    from graphrag.query.indexer_adapters import (
        read_indexer_covariates,
        read_indexer_entities,
        read_indexer_relationships,
        read_indexer_reports,
        read_indexer_text_units,
    )
    from graphrag.query.input.loaders.dfs import store_entity_semantic_embeddings
    from graphrag.query.llm.oai.chat_openai import ChatOpenAI
    from graphrag.query.llm.oai.embedding import OpenAIEmbedding
    from graphrag.query.llm.oai.typing import OpenaiApiType
    from graphrag.query.question_gen.local_gen import LocalQuestionGen
    from graphrag.query.structured_search.local_search.mixed_context import LocalSearchMixedContext
    from graphrag.query.structured_search.local_search.search import LocalSearch
    from graphrag.vector_stores.lancedb import LanceDBVectorStore
```
    
```
    async def main():
        # è®¾ç½®è¾“å…¥ç›®å½•å’Œæ•°æ®è¡¨å
        INPUT_DIR = "./inputs/operation dulce"
        LANCEDB_URI = f"{INPUT_DIR}/lancedb"
        COMMUNITY_REPORT_TABLE = "create_final_community_reports"
        ENTITY_TABLE = "create_final_nodes"
        ENTITY_EMBEDDING_TABLE = "create_final_entities"
        RELATIONSHIP_TABLE = "create_final_relationships"
        COVARIATE_TABLE = "create_final_covariates"
        TEXT_UNIT_TABLE = "create_final_text_units"
        COMMUNITY_LEVEL = 2
```
    
```
        # è¯»å–å®ä½“æ•°æ®
        entity_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_TABLE}.parquet")
        entity_embedding_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet")
        entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)
```
    
```
        # è®¾ç½®å’ŒåŠ è½½å®ä½“æè¿°åµŒå…¥
        description_embedding_store = LanceDBVectorStore(collection_name="entity_description_embeddings")
        description_embedding_store.connect(db_uri=LANCEDB_URI)
        store_entity_semantic_embeddings(entities=entities, vectorstore=description_embedding_store)
```
    
```
        # è¯»å–å…³ç³»æ•°æ®
        relationship_df = pd.read_parquet(f"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet")
        relationships = read_indexer_relationships(relationship_df)
```
    
```
        # è¯»å–åå˜é‡æ•°æ®
        covariate_df = pd.read_parquet(f"{INPUT_DIR}/{COVARIATE_TABLE}.parquet")
        claims = read_indexer_covariates(covariate_df)
        covariates = {"claims": claims}
```
    
```
        # è¯»å–ç¤¾åŒºæŠ¥å‘Šæ•°æ®
        report_df = pd.read_parquet(f"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet")
        reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)
```
    
```
        # è¯»å–æ–‡æœ¬å•å…ƒæ•°æ®
        text_unit_df = pd.read_parquet(f"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet")
        text_units = read_indexer_text_units(text_unit_df)
```
    
```
        # è®¾ç½®LLMå’ŒåµŒå…¥æ¨¡å‹
        api_key = os.environ["GRAPHRAG_API_KEY"]
        llm_model = os.environ["GRAPHRAG_LLM_MODEL"]
        embedding_model = os.environ["GRAPHRAG_EMBEDDING_MODEL"]
```
    
```
        llm = ChatOpenAI(
            api_key=api_key,
            model=llm_model,
            api_type=OpenaiApiType.OpenAI,
            max_retries=20,
        )
```
    
        token_encoder = tiktoken.get_encoding("cl100k_base")
    
```
        text_embedder = OpenAIEmbedding(
            api_key=api_key,
            api_base=None,
            api_type=OpenaiApiType.OpenAI,
            model=embedding_model,
            deployment_name=embedding_model,
            max_retries=20,
        )
```
    
```
        # åˆ›å»ºæœ¬åœ°æœç´¢ä¸Šä¸‹æ–‡æ„å»ºå™¨
        context_builder = LocalSearchMixedContext(
            community_reports=reports,
            text_units=text_units,
            entities=entities,
            relationships=relationships,
            covariates=covariates,
            entity_text_embeddings=description_embedding_store,
            embedding_vectorstore_key=EntityVectorStoreKey.ID,
            text_embedder=text_embedder,
            token_encoder=token_encoder,
        )
```
    
```
        # è®¾ç½®æœ¬åœ°æœç´¢å‚æ•°
        local_context_params = {
            "text_unit_prop": 0.5,
            "community_prop": 0.1,
            "conversation_history_max_turns": 5,
            "conversation_history_user_turns_only": True,
            "top_k_mapped_entities": 10,
            "top_k_relationships": 10,
            "include_entity_rank": True,
            "include_relationship_weight": True,
            "include_community_rank": False,
            "return_candidate_context": False,
            "embedding_vectorstore_key": EntityVectorStoreKey.ID,
            "max_tokens": 12_000,
        }
```
    
```
        llm_params = {
            "max_tokens": 2_000,
            "temperature": 0.0,
        }
```
    
```
        # åˆ›å»ºæœ¬åœ°æœç´¢å¼•æ“
        search_engine = LocalSearch(
            llm=llm,
            context_builder=context_builder,
            token_encoder=token_encoder,
            llm_params=llm_params,
            context_builder_params=local_context_params,
            response_type="multiple paragraphs",
        )
```
    
```
        # è¿è¡Œæœ¬åœ°æœç´¢ç¤ºä¾‹
        result = await search_engine.asearch("Tell me about Agent Mercer")
        print(result.response)
```
    
        result = await search_engine.asearch("Tell me about Dr. Jordan Hayes")
        print(result.response)
    
```
        # åˆ›å»ºé—®é¢˜ç”Ÿæˆå™¨
        question_generator = LocalQuestionGen(
            llm=llm,
            context_builder=context_builder,
            token_encoder=token_encoder,
            llm_params=llm_params,
            context_builder_params=local_context_params,
        )
```
    
```
        # ç”Ÿæˆå€™é€‰é—®é¢˜
        question_history = [
            "Tell me about Agent Mercer",
            "What happens in Dulce military base?",
        ]
        candidate_questions = await question_generator.agenerate(
            question_history=question_history, context_data=None, question_count=5
        )
        print(candidate_questions.response)
```
    
    if __name__ == "__main__":
        asyncio.run(main())

###  global-ui 
    
    
```
    import os
    import asyncio
    import pandas as pd
    import tiktoken
    import chainlit as cl
```
    
```
    from graphrag.query.indexer_adapters import read_indexer_entities, read_indexer_reports
    from graphrag.query.llm.oai.chat_openai import ChatOpenAI
    from graphrag.query.llm.oai.typing import OpenaiApiType
    from graphrag.query.structured_search.global_search.community_context import GlobalCommunityContext
    from graphrag.query.structured_search.global_search.search import GlobalSearch
```
    
    # å…¨å±€å˜é‡
    search_engine = None
    
    
```
    @cl.on_chat_start
    async def on_chat_start():
        global search_engine
```
    
```
        # ä»ç¯å¢ƒå˜é‡ä¸­è·å–APIå¯†é’¥å’Œæ¨¡å‹åç§°
        api_key = os.environ.get("GRAPHRAG_API_KEY")
        if not api_key:
            await cl.Message(content="é”™è¯¯ï¼šGRAPHRAG_API_KEY æœªè®¾ç½®").send()
            raise ValueError("GRAPHRAG_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
```
    
        llm_model = os.environ.get("GRAPHRAG_LLM_MODEL", "gpt-3.5-turbo")
    
```
        # åˆå§‹åŒ–ChatOpenAIå®ä¾‹
        llm = ChatOpenAI(
            api_key=api_key,
            model=llm_model,
            api_type=OpenaiApiType.OpenAI,
            max_retries=20,
        )
```
    
        # åˆå§‹åŒ–tokenç¼–ç å™¨
        token_encoder = tiktoken.get_encoding("cl100k_base")
    
```
        # åŠ è½½ç¤¾åŒºæŠ¥å‘Šä½œä¸ºå…¨å±€æœç´¢çš„ä¸Šä¸‹æ–‡
        INPUT_DIR = "./inputs/operation dulce"
        COMMUNITY_REPORT_TABLE = "create_final_community_reports"
        ENTITY_TABLE = "create_final_nodes"
        ENTITY_EMBEDDING_TABLE = "create_final_entities"
        COMMUNITY_LEVEL = 2
```
    
```
        try:
            # è¯»å–parquetæ–‡ä»¶
            entity_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_TABLE}.parquet")
            report_df = pd.read_parquet(f"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet")
            entity_embedding_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet")
        except FileNotFoundError as e:
            await cl.Message(content=f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ‰€éœ€çš„parquetæ–‡ä»¶ã€‚{str(e)}").send()
            return
```
    
```
        # è¯»å–ç´¢å¼•å™¨æŠ¥å‘Šå’Œå®ä½“
        reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)
        entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)
        await cl.Message(content=f"æŠ¥å‘Šè®°å½•æ•°: {len(report_df)}").send()
```
    
```
        # åŸºäºç¤¾åŒºæŠ¥å‘Šæ„å»ºå…¨å±€ä¸Šä¸‹æ–‡
        context_builder = GlobalCommunityContext(
            community_reports=reports,
            entities=entities,
            token_encoder=token_encoder,
        )
```
    
```
        # è®¾ç½®ä¸Šä¸‹æ–‡æ„å»ºå™¨å‚æ•°
        context_builder_params = {
            "use_community_summary": False,
            "shuffle_data": True,
            "include_community_rank": True,
            "min_community_rank": 0,
            "community_rank_name": "rank",
            "include_community_weight": True,
            "community_weight_name": "occurrence weight",
            "normalize_community_weight": True,
            "max_tokens": 12_000,
            "context_name": "Reports",
        }
```
    
```
        # è®¾ç½®map LLMå‚æ•°
        map_llm_params = {
            "max_tokens": 1000,
            "temperature": 0.0,
            "response_format": {"type": "json_object"},
        }
```
    
```
        # è®¾ç½®reduce LLMå‚æ•°
        reduce_llm_params = {
            "max_tokens": 2000,
            "temperature": 0.0,
        }
```
    
```
        # åˆå§‹åŒ–å…¨å±€æœç´¢å¼•æ“
        search_engine = GlobalSearch(
            llm=llm,
            context_builder=context_builder,
            token_encoder=token_encoder,
            max_data_tokens=12_000,
            map_llm_params=map_llm_params,
            reduce_llm_params=reduce_llm_params,
            allow_general_knowledge=False,
            json_mode=True,
            context_builder_params=context_builder_params,
            concurrent_coroutines=32,
            response_type="multiple paragraphs",
        )
```
    
        await cl.Message(content="å…¨å±€æœç´¢ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼Œè¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢ã€‚").send()
    
    
```
    @cl.on_message
    async def main(message: cl.Message):
        global search_engine
```
    
```
        if search_engine is None:
            await cl.Message(content="æœç´¢å¼•æ“å°šæœªåˆå§‹åŒ–ã€‚è¯·ç¨åå†è¯•ã€‚").send()
            return
```
    
        query = message.content
        result = await search_engine.asearch(query)
    
        # å‘é€æœç´¢ç»“æœ
        await cl.Message(content=result.response).send()
    
```
        # å‘é€ä¸Šä¸‹æ–‡æ•°æ®æŠ¥å‘Š
        context_data = f"ä¸Šä¸‹æ–‡æ•°æ®æŠ¥å‘Šæ•°é‡: {len(result.context_data['reports'])}"
        await cl.Message(content=context_data).send()
```
    
```
        # å‘é€LLMè°ƒç”¨ä¿¡æ¯
        llm_info = f"LLMè°ƒç”¨æ¬¡æ•°: {result.llm_calls}. LLM tokensæ•°: {result.prompt_tokens}"
        await cl.Message(content=llm_info).send()
```
    
    
    if __name__ == "__main__":
        cl.run()

###  local-ui 
    
    
```
    import os
    import asyncio
    import pandas as pd
    import tiktoken
    import chainlit as cl
```
    
```
    from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
    from graphrag.query.indexer_adapters import (
        read_indexer_covariates,
        read_indexer_entities,
        read_indexer_relationships,
        read_indexer_reports,
        read_indexer_text_units,
    )
    from graphrag.query.input.loaders.dfs import store_entity_semantic_embeddings
    from graphrag.query.llm.oai.chat_openai import ChatOpenAI
    from graphrag.query.llm.oai.embedding import OpenAIEmbedding
    from graphrag.query.llm.oai.typing import OpenaiApiType
    from graphrag.query.question_gen.local_gen import LocalQuestionGen
    from graphrag.query.structured_search.local_search.mixed_context import LocalSearchMixedContext
    from graphrag.query.structured_search.local_search.search import LocalSearch
    from graphrag.vector_stores.lancedb import LanceDBVectorStore
```
    
```
    # å…¨å±€å˜é‡
    search_engine = None
    question_generator = None
    question_history = []
```
    
    
```
    @cl.on_chat_start
    async def on_chat_start():
        global search_engine, question_generator
```
    
```
        try:
            # è®¾ç½®è¾“å…¥ç›®å½•å’Œæ•°æ®è¡¨å
            INPUT_DIR = "./inputs/operation dulce"
            LANCEDB_URI = f"{INPUT_DIR}/lancedb"
            COMMUNITY_REPORT_TABLE = "create_final_community_reports"
            ENTITY_TABLE = "create_final_nodes"
            ENTITY_EMBEDDING_TABLE = "create_final_entities"
            RELATIONSHIP_TABLE = "create_final_relationships"
            COVARIATE_TABLE = "create_final_covariates"
            TEXT_UNIT_TABLE = "create_final_text_units"
            COMMUNITY_LEVEL = 2
```
    
```
            # è¯»å–å®ä½“æ•°æ®
            entity_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_TABLE}.parquet")
            entity_embedding_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet")
            entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)
```
    
```
            # è®¾ç½®å’ŒåŠ è½½å®ä½“æè¿°åµŒå…¥
            description_embedding_store = LanceDBVectorStore(collection_name="entity_description_embeddings")
            description_embedding_store.connect(db_uri=LANCEDB_URI)
            store_entity_semantic_embeddings(entities=entities, vectorstore=description_embedding_store)
```
    
```
            # è¯»å–å…³ç³»æ•°æ®
            relationship_df = pd.read_parquet(f"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet")
            relationships = read_indexer_relationships(relationship_df)
```
    
```
            # è¯»å–åå˜é‡æ•°æ®
            covariate_df = pd.read_parquet(f"{INPUT_DIR}/{COVARIATE_TABLE}.parquet")
            claims = read_indexer_covariates(covariate_df)
            covariates = {"claims": claims}
```
    
```
            # è¯»å–ç¤¾åŒºæŠ¥å‘Šæ•°æ®
            report_df = pd.read_parquet(f"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet")
            reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)
```
    
```
            # è¯»å–æ–‡æœ¬å•å…ƒæ•°æ®
            text_unit_df = pd.read_parquet(f"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet")
            text_units = read_indexer_text_units(text_unit_df)
```
    
```
            # è®¾ç½®LLMå’ŒåµŒå…¥æ¨¡å‹
            api_key = os.environ.get("GRAPHRAG_API_KEY")
            if not api_key:
                raise ValueError("GRAPHRAG_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
            llm_model = os.environ.get("GRAPHRAG_LLM_MODEL", "gpt-3.5-turbo")
            embedding_model = os.environ.get("GRAPHRAG_EMBEDDING_MODEL", "text-embedding-3-small")
```
    
```
            llm = ChatOpenAI(
                api_key=api_key,
                model=llm_model,
                api_type=OpenaiApiType.OpenAI,
                max_retries=20,
            )
```
    
            token_encoder = tiktoken.get_encoding("cl100k_base")
    
```
            text_embedder = OpenAIEmbedding(
                api_key=api_key,
                api_base=None,
                api_type=OpenaiApiType.OpenAI,
                model=embedding_model,
                deployment_name=embedding_model,
                max_retries=20,
            )
```
    
```
            # åˆ›å»ºæœ¬åœ°æœç´¢ä¸Šä¸‹æ–‡æ„å»ºå™¨
            context_builder = LocalSearchMixedContext(
                community_reports=reports,
                text_units=text_units,
                entities=entities,
                relationships=relationships,
                covariates=covariates,
                entity_text_embeddings=description_embedding_store,
                embedding_vectorstore_key=EntityVectorStoreKey.ID,
                text_embedder=text_embedder,
                token_encoder=token_encoder,
            )
```
    
```
            # è®¾ç½®æœ¬åœ°æœç´¢å‚æ•°
            local_context_params = {
                "text_unit_prop": 0.5,
                "community_prop": 0.1,
                "conversation_history_max_turns": 5,
                "conversation_history_user_turns_only": True,
                "top_k_mapped_entities": 10,
                "top_k_relationships": 10,
                "include_entity_rank": True,
                "include_relationship_weight": True,
                "include_community_rank": False,
                "return_candidate_context": False,
                "embedding_vectorstore_key": EntityVectorStoreKey.ID,
                "max_tokens": 12_000,
            }
```
    
```
            llm_params = {
                "max_tokens": 2_000,
                "temperature": 0.0,
            }
```
    
```
            # åˆ›å»ºæœ¬åœ°æœç´¢å¼•æ“
            search_engine = LocalSearch(
                llm=llm,
                context_builder=context_builder,
                token_encoder=token_encoder,
                llm_params=llm_params,
                context_builder_params=local_context_params,
                response_type="multiple paragraphs",
            )
```
    
```
            # åˆ›å»ºé—®é¢˜ç”Ÿæˆå™¨
            question_generator = LocalQuestionGen(
                llm=llm,
                context_builder=context_builder,
                token_encoder=token_encoder,
                llm_params=llm_params,
                context_builder_params=local_context_params,
            )
```
    
            await cl.Message(
                content="æœ¬åœ°æœç´¢ç³»ç»Ÿå’Œé—®é¢˜ç”Ÿæˆå™¨å·²å‡†å¤‡å°±ç»ªã€‚æ‚¨å¯ä»¥å¼€å§‹æé—®ï¼Œæˆ–è¾“å…¥ '/generate' æ¥ç”Ÿæˆæ–°çš„é—®é¢˜ã€‚").send()
    
        except Exception as e:
            await cl.Message(content=f"åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}").send()
    
    
```
    @cl.on_message
    async def main(message: cl.Message):
        global search_engine, question_generator, question_history
```
    
```
        if search_engine is None or question_generator is None:
            await cl.Message(content="ç³»ç»Ÿå°šæœªå®Œå…¨åˆå§‹åŒ–ï¼Œè¯·ç¨åå†è¯•ã€‚").send()
            return
```
    
```
        try:
            if message.content.strip().lower() == "/generate":
                # ç”Ÿæˆæ–°é—®é¢˜
                await cl.Message(content="æ­£åœ¨ç”Ÿæˆé—®é¢˜ï¼Œè¯·ç¨å€™...").send()
                candidate_questions = await question_generator.agenerate(
                    question_history=question_history, context_data=None, question_count=5
                )
                if isinstance(candidate_questions.response, list):
                    questions_text = "\n".join([f"{i + 1}. {q}" for i, q in enumerate(candidate_questions.response)])
                else:
                    questions_text = candidate_questions.response
                await cl.Message(content=f"ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®çš„é—®é¢˜ï¼š\n{questions_text}").send()
            else:
                # æ‰§è¡Œæœç´¢
                await cl.Message(content="æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨å€™...").send()
                question_history.append(message.content)
                result = await search_engine.asearch(message.content)
```
    
                await cl.Message(content=result.response).send()
    
                context_data = f"ä¸Šä¸‹æ–‡æ•°æ®æŠ¥å‘Šæ•°é‡: {len(result.context_data['reports'])}"
                await cl.Message(content=context_data).send()
    
```
                llm_info = f"LLMè°ƒç”¨æ¬¡æ•°: {result.llm_calls}. LLM tokensæ•°: {result.prompt_tokens}"
                await cl.Message(content=llm_info).send()
        except Exception as e:
            error_message = f"å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            await cl.Message(content=error_message).send()
            print(f"Error in main function: {str(e)}")  # ä¸ºäº†è°ƒè¯•ç›®çš„ï¼Œåœ¨æ§åˆ¶å°æ‰“å°é”™è¯¯
```
    
    
    if __name__ == "__main__":
        cl.run()

###  markdown_to_text.py 
    
    
    ##è¿è¡Œæ–¹å¼ python markdown_to_text.py book.md book.txt
    
```
    import markdown
    from bs4 import BeautifulSoup
    import re
    import argparse
```
    
    
```
    def markdown_to_text(markdown_content):
        # Convert Markdown to HTML
        html = markdown.markdown(markdown_content)
```
    
```
        # Use BeautifulSoup to parse HTML and extract text
        soup = BeautifulSoup(html, 'html.parser')
        text = soup.get_text(separator='\n\n')
```
    
```
        # Additional cleaning
        text = re.sub(r'\n{3,}', '\n\n', text)  # Replace multiple newlines with double newlines
        text = text.strip()  # Remove leading/trailing whitespace
```
    
        return text
    
    
```
    def convert_file(input_file, output_file):
        # Read the Markdown file
        with open(input_file, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
```
    
        # Convert to plain text
        plain_text = markdown_to_text(markdown_content)
    
```
        # Write the plain text to the output file
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(plain_text)
```
    
    
```
    if __name__ == "__main__":
        parser = argparse.ArgumentParser(description="Convert Markdown file to plain text")
        parser.add_argument("input_file", help="Path to the input Markdown file")
        parser.add_argument("output_file", help="Path to the output plain text file")
        args = parser.parse_args()
```
    
        convert_file(args.input_file, args.output_file)
        print(f"Conversion complete. Plain text saved to {args.output_file}")
