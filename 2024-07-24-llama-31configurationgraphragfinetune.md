---
layout: single
title: "Llama 3.1æœ¬åœ°é…ç½®+GraphRAG+å¾®è°ƒ"
sidebar:
  nav: "docs"
date: 2024-07-24 00:00:00 +0800
categories: [Fine-Tuning, LLM, RAG]
tags: [AI, GraphRAG, HuggingFace, LLM, Llama-3, Ollama, RAG]
classes: wide
author_profile: true
---


#  Llama 3.1æœ¬åœ°é…ç½®+GraphRAG+å¾®è°ƒ 

###  å¾®è°ƒä»£ç  [ https://colab.research.google.com/drive/1vi6vLcjR_lMtZzRIWzIsa4aLWa35C_8s?usp=sharing ](<https://colab.research.google.com/drive/1vi6vLcjR_lMtZzRIWzIsa4aLWa35C_8s?usp=sharing>)

###  ä¸‹è½½åœ°å€ 

[ https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct ](<https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct>)

[ https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct ](<https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct>)

###  ç®—æ³•æµ‹è¯• 

[ https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iv/description/ ](<https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iv/description/>)

[ https://leetcode.com/problems/candy/description/ ](<https://leetcode.com/problems/candy/description/>)

[ https://leetcode.com/problems/number-of-subarrays-with-and-value-of-k/description/ ](<https://leetcode.com/problems/number-of-subarrays-with-and-value-of-k/description/>)

###  æ¨ç†é¢˜ 
    
    
```
    åœ¨ä¸€ä¸ªç«‹æ–¹ä½“æˆ¿é—´é‡Œ,å››é¢å¢™å£åˆ†åˆ«æœå‘ä¸œå—è¥¿åŒ—,æ¯é¢å¢™ä¸Šéƒ½æœ‰ä¸€æ‰‡é—¨ã€‚
    æˆ¿é—´ä¸­å¤®æœ‰ä¸€ä¸ªæ­£æ–¹å½¢è½¬ç›˜,è½¬ç›˜çš„å››ä¸ªè§’æ”¾ç½®ç€4ä¸ªä¸åŒé¢œè‰²çš„ç«‹æ–¹ä½“ç§¯æœ¨:çº¢ã€è“ã€ç»¿ã€é»„ã€‚
    æ­¤æ—¶çº¢è‰²ç«‹æ–¹ä½“å¯¹ç€ä¸œé—¨ï¼Œè“è‰²ç«‹æ–¹ä½“å¯¹ç€å—é—¨ï¼Œç»¿è‰²ç«‹æ–¹ä½“å¯¹ç€è¥¿é—¨ï¼Œé»„è‰²ç«‹æ–¹ä½“å¯¹ç€åŒ—é—¨ã€‚
    ç°åœ¨å¼€å§‹è®¡æ—¶ï¼Œæ­£æ–¹å½¢è½¬ç›˜å¼€å§‹è½¬åŠ¨ã€‚
    æ¯è¿‡äº”åˆ†é’Ÿï¼Œæ­£æ–¹å½¢è½¬ç›˜è½¬åŠ¨ä¸€å‘¨ã€‚
    å½“æ­£æ–¹å½¢è½¬ç›˜è½¬åŠ¨ä¸€å‘¨æ—¶ï¼Œçº¢è‰²ç«‹æ–¹ä½“å’Œç»¿è‰²ç«‹æ–¹ä½“äº¤æ¢ä½ç½®ï¼Œè“è‰²ç«‹æ–¹ä½“å’Œé»„è‰²ç«‹æ–¹ä½“äº¤æ¢ä½ç½®ã€‚
    æ¯è¿‡15åˆ†é’Ÿï¼Œæ—¶é—´å°±ä¼šå€’æµ5åˆ†é’Ÿã€‚
    å°±åœ¨è®¡æ—¶åˆ°äº†ç¬¬ä¸‰åäº”åˆ†é’Ÿæ—¶ï¼Œçº¢ã€è“ã€ç»¿ã€é»„ç«‹æ–¹ä½“åˆ†åˆ«å¯¹åº”å“ªä¸ªæ–¹å‘çš„é—¨ï¼Ÿ
```
    
```
    ç­”æ¡ˆï¼š
    çº¢->è¥¿
    è“->åŒ—
    ç»¿->ä¸œ
    é»„->å—
```
    
    
    

###  ollamaè¿è¡Œllama3.1 
    
    
```
    ollama run llama3.1
    ollama run llama3.1:70b
    ollama run llama3.1:405b
```

###  vllmæ¨ç†llama3.1 
    
    
    #http://localhost:8000/v1/
    conda create -n myenv python=3.10 -y
    
    conda activate myenv
    
    
    python -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3.1-70B-Instruct --dtype auto --api-key token-abc123 --host 0.0.0.0 --trust-remote-code

##  **ğŸ‘‰ğŸ‘‰ğŸ‘‰å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng**

##  **ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡**

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰æˆ‘çš„å¼€æºé¡¹ç›®** **[ https://github.com/win4r/AISuperDomain ](<https://github.com/win4r/AISuperDomain>) **

##  APIè°ƒç”¨ 
    
    
    # https://docs.together.ai/docs/quickstart
    
```
    #together
    import os
    from together import Together
```
    
    client = Together(api_key=os.environ.get("TOGETHER_API_KEY"))
    
```
    stream = client.chat.completions.create(
      model="meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
      messages=[{"role": "user", "content": "é²è¿…å’Œå‘¨æ ‘äººæ˜¯ä»€ä¹ˆå…³ç³»?"}],
      stream=True,
    )
```
    
    for chunk in stream:
      print(chunk.choices[0].delta.content or "", end="", flush=True)
    
    
    #------------------------------------------------------------
    
    #groq
    import os
    
    from groq import Groq
    
```
    client = Groq(
        api_key=os.environ.get("GROQ_API_KEY"),
    )
```
    
```
    chat_completion = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": "é²è¿…å’Œå‘¨æ ‘äººæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ",
            }
        ],
        model="llama-3.1-70b-versatile",
    )
```
    
    print(chat_completion.choices[0].message.content)
    
    
    

##  GraphRAGå¯¹æ¥llama 3.1 

###  å‘½ä»¤ 
    
    
    python -m graphrag.index --init --root ./ragtest
    
    
    python -m graphrag.index --root ./ragtest
    
    
```
    python -m graphrag.query \                                                              
    --root ./ragtest \
    --method local \
    "tell me a Prompt formula about Instructions Prompt Technique."
```
    

###  settings.yaml 
    
    
```
    encoding_model: cl100k_base
    skip_workflows: []
    llm:
      api_key: ${GRAPHRAG_API_KEY}
      type: openai_chat # or azure_openai_chat
      model: llama3.1
      model_supports_json: true # recommended if this is available for your model.
      # max_tokens: 4000
      # request_timeout: 180.0
      api_base: http://localhost:8166/v1
      # api_version: 2024-02-15-preview
      # organization: <organization_id>
      # deployment_name: <azure_model_deployment_name>
      # tokens_per_minute: 150_000 # set a leaky bucket throttle
      # requests_per_minute: 10_000 # set a leaky bucket throttle
      # max_retries: 10
      # max_retry_wait: 10.0
      # sleep_on_rate_limit_recommendation: true # whether to sleep when azure suggests wait-times
      # concurrent_requests: 25 # the number of parallel inflight requests that may be made
```
    
```
    parallelization:
      stagger: 0.3
      # num_threads: 50 # the number of threads to use for parallel processing
```
    
    async_mode: threaded # or asyncio
    
```
    embeddings:
      ## parallelization: override the global parallelization settings for embeddings
      async_mode: threaded # or asyncio
      llm:
        api_key: ${GRAPHRAG_API_KEY}
        type: openai_embedding # or azure_openai_embedding
        model: text-embedding-3-small
        api_base: http://localhost:8166/v1
```

###  nomic-embed-text-v1.5æ¨¡å‹è¿è¡Œ 
    
    
```
    from sentence_transformers import SentenceTransformer
    from pydantic import BaseModel
    from fastapi import FastAPI
    import torch
    from transformers import AutoTokenizer
```
    
    app = FastAPI()
    
```
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model = SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)
```
    
    
```
    class Item(BaseModel):
        input: list
        model: str
        encoding_format: str = None
```
    
    
```
    @app.post("/v1/embeddings")
    async def create_embedding(item: Item):
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
        texts = [str(x) for x in item.input]
```
    
```
        # è®¡ç®—tokenæ•°é‡
        tokens = tokenizer(texts, padding=True, truncation=True)
        token_count = sum(len(ids) for ids in tokens['input_ids'])
```
    
```
        # ç”ŸæˆåµŒå…¥
        with torch.no_grad():
            embeddings = model.encode(texts, convert_to_tensor=True)
```
    
        # å°†å¼ é‡è½¬æ¢ä¸ºåˆ—è¡¨
        embeddings_list = embeddings.tolist()
    
```
        # æ„å»ºå“åº”
        data = [
            {
                "object": "embedding",
                "index": i,
                "embedding": emb
            }
            for i, emb in enumerate(embeddings_list)
        ]
```
    
```
        return {
            "object": "list",
            "data": data,
            "model": "text-embedding-3-small",  # æ”¹å›åŸæ¥çš„æ¨¡å‹åç§°
            "usage": {
                "prompt_tokens": token_count,
                "total_tokens": token_count
            }
        }
```
    
    
    if __name__ == "__main__":
        import uvicorn
    
        uvicorn.run(app, host="0.0.0.0", port=8166)
