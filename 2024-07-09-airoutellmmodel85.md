---
layout: single
title: "ã€AIé©å‘½ã€‘RouteLLMï¼šè®©å¤§è¯­è¨€æ¨¡å‹é™æœ¬85%çš„ç¥å¥‡æŠ€æœ¯ï¼"
sidebar:
  nav: "docs"
date: 2024-07-09 00:00:00 +0800
categories: [Fine-Tuning, LLM]
tags: [AI, GPT, GPT-4, LLM, Mistral, Ollama]
classes: wide
author_profile: true
---


#  ã€AIé©å‘½ã€‘RouteLLMï¼šè®©å¤§è¯­è¨€æ¨¡å‹é™æœ¬85%çš„ç¥å¥‡æŠ€æœ¯ï¼ 
    
    
    pip install "routellm[serve,eval]"
    
    
    
    export OPENAI_API_KEY="sk-proj-wDPaUDu1I"
    export ANYSCALE_API_KEY="secret_r4jh"
    
    
    #test question
    #what is the square root of 1787569?
    
    
    
    
    import os
    from routellm.controller import Controller
    
```
    os.environ["OPENAI_API_KEY"] = "sk-proj-wDPaUDu1Iimxi8SG3BfwT3B5M"
    # Replace with your model provider, we use Anyscale's Mixtral here.
    os.environ["ANYSCALE_API_KEY"] = "esecret_r4jhxj9jx75mewwt3"
```
    
```
    client = Controller(
      routers=["mf"],
      strong_model="gpt-4-1106-preview",
      weak_model="anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1",
    )
```
    
```
    response = client.chat.completions.create(
      # This tells RouteLLM to use the MF router with a cost threshold of 0.11593
      model="router-mf-0.11593",
      messages=[
        {"role": "user", "content": "Hello!"}
      ]
    )
```
    

###  ollama 
    
    
    import os
    from routellm.controller import Controller
    
    # è®¾ç½® OpenAI API å¯†é’¥
    
    
```
    client = Controller(
      routers=["mf"],
      strong_model="gpt-4-1106-preview",
      weak_model="ollama_chat/gemma2",
    )
```
    
    
```
    response = client.chat.completions.create(
      model="router-mf-0.11593",
      messages=[
        {"role": "user", "content": "Hello!"}
      ]
    )
```
    
    print(response.choices[0].message.content)
    

##  **ğŸ‘‰ğŸ‘‰ğŸ‘‰å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng**

##  **ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡**

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰æˆ‘çš„å¼€æºé¡¹ç›®** **[ https://github.com/win4r/AISuperDomain ](<https://github.com/win4r/AISuperDomain>) **

###  stream 
    
    
    import os
    from routellm.controller import Controller
    
```
    client = Controller(
      routers=["mf"],
      strong_model="gpt-4-1106-preview",
      weak_model="ollama_chat/gemma2",
    )
```
    
```
    # åˆ›å»ºæµå¼å“åº”
    stream = client.chat.completions.create(
      model="router-mf-0.11593",
      messages=[
        {"role": "user", "content": "who are you?"}
      ],
      stream=True  # å¯ç”¨æµå¼è¾“å‡º
    )
```
    
```
    # éå†æµå¹¶æ‰“å°æ¯ä¸ªéƒ¨åˆ†çš„å†…å®¹
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end='', flush=True)
```
    
    # æ‰“å°æ¢è¡Œç¬¦ï¼Œä½¿ä¸‹ä¸€ä¸ªæç¤ºç¬¦å‡ºç°åœ¨æ–°è¡Œ
    print()

### 
