---
layout: single
title: "roboflow+Paligemmaå¾®è°ƒè¯†åˆ«Xå…‰ç‰‡å¤§æ¨¡å‹"
sidebar:
  nav: "docs"
date: 2024-05-19 00:00:00 +0800
categories: [Fine-Tuning, LLM, Vision]
tags: [AI, HuggingFace, TensorFlow, Vision]
classes: wide
author_profile: true
---


#  roboflow+Paligemmaå¾®è°ƒè¯†åˆ«Xå…‰ç‰‡å¤§æ¨¡å‹ 

###  colabåœ¨çº¿ä½“éªŒ 

[ https://colab.research.google.com/drive/1xBmU7VNDRXPjhctFiBHqimA446I0i6qe#scrollTo=cb9NEdq2s-nf ](<https://colab.research.google.com/drive/1xBmU7VNDRXPjhctFiBHqimA446I0i6qe#scrollTo=cb9NEdq2s-nf>)

###  æœ¬åœ°è¿è¡Œ 
    
    
```
    # è¯·åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œä»¥ä¸‹å®‰è£…å‘½ä»¤ï¼Œæˆ–ç¡®ä¿è¿™äº›åº“å·²æ­£ç¡®å®‰è£…
    # pip install torch numpy Pillow requests
    # pip install git+https://github.com/huggingface/transformers.git
    #AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›è§†é¢‘(è§†é¢‘ç¦æ­¢ç›—æ¬ï¼Œè¿è€…å¿…ç©¶ï¼)
```
    
```
    import torch
    import numpy as np
    from PIL import Image
    import requests
```
    
```
    # å®šä¹‰è¾“å…¥æ–‡æœ¬å’Œå›¾åƒURL
    input_text = "What color is the flower that bee is standing on?"
    img_url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/bee.JPG?download=true"
    response = requests.get(img_url, stream=True)
    input_image = Image.open(response.raw).convert("RGB")
```
    
    from transformers import PaliGemmaForConditionalGeneration, PaliGemmaProcessor
    
    # è®¾ç½®è®¾å¤‡ï¼Œè‡ªåŠ¨æ£€æµ‹æ˜¯å¦æ”¯æŒCUDA
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
```
    # åˆå§‹åŒ–æ¨¡å‹å’Œå¤„ç†å™¨
    model_id = "leo009/paligemma-3b-mix-224"
    model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)
    processor = PaliGemmaProcessor.from_pretrained(model_id)
```
    
    # å¤„ç†è¾“å…¥
    inputs = processor(text=input_text, images=input_image, padding="longest", return_tensors="pt").to(device)
    
```
    # ç”Ÿæˆè¾“å‡º
    with torch.no_grad():
        output = model.generate(**inputs, max_length=496)
```
    
```
    # æ‰“å°è§£ç åçš„è¾“å‡º
    result = processor.decode(output[0], skip_special_tokens=True)
    print(result)
```
    

###  ç¯å¢ƒé…ç½® 
    
    
    # pip install roboflow supervision
    
    
```
    #Roboflowæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºå’Œç®¡ç†è®¡ç®—æœºè§†è§‰åº”ç”¨çš„æ•°æ®æ ‡æ³¨å¹³å°ã€‚
    #å®ƒæä¾›äº†ä¸€å¥—å·¥å…·ï¼Œç”¨äºè½»æ¾åœ°æ”¶é›†ã€æ ‡æ³¨å’Œå¢å¼ºå›¾åƒå’Œè§†é¢‘æ•°æ®é›†ã€‚
    #Roboflowè¿˜æä¾›äº†ä¸€ä¸ªå…¬å¼€çš„å¹³å°ï¼Œç”¨äºå…±äº«å’Œä¸‹è½½æ•°æ®é›†å’Œæ¨¡å‹ã€‚
```
    
    #AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›è§†é¢‘(è§†é¢‘ç¦æ­¢ç›—æ¬ï¼Œè¿è€…å¿…ç©¶ï¼)
    
    ###æ‰§è¡Œä¸‹è½½roboflowæ•°æ®é›†
    
    import os
    from roboflow import Roboflow
    
    # ç›´æ¥èµ‹å€¼ API å¯†é’¥ æ­¤å¤„ç”¨Private API Key https://app.roboflow.com/win4r/settings/api
    ROBOFLOW_API_KEY = "hrhnEY2OmKYHI6BbVYP5"
    
```
    # ä½¿ç”¨ Roboflow API å¯†é’¥åˆå§‹åŒ–
    rf = Roboflow(api_key=ROBOFLOW_API_KEY)
    project = rf.workspace("srinithi-s-tzdkb").project("fracture-detection-rhud5")
    version = project.version(4)
    dataset = version.download("paligemma")
```
    
    
    
    # pip install numpy supervision
    
    # python annotate_image.py
    
    #AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›è§†é¢‘(è§†é¢‘ç¦æ­¢ç›—æ¬ï¼Œè¿è€…å¿…ç©¶ï¼)
    
    ###è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ª `from_pali_gemma` å‡½æ•°ï¼Œç”¨äºè§£æå¸¦æœ‰æ£€æµ‹ç»“æœçš„å­—ç¬¦ä¸²ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º `sv.Detections` å¯¹è±¡ã€‚ç„¶åï¼Œä»£ç ä»æ³¨é‡Šæ–‡ä»¶ä¸­è¯»å–å›¾åƒä¿¡æ¯å’Œæ£€æµ‹ç»“æœï¼ŒåŠ è½½å›¾åƒå¹¶åº”ç”¨æ£€æµ‹ç»“æœè¿›è¡Œæ ‡æ³¨ï¼Œæœ€åæ˜¾ç¤ºæ ‡æ³¨åçš„å›¾åƒã€‚
    
```
    import re
    import numpy as np
    import supervision as sv
    from typing import Tuple, List, Optional
    from PIL import Image
    import json
```
    
```
    # å®šä¹‰ from_pali_gemma å‡½æ•°
    def from_pali_gemma(
        response: str,
        resolution_wh: Tuple[int, int],
        classes: Optional[List[str]] = None
    ) -> sv.Detections:
        _SEGMENT_DETECT_RE = re.compile(
            r'(.*?)' +
            r'<loc(\d{4})>' * 4 + r'\s*' +
            '(?:%s)?' % (r'<seg(\d{3})>' * 16) +
            r'\s*([^;<>]+)? ?(?:; )?',
        )
```
    
```
        width, height = resolution_wh
        xyxy_list = []
        class_name_list = []
```
    
```
        while response:
            m = _SEGMENT_DETECT_RE.match(response)
            if not m:
                break
```
    
```
            gs = list(m.groups())
            before = gs.pop(0)
            name = gs.pop()
            y1, x1, y2, x2 = [int(x) / 1024 for x in gs[:4]]
            y1, x1, y2, x2 = map(round, (y1*height, x1*width, y2*height, x2*width))
```
    
```
            content = m.group()
            if before:
                response = response[len(before):]
                content = content[len(before):]
```
    
```
            xyxy_list.append([x1, y1, x2, y2])
            class_name_list.append(name.strip())
            response = response[len(content):]
```
    
        xyxy = np.array(xyxy_list)
        class_name = np.array(class_name_list)
    
```
        if classes is None:
            class_id = None
        else:
            class_id = np.array([classes.index(name) for name in class_name])
```
    
```
        return sv.Detections(
            xyxy=xyxy,
            class_id=class_id,
            data={'class_name': class_name}
        )
```
    
```
    # ä»æ•°æ®é›†åŠ è½½ç¬¬ä¸€ä¸ªæ³¨é‡Š
    dataset_location = "/home/Ubuntu/fracture-detection-4"  # æ›¿æ¢ä¸ºå®é™…çš„æ•°æ®é›†è·¯å¾„
    first = json.loads(open(f"{dataset_location}/dataset/_annotations.train.jsonl").readline())
    print(first)
```
    
```
    # æ‰“å¼€å›¾åƒæ–‡ä»¶
    image = Image.open(f"{dataset_location}/dataset/{first.get('image')}")
    CLASSES = first.get('prefix').replace("detect ", "").split(" ; ")
    detections = from_pali_gemma(first.get('suffix'), image.size, CLASSES)
```
    
    # è¿›è¡Œå›¾åƒæ ‡æ³¨
    sv.BoundingBoxAnnotator().annotate(image, detections)
    

###  ä¸‹è½½é…ç½®big_vision 
    
    
    # ä¸‹è½½ big_vision ä»“åº“ï¼š
    #AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›è§†é¢‘(è§†é¢‘ç¦æ­¢ç›—æ¬ï¼Œè¿è€…å¿…ç©¶ï¼)
    
    
    git clone --quiet --branch=main --depth=1 https://github.com/google-research/big_vision big_vision_repo
    
    
    # è®¾ç½® PYTHONPATH ç¯å¢ƒå˜é‡ï¼š
    export PYTHONPATH=$PYTHONPATH:$(pwd)/big_vision_repo
    

###  ä¸‹è½½æ¨¡å‹ 
    
    
    #AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›è§†é¢‘(è§†é¢‘ç¦æ­¢ç›—æ¬è¿ï¼Œè¿è€…å¿…ç©¶ï¼)
    
    #å¸¸è§„ä¸‹è½½æ–¹å¼
    https://www.kaggle.com/api/v1/models/google/paligemma/jax/paligemma-3b-pt-224/1/download/paligemma-3b-pt-224.f16.npz
    
    #AIè¶…å…ƒåŸŸé¢‘é“æä¾›çš„ä¸‹è½½é“¾æ¥
    https://huggingface.co/leo009/paligemma-3b-pt-224.f16.npz/resolve/main/paligemma-3b-pt-224.f16.npz
    
    ä¸‹è½½åæˆ‘æ”¾åœ¨äº†è¿™ä¸ªè·¯å¾„:
    /home/Ubuntu/paligemma-3b-pt-224.f16.npz

###  ä¸‹è½½ **paligemma_tokenizer.model**
    
    
```
    #AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›è§†é¢‘(è§†é¢‘ç¦æ­¢ç›—æ¬è¿ï¼Œè¿è€…å¿…ç©¶ï¼)
    #å¸¸è§„ä¸‹è½½æ–¹å¼
    pip install gsutil
```
    
    gsutil cp gs://big_vision/paligemma_tokenizer.model /home/Ubuntu/paligemma_tokenizer.model
    
    
    #AIè¶…å…ƒåŸŸé¢‘é“æä¾›çš„ä¸‹è½½é“¾æ¥
    https://huggingface.co/leo009/paligemma_tokenizer.model/resolve/main/paligemma_tokenizer.model
    
    #æ–‡ä»¶å®Œæ•´è·¯å¾„ /home/Ubuntu/paligemma_tokenizer.model
    
    

###  åŠ è½½æ¨¡å‹å’Œæ ‡è®°å™¨ 
    
    
    #python load_model_and_tokenizer.py
    
    #AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›è§†é¢‘(è§†é¢‘ç¦æ­¢ç›—æ¬è¿ï¼Œè¿è€…å¿…ç©¶ï¼)
    import os
    
```
    # è®¾ç½®æœ¬åœ°æ–‡ä»¶è·¯å¾„
    MODEL_PATH = "/home/Ubuntu/paligemma-3b-pt-224.f16.npz"
    TOKENIZER_PATH = "/home/Ubuntu/paligemma_tokenizer.model"
```
    
```
    # åŠ è½½æ¨¡å‹ç¤ºä¾‹
    def load_model(model_path):
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹æ–‡ä»¶æ ¼å¼åŠ è½½æ¨¡å‹
        print(f"Loading model from {model_path}...")
        # æ¨¡å‹åŠ è½½ä»£ç 
        pass
```
    
```
    # åŠ è½½æ ‡è®°å™¨ç¤ºä¾‹
    def load_tokenizer(tokenizer_path):
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ ‡è®°å™¨æ–‡ä»¶æ ¼å¼åŠ è½½æ ‡è®°å™¨
        print(f"Loading tokenizer from {tokenizer_path}...")
        # æ ‡è®°å™¨åŠ è½½ä»£ç 
        pass
```
    
```
    # åŠ è½½æ¨¡å‹å’Œæ ‡è®°å™¨
    load_model(MODEL_PATH)
    load_tokenizer(TOKENIZER_PATH)
```
    
    print("Model and tokenizer loaded successfully.")
    

###  è®­ç»ƒ 
    
    
```
    #AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›è§†é¢‘(è§†é¢‘ç¦æ­¢ç›—æ¬è¿ï¼Œè¿è€…å¿…ç©¶ï¼)
    #å®‰è£…ä¾èµ–é¡¹
    pip install jaxlib tensorflow sentencepiece ml_collections ipython pillow
```
    
    pip install flax optax tensorflow-datasets
    
    pip install einops
    
    pip install jax[cuda12]
    

##  ğŸ”¥ğŸ”¥ğŸ”¥å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡: stoeng 

  
  


###  å®Œæ•´ä»£ç  
    
    
    #AIè¶…å…ƒåŸŸé¢‘é“åŸåˆ›è§†é¢‘(è§†é¢‘ç¦æ­¢ç›—æ¬è¿ï¼Œè¿è€…å¿…ç©¶ï¼)
    
```
    import base64
    import functools
    import html
    import io
    import os
    import warnings
```
    
```
    import jax
    import jax.numpy as jnp
    import numpy as np
    import ml_collections
```
    
    import tensorflow as tf
    import sentencepiece
    
    from IPython.core.display import display, HTML
    
    import torch  # ç¡®ä¿å¯¼å…¥ torch
    from PIL import Image
    
```
    # Import model definition from big_vision
    from big_vision.models.proj.paligemma import paligemma
    from big_vision.trainers.proj.paligemma import predict_fns
```
    
```
    # Import big vision utilities
    import big_vision.datasets.jsonl
    import big_vision.utils
    import big_vision.sharding
```
    
```
    # è®¾ç½®æœ¬åœ°æ–‡ä»¶è·¯å¾„
    MODEL_PATH = "/home/Ubuntu/paligemma-3b-pt-224.f16.npz"
    TOKENIZER_PATH = "/home/Ubuntu/paligemma_tokenizer.model"
    dataset_location = "/home/Ubuntu/fracture-detection-4"
```
    
    
```
    # åŠ è½½æ¨¡å‹ç¤ºä¾‹
    def load_model(model_path):
        print(f"Loading model from {model_path}...")
        model = np.load(model_path)  # ä½¿ç”¨ NumPy åŠ è½½æ¨¡å‹
        return model
```
    
```
    # åŠ è½½æ ‡è®°å™¨ç¤ºä¾‹
    def load_tokenizer(tokenizer_path):
        print(f"Loading tokenizer from {tokenizer_path}...")
        with open(tokenizer_path, 'rb') as f:
            tokenizer = f.read()  # æ ¹æ®æ ‡è®°å™¨çš„å®é™…æ ¼å¼è°ƒæ•´
        return tokenizer
```
    
```
    # åŠ è½½æ¨¡å‹å’Œæ ‡è®°å™¨
    model = load_model(MODEL_PATH)
    tokenizer = load_tokenizer(TOKENIZER_PATH)
    print("Model and tokenizer loaded successfully.")
```
    
```
    # ä¸è¦é™åˆ¶TFä½¿ç”¨GPUæˆ–TPU
    # tf.config.set_visible_devices([], "GPU")
    # tf.config.set_visible_devices([], "TPU")
```
    
```
    # æ£€æŸ¥æ˜¯å¦æœ‰GPUå¯ç”¨
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # è®¾ç½®GPUä½¿ç”¨é™åˆ¶ä¸ºæŒ‰éœ€å¢é•¿ï¼Œè€Œä¸æ˜¯é¢„å…ˆåˆ†é…æ‰€æœ‰çš„æ˜¾å­˜
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            print(e)
```
    
```
    backend = jax.lib.xla_bridge.get_backend()
    print(f"JAX version:  {jax.__version__}")
    print(f"JAX platform: {backend.platform}")
    print(f"JAX devices:  {jax.device_count()}")
```
    
```
    # å¦‚æœä½ å¸Œæœ›JAXä½¿ç”¨GPUï¼Œç¡®ä¿CUDAå’ŒcuDNNå·²æ­£ç¡®å®‰è£…å¹¶é…ç½®
    if backend.platform == 'gpu':
        print("JAX is using GPU")
    else:
        print("JAX is not using GPU, check your CUDA/cuDNN installation.")
```
    
    
    
    # @title Construct model and load params into RAM.
    
```
    # Define model
    model_config = ml_collections.FrozenConfigDict({
        "llm": {"vocab_size": 257_152},
        "img": {"variant": "So400m/14", "pool_type": "none", "scan": True, "dtype_mm": "float16"}
    })
    model = paligemma.Model(**model_config)
    tokenizer = sentencepiece.SentencePieceProcessor(TOKENIZER_PATH)
```
    
    # åŠ è½½å‚æ•°
    params = paligemma.load(None, MODEL_PATH, model_config)
    
```
    # å®šä¹‰ `decode` å‡½æ•°ä»¥ä»æ¨¡å‹ä¸­é‡‡æ ·è¾“å‡ºã€‚
    decode_fn = predict_fns.get_all(model)['decode']
    decode = functools.partial(decode_fn, devices=jax.devices(), eos_token=tokenizer.eos_id())
```
    
    
    
```
    # å°†å‚æ•°ç§»è‡³ GPU å†…å­˜
    #
    # åˆ›å»ºå¯è®­ç»ƒå‚æ•°çš„ pytree æ©ç ã€‚
    def is_trainable_param(name, param):  # pylint: disable=unused-argument
      if name.startswith("llm/layers/attn/"):  return True
      if name.startswith("llm/"):              return False
      if name.startswith("img/"):              return False
      raise ValueError(f"Unexpected param name {name}")
    trainable_mask = big_vision.utils.tree_map_with_names(is_trainable_param, params)
```
    
```
    #
    # å¦‚æœæœ‰å¤šä¸ªè®¾å¤‡å¯ç”¨ï¼ˆä¾‹å¦‚ï¼Œå¤šå— GPUï¼‰ï¼Œå¯ä»¥å°†å‚æ•°åˆ†ç‰‡åˆ°è¿™äº›è®¾å¤‡ä¸Šï¼Œä»¥å‡å°‘æ¯ä¸ªè®¾å¤‡çš„ HBM ä½¿ç”¨é‡ã€‚
    mesh = jax.sharding.Mesh(jax.devices(), ("data"))
```
    
    data_sharding = jax.sharding.NamedSharding(
        mesh, jax.sharding.PartitionSpec("data"))
    
    params_sharding = big_vision.sharding.infer_sharding(
        params, strategy=[('.*', 'fsdp(axis="data")')], mesh=mesh)
    
```
    # Yes: Some donated buffers are not usable.
    warnings.filterwarnings(
        "ignore", message="Some donated buffers were not usable")
```
    
```
    @functools.partial(jax.jit, donate_argnums=(0,), static_argnums=(1,))
    def maybe_cast_to_f32(params, trainable):
      return jax.tree.map(lambda p, m: p.astype(jnp.float32) if m else p,
                          params, trainable)
```
    
```
    # Loading all params in simultaneous - albeit much faster and more succinct -
    # requires more RAM than the T4 colab runtimes have by default (12GB RAM).
    # Instead we do it param by param.
    params, treedef = jax.tree.flatten(params)
    sharding_leaves = jax.tree.leaves(params_sharding)
    trainable_leaves = jax.tree.leaves(trainable_mask)
    for idx, (sharding, trainable) in enumerate(zip(sharding_leaves, trainable_leaves)):
      params[idx] = big_vision.utils.reshard(params[idx], sharding)
      params[idx] = maybe_cast_to_f32(params[idx], trainable)
      params[idx].block_until_ready()
    params = jax.tree.unflatten(treedef, params)
```
    
```
    # Print params to show what the model is made of.
    def parameter_overview(params):
      for path, arr in big_vision.utils.tree_flatten_with_names(params)[0]:
        print(f"{path:80s} {str(arr.shape):22s} {arr.dtype}")
```
    
    print(" == Model params == ")
    parameter_overview(params)
    
    
    
    # @title Define preprocess functions to create inputs to the model.
    
```
    def preprocess_image(image, size=224):
      # æ¨¡å‹å·²è¢«è®­ç»ƒå¤„ç†ä¸åŒçºµæ¨ªæ¯”çš„å›¾åƒï¼Œè¿™äº›å›¾åƒè¢«è°ƒæ•´ä¸º 224x224ï¼ŒèŒƒå›´åœ¨ [-1, 1] ä¹‹é—´ã€‚
      # åŒçº¿æ€§å’ŒæŠ—é”¯é½¿ç¼©æ”¾é€‰é¡¹æœ‰åŠ©äºæé«˜æŸäº›ä»»åŠ¡çš„è´¨é‡ã€‚
      image = np.asarray(image)
      if image.ndim == 2:  # Convert image without last channel into greyscale.
        image = np.stack((image,)*3, axis=-1)
      image = image[..., :3]  # Remove alpha layer.
      assert image.shape[-1] == 3
```
    
```
      image = tf.constant(image)
      image = tf.image.resize(image, (size, size), method='bilinear', antialias=True)
      return image.numpy() / 127.5 - 1.0  # [0, 255]->[-1,1]
```
    
```
    def preprocess_tokens(prefix, suffix=None, seqlen=None):
      # æ¨¡å‹å·²è¢«è®­ç»ƒå¤„ç†ç”±å¸¦æœ‰å…¨æ³¨æ„åŠ›çš„å‰ç¼€å’Œå¸¦æœ‰å› æœæ³¨æ„åŠ›çš„åç¼€ç»„æˆçš„æ ‡è®°åŒ–æ–‡æœ¬ã€‚
      separator = "\n"
      tokens = tokenizer.encode(prefix, add_bos=True) + tokenizer.encode(separator)
      mask_ar = [0] * len(tokens)    # 0 to use full attention for prefix.
      mask_loss = [0] * len(tokens)  # 0 to not use prefix tokens in the loss.
```
    
```
      if suffix:
        suffix = tokenizer.encode(suffix, add_eos=True)
        tokens += suffix
        mask_ar += [1] * len(suffix)    # 1 to use causal attention for suffix.
        mask_loss += [1] * len(suffix)  # 1 to use suffix tokens in the loss.
```
    
```
      mask_input = [1] * len(tokens)    # 1 if its a token, 0 if padding.
      if seqlen:
        padding = [0] * max(0, seqlen - len(tokens))
        tokens = tokens[:seqlen] + padding
        mask_ar = mask_ar[:seqlen] + padding
        mask_loss = mask_loss[:seqlen] + padding
        mask_input = mask_input[:seqlen] + padding
```
    
      return jax.tree.map(np.array, (tokens, mask_ar, mask_loss, mask_input))
    
```
    def postprocess_tokens(tokens):
      tokens = tokens.tolist()  # np.array to list[int]
      try:  # Remove tokens at and after EOS if any.
        eos_pos = tokens.index(tokenizer.eos_id())
        tokens = tokens[:eos_pos]
      except ValueError:
        pass
      return tokenizer.decode(tokens)
```
    
    
    
    # å‡½æ•°ç”¨äºéå†è®­ç»ƒå’ŒéªŒè¯æ ·æœ¬ã€‚
    SEQLEN = 128
    
```
    # TODO: è€ƒè™‘ä½¿ç”¨è·³è¿‡ big_vision å’Œ tf.data çš„æ•°æ®è¿­ä»£å™¨ï¼Ÿ
    train_dataset = big_vision.datasets.jsonl.DataSource(
        os.path.join(dataset_location, "dataset/_annotations.train.jsonl"),
        fopen_keys={"image": f"{dataset_location}/dataset"})
```
    
```
    val_dataset = big_vision.datasets.jsonl.DataSource(
        os.path.join(dataset_location, "dataset/_annotations.valid.jsonl"),
        fopen_keys={"image": f"{dataset_location}/dataset"})
```
    
    
```
    def train_data_iterator():
      """æ— é™å¾ªç¯éå†è®­ç»ƒæ ·æœ¬çš„è¿­ä»£å™¨ã€‚"""
      # å¯¹æ ·æœ¬è¿›è¡Œæ‰“ä¹±å¹¶é‡å¤ï¼Œè¿™æ ·å¯ä»¥è¿›è¡Œå¤šè½®è®­ç»ƒã€‚
      dataset = train_dataset.get_tfdata().shuffle(1_000).repeat()
      for example in dataset.as_numpy_iterator():
        image = Image.open(io.BytesIO(example["image"]))
        image = preprocess_image(image)
```
    
```
        # prefix = "caption en"  # Could also be a different prefix per example.
        prefix = example["prefix"].decode().lower()
        suffix = example["suffix"].decode().lower()
        tokens, mask_ar, mask_loss, _ = preprocess_tokens(prefix, suffix, SEQLEN)
```
    
```
        yield {
            "image": np.asarray(image),
            "text": np.asarray(tokens),
            "mask_ar": np.asarray(mask_ar),
            "mask_loss": np.asarray(mask_loss),
        }
```
    
    
```
    def validation_data_iterator():
      """éå†éªŒè¯æ ·æœ¬çš„å•æ¬¡è¿­ä»£å™¨ã€‚"""
      for example in val_dataset.get_tfdata(ordered=True).as_numpy_iterator():
        image = Image.open(io.BytesIO(example["image"]))
        image = preprocess_image(image)
```
    
```
        # prefix = "caption en"  # æ¯ä¸ªæ ·æœ¬ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸åŒçš„å‰ç¼€ã€‚
        prefix = example["prefix"].decode().lower()
        tokens, mask_ar, _, mask_input = preprocess_tokens(prefix, seqlen=SEQLEN)
```
    
```
        yield {
            "image": np.asarray(image),
            "text": np.asarray(tokens),
            "mask_ar": np.asarray(mask_ar),
            "mask_input": np.asarray(mask_input),
        }
```
    
    
    
    
    
```
    # æ£€æŸ¥è®­ç»ƒæ ·æœ¬ã€‚
    def split_and_keep_second_part(s):
      parts = s.split('\n', 1)
      if len(parts) > 1:
        return parts[1]
      return s
```
    
```
    def render_inline(image, resize=(128, 128)):
      """Convert image into inline html."""
      image = Image.fromarray(image)
      image.resize(resize)
      with io.BytesIO() as buffer:
        image.save(buffer, format='jpeg')
        image_b64 = str(base64.b64encode(buffer.getvalue()), "utf-8")
        return f"data:image/jpeg;base64,{image_b64}"
```
    
```
    def render_example(image, caption):
      image = ((image + 1)/2 * 255).astype(np.uint8)  # [-1,1] -> [0, 255]
      h, w, _ = image.shape
      try:
          detections = from_pali_gemma(caption, (w, h), CLASSES)
          image = sv.BoundingBoxAnnotator().annotate(image, detections)
      except:
          print("result render failed, result:", caption)
      return f"""
        <div style="display: inline-flex; align-items: center; justify-content: center;">
            <img style="width:128px; height:128px;" src="{render_inline(image, resize=(64,64))}" />
            <p style="width:256px; margin:10px; font-size:small;">{html.escape(caption)}</p>
        </div>
        """
```
    
```
    html_out = ""
    for idx, example in zip(range(8), train_data_iterator()):
      caption = postprocess_tokens(example["text"])  # detokenize model input.
      caption = split_and_keep_second_part(caption)
      html_out += render_example(example["image"], caption)
```
    
    print("Training examples")
    display(HTML(html_out))
    
    #print(html_out)  # ç›´æ¥æ‰“å°HTMLå†…å®¹
    
```
    # ä¿å­˜HTMLå†…å®¹åˆ°æ–‡ä»¶
    with open("/home/Ubuntu/Trainingexamples.html", "w") as f:
        f.write(html_out)
    print("HTML content saved to Trainingexamples.html")
```
    
    
    #------------------------------------------
    
```
    # å®šä¹‰è®­ç»ƒæ­¥éª¤å’Œè¯„ä¼°å¾ªç¯
    #
    # ä½¿ç”¨ç®€å•éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰çš„ä¸»è¦æ›´æ–°å‡½æ•°ã€‚
    #
    @functools.partial(jax.jit, donate_argnums=(0,))
    def update_fn(params, batch, learning_rate):
      imgs, txts, mask_ar = batch["image"], batch["text"], batch["mask_ar"]
```
    
```
      def loss_fn(params):
        text_logits, _ = model.apply({"params": params}, imgs, txts[:, :-1], mask_ar[:, :-1], train=True)
        logp = jax.nn.log_softmax(text_logits, axis=-1)
```
    
```
        #  æ¨¡å‹çš„è¾“å…¥æ˜¯ txts[:, :-1]ï¼Œä½†æŸå¤±æ˜¯å®šä¹‰ä¸ºé¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®° txts[:, 1:]ã€‚
        # æ­¤å¤–ï¼Œmask_loss[:, 1:] è¡¨ç¤ºå“ªäº›æ ‡è®°æ˜¯æŸå¤±çš„ä¸€éƒ¨åˆ†ï¼ˆä¾‹å¦‚ï¼Œå‰ç¼€å’Œå¡«å……æ ‡è®°ä¸åŒ…æ‹¬åœ¨å†…ï¼‰ã€‚
        mask_loss = batch["mask_loss"][:, 1:]
        targets = jax.nn.one_hot(txts[:, 1:], text_logits.shape[-1])
```
    
```
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±ï¼Œå³æ¯ä¸ªæ ‡è®°å›°æƒ‘åº¦çš„å¹³å‡å€¼ã€‚
        # ç”±äºæ¯ä¸ªæ ·æœ¬çš„æ ‡è®°æ•°é‡ä¸åŒï¼Œæˆ‘ä»¬è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚
        token_pplx = jnp.sum(logp * targets, axis=-1)  # sum across vocab_size.
        example_loss = -jnp.sum(token_pplx * mask_loss, axis=-1)  # sum across seq_len.
        example_loss /= jnp.clip(jnp.sum(mask_loss, -1), 1)  # weight by num of tokens.
```
    
        # batch_loss: mean of per example loss.
        return jnp.mean(example_loss)
    
      loss, grads = jax.value_and_grad(loss_fn)(params)
    
```
      # ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰å°†æ¢¯åº¦åº”ç”¨äºå¯è®­ç»ƒå‚æ•°.
      def apply_grad(param, gradient, trainable):
        if not trainable: return param
        return param - learning_rate * gradient
```
    
      params = jax.tree_util.tree_map(apply_grad, params, grads, trainable_mask)
    
      return params, loss
    
```
    # è¯„ä¼°/æ¨ç†å¾ªç¯ã€‚
    def make_predictions(data_iterator, *, num_examples=None,
                         batch_size=4, seqlen=SEQLEN, sampler="greedy"):
      outputs = []
      while True:
        # æ„å»ºæ‰¹æ¬¡ä¸­çš„æ ·æœ¬åˆ—è¡¨ã€‚
        examples = []
        try:
          for _ in range(batch_size):
            examples.append(next(data_iterator))
            examples[-1]["_mask"] = np.array(True)  # Indicates true example.
        except StopIteration:
          if len(examples) == 0:
            return outputs
```
    
```
        #æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œæ— æ³•å®Œæˆä¸€ä¸ªæ‰¹æ¬¡ã€‚é€šè¿‡é‡å¤æœ€åä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¡«å……ã€‚
        while len(examples) % batch_size:
          examples.append(dict(examples[-1]))
          examples[-1]["_mask"] = np.array(False)  # Indicates padding example.
```
    
```
        # Convert list of examples into a dict of np.arrays and load onto devices.
        batch = jax.tree.map(lambda *x: np.stack(x), *examples)
        batch = big_vision.utils.reshard(batch, data_sharding)
```
    
```
        # è¿›è¡Œæ¨¡å‹é¢„æµ‹
        tokens = decode({"params": params}, batch=batch,
                        max_decode_len=seqlen, sampler=sampler)
```
    
```
        # å°†æ¨¡å‹é¢„æµ‹è·å–åˆ°è®¾å¤‡å¹¶è¿›è¡Œå»æ ‡è®°åŒ–ã€‚
        tokens, mask = jax.device_get((tokens, batch["_mask"]))
        tokens = tokens[mask]  # remove padding examples.
        responses = [postprocess_tokens(t) for t in tokens]
```
    
```
        # é™„åŠ åˆ° html è¾“å‡ºã€‚
        for example, response in zip(examples, responses):
          outputs.append((example["image"], response))
          if num_examples and len(outputs) >= num_examples:
            return outputs
```
    
    
    
    #æ²¡æœ‰å¾®è°ƒçš„æ—¶å€™æ£€æŸ¥æ¨¡å‹æ€§èƒ½
    
```
    print("Model predictions")
    html_out = ""
    for image, caption in make_predictions(validation_data_iterator(), num_examples=8, batch_size=4):
      html_out += render_example(image, caption)
    display(HTML(html_out))
```
    
    
    #print(html_out)  # ç›´æ¥æ‰“å°HTMLå†…å®¹
    
```
    # ä¿å­˜HTMLå†…å®¹åˆ°æ–‡ä»¶
    with open("/home/Ubuntu/output_without_finetuning.html", "w") as f:
        f.write(html_out)
    print("HTML content saved to output_without_finetuning.html")
```
    
    
    
    
    
```
    # ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨è¿è¡Œç®€çŸ­çš„è®­ç»ƒå¾ªç¯ã€‚
    #
    # æ³¨æ„ï¼šç”±äº XLA ç¼–è¯‘ jax.jit å‡½æ•°ï¼Œåœ¨æŸäº›æœºå™¨ä¸Šç¬¬ä¸€æ¬¡è¿­ä»£å¯èƒ½ä¼šéå¸¸æ…¢ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰ã€‚
```
    
```
    BATCH_SIZE = 8
    TRAIN_EXAMPLES = 512
    # TRAIN_EXAMPLES = 256
    LEARNING_RATE = 0.01
```
    
    TRAIN_STEPS = TRAIN_EXAMPLES // BATCH_SIZE
    EVAL_STEPS = TRAIN_STEPS // 8
    
    train_data_it = train_data_iterator()
    
```
    sched_fn = big_vision.utils.create_learning_rate_schedule(
        total_steps=TRAIN_STEPS+1, base=LEARNING_RATE,
        decay_type="cosine", warmup_percent=0.10)
```
    
```
    for step in range(1, TRAIN_STEPS+1):
      # åˆ—å‡º N ä¸ªè®­ç»ƒæ ·æœ¬ã€‚
      examples = [next(train_data_it) for _ in range(BATCH_SIZE)]
```
    
```
      # å°†ç¤ºä¾‹åˆ—è¡¨è½¬æ¢ä¸º np.arrays çš„å­—å…¸å¹¶åŠ è½½åˆ°è®¾å¤‡ä¸Šã€‚
      batch = jax.tree.map(lambda *x: np.stack(x), *examples)
      batch = big_vision.utils.reshard(batch, data_sharding)
```
    
```
      # è®­ç»ƒæ­¥éª¤å¹¶æŠ¥å‘Šè®­ç»ƒæŸå¤±
      learning_rate = sched_fn(step)
      params, loss = update_fn(params, batch, learning_rate)
```
    
      loss = jax.device_get(loss)
      print(f"step: {step:2d}/{TRAIN_STEPS:2d}   lr: {learning_rate:.5f}   loss: {loss:.4f}")
    
```
      if step == 1 or (step % EVAL_STEPS) == 0:
        print(f"Model predictions at step {step}")
        html_out = ""
        for image, caption in make_predictions(
            validation_data_iterator(), num_examples=4, batch_size=4):
          html_out += render_example(image, caption)
        display(HTML(html_out))
        # ä¿å­˜HTMLå†…å®¹åˆ°æ–‡ä»¶
        with open("/home/Ubuntu/output_training_loop.html", "w") as f:
            f.write(html_out)
        print("HTML content saved to output_training_loop.html")
```
    
    
    
```
    print("Model predictions")
    html_out = ""
    for image, caption in make_predictions(validation_data_iterator(), batch_size=4):
      html_out += render_example(image, caption)
    display(HTML(html_out))
```
    
```
    with open("/home/Ubuntu/output_Evaluate.html", "w") as f:
        f.write(html_out)
    print("HTML content saved to output_Evaluate.html")
```
    
    
```
    #ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
    flat, _ = big_vision.utils.tree_flatten_with_names(params)
    with open("/home/Ubuntu/fine-tuned-paligemma-3b-pt-224.f16.npz", "wb") as f:
      np.savez(f, **{k: v for k, v in flat})
```

##  å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡: stoeng   
  


#  ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡ 

#  ğŸ‘‰ğŸ‘‰ğŸ‘‰ [ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>)

#  ğŸ‘‰ğŸ‘‰ğŸ‘‰ [ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>)
