---
layout: single
title: "é˜¿é‡Œå·´å·´é‡ç£…å‘å¸ƒQwen2-VLï¼šè¶…è¶Šäººç±»çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œä»åŒ»å­¦å½±åƒåˆ°æ‰‹å†™è¯†åˆ«ï¼Œè¿™æ¬¾å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹ç©¶ç«Ÿæœ‰å¤šå¼ºï¼Ÿæ·±åº¦è§£æä¸å®æµ‹"
sidebar:
  nav: "docs"
date: 2024-08-31 00:00:00 +0800
categories: [Fine-Tuning, LLM, Vision]
tags: [AI, HuggingFace, PyTorch, Qwen, Vision]
classes: wide
author_profile: true
---


#  é˜¿é‡Œå·´å·´é‡ç£…å‘å¸ƒQwen2-VLï¼šè¶…è¶Šäººç±»çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œä»åŒ»å­¦å½±åƒåˆ°æ‰‹å†™è¯†åˆ«ï¼Œè¿™æ¬¾å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹ç©¶ç«Ÿæœ‰å¤šå¼ºï¼Ÿæ·±åº¦è§£æä¸å®æµ‹ 

###  Qwen2-VL-72B 

[ https://huggingface.co/spaces/Qwen/Qwen2-VL ](<https://huggingface.co/spaces/Qwen/Qwen2-VL>)

[ https://colab.research.google.com/drive/1y7g9-StAWSf_fbSybQgBFjH-fMuRR5PN#scrollTo=o5SXsfB1VENw ](<https://colab.research.google.com/drive/1y7g9-StAWSf_fbSybQgBFjH-fMuRR5PN#scrollTo=o5SXsfB1VENw>)

###  æ¨¡å‹ç®€ä»‹ 

> Qwen2-VL æœ‰å¤šç§å®é™…ç”¨é€”ï¼Œæ¶µç›–äº†å¤šä¸ªé¢†åŸŸçš„åº”ç”¨åœºæ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€æ•°æ®å¤„ç†æ–¹é¢ã€‚ä»¥ä¸‹æ˜¯è¯¥æ¨¡å‹çš„ä¸€äº›ä¸»è¦åº”ç”¨åœºæ™¯ï¼š 
> 
>   1. **å®æ—¶è§†é¢‘åˆ†æä¸äº’åŠ¨** ï¼šQwen2-VL å¯ç”¨äºå®æ—¶è§†é¢‘åˆ†æï¼Œåœ¨è§†é¢‘é€šè¯æˆ–ç›´æ’­ä¸­å›ç­”ç”¨æˆ·æå‡ºçš„é—®é¢˜ã€‚è¿™ç§èƒ½åŠ›å¯ä»¥åº”ç”¨äº **å®¢æœ** åœºæ™¯ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è§†é¢‘å±•ç¤ºäº§å“æˆ–æ¡å½¢ç ï¼Œæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å¹¶æä¾›ç›¸å…³ä¿¡æ¯ã€‚ 
> 

>   2. **å›¾åƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡** ï¼šQwen2-VL èƒ½å¤Ÿå¤„ç†å¤æ‚çš„å›¾åƒå’Œè§†é¢‘è¾“å…¥ï¼Œæ”¯æŒå¦‚ **è§†è§‰å†…å®¹åˆ†æã€å›¾åƒè¯†åˆ«ã€è§†é¢‘æ‘˜è¦** ç­‰ä»»åŠ¡ã€‚å®ƒåœ¨ç”µå­å•†åŠ¡ã€ç›‘æ§ã€åŒ»ç–—å½±åƒåˆ†æç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„æ½œåœ¨åº”ç”¨ã€‚ 
> 

>   3. **å¤šè¯­è¨€å¤šæ¨¡æ€äº¤äº’** ï¼šè¯¥æ¨¡å‹æ”¯æŒæ–‡æœ¬ä¸è§†è§‰ä¿¡æ¯çš„ç»“åˆï¼Œèƒ½å®ç°å¤šè¯­è¨€çš„æ–‡æœ¬ç”Ÿæˆå’Œç¿»è¯‘ï¼Œè¿˜å¯ä»¥å¤„ç†åŒ…æ‹¬å›¾åƒå’Œè§†é¢‘åœ¨å†…çš„å¤æ‚å†…å®¹ã€‚è¿™ç§ç‰¹æ€§ä½¿å…¶åœ¨ **è·¨å›½å®¢æœã€è¯­è¨€ç¿»è¯‘ã€æ™ºèƒ½åŠ©ç†** ç­‰åœºæ™¯ä¸­å…·æœ‰å¹¿æ³›åº”ç”¨ã€‚ 
> 

>   4. **å·¥å…·è°ƒç”¨ä¸å¤–éƒ¨æ•°æ®é›†æˆ** ï¼šQwen2-VL æ”¯æŒè°ƒç”¨å¤–éƒ¨å·¥å…·å’Œè®¿é—®å¤–éƒ¨æ•°æ®ï¼Œå¦‚ **èˆªç­çŠ¶æ€æŸ¥è¯¢ã€å¤©æ°”é¢„æŠ¥ã€ç‰©æµè·Ÿè¸ª** ç­‰ï¼Œä½¿å…¶åœ¨ç‰©æµå’Œç°åœºæ“ä½œä¸­å…·æœ‰å¾ˆå¼ºçš„å®ç”¨æ€§ã€‚ 
> 

>   5. **è§†è§‰å†…å®¹ç”Ÿæˆå’Œæ¨ç†** ï¼šè¯¥æ¨¡å‹å¯ä»¥æ ¹æ®è¾“å…¥çš„å›¾åƒæˆ–è§†é¢‘å†…å®¹ç”Ÿæˆç›¸åº”çš„æ–‡æœ¬æè¿°æˆ–æ¨ç†ç»“æœï¼Œé€‚ç”¨äº **å¹¿å‘Šè®¾è®¡ã€åˆ›æ„ç”Ÿæˆã€è§†é¢‘æ‘˜è¦** ç­‰é¢†åŸŸã€‚ 
> 


###  æœ¬åœ°é…ç½® 
    
    
    conda create -n qwen2_vl python=3.11
    conda activate qwen2_vl
    
```
    conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
    pip install pillow requests
    pip install git+https://github.com/huggingface/transformers
```
    
    
    pip install 'accelerate>=0.26.0'
    
    

##  **ğŸ‘‰ğŸ‘‰ğŸ‘‰å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng**

##  **ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡**

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰æˆ‘çš„å¼€æºé¡¹ç›®** **[ https://github.com/win4r/AISuperDomain ](<https://github.com/win4r/AISuperDomain>) **

###  code 
    
    
    from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
    from qwen_vl_utils import process_vision_info
    
```
    # default: Load the model on the available device(s)
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        "Qwen/Qwen2-VL-7B-Instruct", torch_dtype="auto", device_map="auto"
    )
```
    
```
    # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
    # model = Qwen2VLForConditionalGeneration.from_pretrained(
    #     "Qwen/Qwen2-VL-7B-Instruct",
    #     torch_dtype=torch.bfloat16,
    #     attn_implementation="flash_attention_2",
    #     device_map="auto",
    # )
```
    
    # default processer
    processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")
    
```
    # The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.
    # min_pixels = 256*28*28
    # max_pixels = 1280*28*28
    # processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)
```
    
```
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                },
                {"type": "text", "text": "Describe this image."},
            ],
        }
    ]
```
    
```
    # Preparation for inference
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to("cuda")
```
    
```
    # Inference: Generation of the output
    generated_ids = model.generate(**inputs, max_new_tokens=128)
    generated_ids_trimmed = [
        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )
    print(output_text)
```
    

> è¿™æ®µä»£ç çš„ä¸»è¦åŠŸèƒ½æ˜¯ä½¿ç”¨Qwen2VLæ¨¡å‹ï¼ˆä¸€ä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰æ¥æè¿°ç»™å®šçš„å›¾åƒã€‚å®ƒé¦–å…ˆåŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œç„¶åä¸‹è½½ä¸€ä¸ªå›¾åƒï¼Œæ„å»ºä¸€ä¸ªåŒ…å«å›¾åƒå’Œæ–‡æœ¬æç¤ºçš„å¯¹è¯ç»“æ„ã€‚æ¥ç€ï¼Œå®ƒé¢„å¤„ç†è¾“å…¥æ•°æ®ï¼Œä½¿ç”¨æ¨¡å‹ç”Ÿæˆæè¿°ï¼Œæœ€åè§£ç å¹¶æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬æè¿°ã€‚ 
> 
> è¿™ä¸ªè¿‡ç¨‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ç°ä»£çš„å¤šæ¨¡æ€AIæ¨¡å‹æ¥æ‰§è¡Œå›¾åƒæè¿°ä»»åŠ¡ï¼Œç»“åˆäº†è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ã€‚ 
    
    
```
    # å¯¼å…¥æ‰€éœ€çš„åº“
    from PIL import Image  # ç”¨äºå›¾åƒå¤„ç†
    import requests  # ç”¨äºå‘é€HTTPè¯·æ±‚
    import torch  # PyTorchæ·±åº¦å­¦ä¹ åº“
    from torchvision import io  # PyTorchçš„è®¡ç®—æœºè§†è§‰å·¥å…·åŒ…
    from typing import Dict  # ç”¨äºç±»å‹æ³¨è§£
    from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor  # Hugging Faceçš„transformersåº“ï¼Œç”¨äºåŠ è½½å’Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
```
    
```
    # åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°ï¼Œè‡ªåŠ¨é€‰æ‹©å¯ç”¨è®¾å¤‡
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        "Qwen/Qwen2-VL-7B-Instruct", torch_dtype="auto", device_map="auto"
    )
    # åŠ è½½å¤„ç†å™¨ï¼Œç”¨äºé¢„å¤„ç†è¾“å…¥æ•°æ®
    processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")
```
    
```
    # è®¾ç½®å›¾åƒURLå¹¶ä¸‹è½½å›¾åƒ
    url = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"
    image = Image.open(requests.get(url, stream=True).raw)
```
    
```
    # æ„å»ºå¯¹è¯ç»“æ„ï¼ŒåŒ…å«ç”¨æˆ·è§’è‰²ã€å›¾åƒå’Œæ–‡æœ¬æç¤º
    conversation = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                },
                {"type": "text", "text": "æè¿°è¿™å¼ å›¾."},
            ],
        }
    ]
```
    
    # ä½¿ç”¨å¤„ç†å™¨åº”ç”¨èŠå¤©æ¨¡æ¿ï¼Œç”Ÿæˆæ–‡æœ¬æç¤º
    text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
    
```
    # é¢„å¤„ç†è¾“å…¥æ•°æ®ï¼Œå°†æ–‡æœ¬å’Œå›¾åƒè½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼
    inputs = processor(
        text=[text_prompt], images=[image], padding=True, return_tensors="pt"
    )
    inputs = inputs.to("cuda")  # å°†è¾“å…¥æ•°æ®ç§»è‡³GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
```
    
    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè¾“å‡º
    output_ids = model.generate(**inputs, max_new_tokens=128)
    
```
    # æå–ç”Ÿæˆçš„æ–°tokenï¼ˆå»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
    generated_ids = [
        output_ids[len(input_ids) :]
        for input_ids, output_ids in zip(inputs.input_ids, output_ids)
    ]
```
    
```
    # è§£ç ç”Ÿæˆçš„tokenä¸ºå¯è¯»æ–‡æœ¬
    output_text = processor.batch_decode(
        generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
    )
```
    
    # æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬
    print(output_text)
