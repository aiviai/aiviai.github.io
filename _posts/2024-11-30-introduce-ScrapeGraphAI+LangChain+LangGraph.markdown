---
layout: single  
title: "ScrapeGraphAIå¼€å¯æ™ºèƒ½æ•°æ®æŠ“å–æ–°æ—¶ä»£ï¼ç”¨AIé‡å¡‘æ•°æ®æå–æ–¹å¼ï¼ScrapeGraphAI+LangChain+LangGraphæ‰“é€ æœ€å¼ºæ–‡ç« é‡‡é›†å’Œå†™ä½œAIæ™ºèƒ½ä½“ï¼è®©å†…å®¹åˆ›ä½œæ›´ç®€å•"  
sidebar:
  nav: "docs"
date: 2024-12-30 10:00:00 +0800  
categories: AIAgents
tags: [Aiæ™ºèƒ½ä½“, çˆ¬è™«, å¼€æº, è‡ªç„¶è¯­è¨€å¤„ç†,LangChain,LangGraph]
classes: wide  

author_profile: true  
---

ä¼ ç»Ÿçš„ç½‘é¡µæŠ“å–å·¥å…·ä¸¥é‡ä¾èµ–äºé¢„å®šä¹‰çš„è§„åˆ™å’Œæ¨¡å¼ï¼Œä¸€æ—¦ç›®æ ‡ç½‘ç«™çš„ç»“æ„å‘ç”Ÿå˜åŒ–ï¼Œå°±éœ€è¦æ‰‹åŠ¨æ›´æ–°ä»£ç ï¼Œè€—æ—¶è´¹åŠ›ã€‚è€Œ ScrapeGraphAI åˆ™å¦è¾Ÿè¹Šå¾„ï¼Œå®ƒåˆ©ç”¨ LLMs çš„å¼ºå¤§ç†è§£èƒ½åŠ›ï¼Œè®©ç”¨æˆ·åªéœ€ç”¨è‡ªç„¶è¯­è¨€æè¿°æ‰€éœ€æ•°æ®ï¼Œå‰©ä¸‹çš„å¤æ‚å·¥ä½œå…¨éƒ¨äº¤ç»™ AI å®Œæˆï¼

### æœ¬ç¯‡ç¬”è®°æ‰€å¯¹åº”çš„è§†é¢‘ï¼š

- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡å“”å“©å“”å“©è§‚çœ‹](https://bili2233.cn/reWSLAE)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡YouTubeè§‚çœ‹](https://youtu.be/PEB8z48mAhw)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¼€æºé¡¹ç›®](https://github.com/win4r/AISuperDomain)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ è¯·æˆ‘å–å’–å•¡](https://ko-fi.com/aila)
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¾®ä¿¡ï¼šstoeng


### ScrapeGraphAIï¼šé©æ–°ç½‘ç»œçˆ¬å–çš„å¼€æºå·¥å…·

**ScrapeGraphAI** æ˜¯ä¸€ä¸ªå¼€æºçš„ Python åº“ï¼Œé€šè¿‡ç»“åˆ**å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒåŸºäºå›¾çš„é€»è¾‘**ï¼Œå½»åº•é©æ–°äº†ç½‘ç»œçˆ¬å–æŠ€æœ¯ã€‚ç”¨æˆ·åªéœ€ç”¨ç®€å•çš„è‡ªç„¶è¯­è¨€æè¿°éœ€è¦æå–çš„ä¿¡æ¯ï¼Œå³å¯ä»ç½‘ç«™æˆ–å¤šç§æ–‡æ¡£æ ¼å¼ä¸­æå–ç»“æ„åŒ–æ•°æ®ã€‚

---

### æ ¸å¿ƒåŠŸèƒ½

### **è‡ªé€‚åº”çˆ¬å–**

ScrapeGraphAI åˆ©ç”¨ LLM æŠ€æœ¯ï¼Œèƒ½å¤Ÿè‡ªåŠ¨é€‚åº”ç½‘ç«™ç»“æ„çš„å˜åŒ–ï¼Œå¤§å¹…é™ä½äº†å¯¹é¢‘ç¹ç»´æŠ¤å’Œæ›´æ–°çš„éœ€æ±‚ã€‚ä¸åƒä¼ ç»Ÿä¾èµ–å›ºå®šæ¨¡å¼çš„çˆ¬å–å·¥å…·ï¼Œå®ƒå¯ä»¥åº”å¯¹ä¸æ–­å˜åŒ–çš„ç½‘ç«™å¸ƒå±€ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚

### **å¤šå¹³å°æ”¯æŒ**

è¯¥å¹³å°å…¼å®¹å¤šç§ LLM æä¾›å•†ï¼ŒåŒ…æ‹¬ GPTã€Geminiã€Groqã€Azure å’Œ Hugging Faceï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒé€šè¿‡ Ollama ä½¿ç”¨æœ¬åœ°æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥å¤„ç† XMLã€HTMLã€JSON å’Œ Markdown ç­‰å¤šç§æ–‡æ¡£æ ¼å¼ã€‚

---

### å¯ç”¨ç®¡é“

| **ç®¡é“åç§°** | **åŠŸèƒ½æè¿°** |
| --- | --- |
| **SmartScraperGraph** | åŸºäºæç¤ºè¯å’Œæ¥æºçš„å•é¡µé¢æ•°æ®æå– |
| **SearchGraph** | ä»æœç´¢ç»“æœä¸­è¿›è¡Œå¤šé¡µé¢æ•°æ®æå– |
| **SpeechGraph** | å°†æå–å†…å®¹è½¬æ¢ä¸ºéŸ³é¢‘ |
| **ScriptCreatorGraph** | ç”Ÿæˆçˆ¬å–æ‰€éœ€çš„ Python è„šæœ¬ |

---

### å®ç°æ–¹å¼

ScrapeGraphAI é‡‡ç”¨åŸºäºå›¾çš„è®¾è®¡ï¼Œå°†çˆ¬å–æµç¨‹åˆ†è§£ä¸ºå¤šä¸ªç¦»æ•£èŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹è´Ÿè´£å¤„ç†ä¸åŒçš„ä»»åŠ¡ã€‚è¿™ç§æ¨¡å—åŒ–çš„æ–¹æ³•ä¸ä»…æå‡äº†çˆ¬å–ç®¡é“çš„çµæ´»æ€§å’Œé²æ£’æ€§ï¼Œè¿˜ä½¿å¾—åŠŸèƒ½çš„æ‰©å±•å’Œä¿®æ”¹å˜å¾—æ›´åŠ é«˜æ•ˆã€‚

---

### ä¼˜åŠ¿åˆ†æ

### **ç®€åŒ–æ“ä½œ**

ç”¨æˆ·åªéœ€æè¿°æƒ³è¦æå–çš„æ•°æ®ï¼Œå…¶ä½™æŠ€æœ¯ç»†èŠ‚å‡ç”± ScrapeGraphAI è´Ÿè´£å¤„ç†ã€‚è¿™ä½¿å¾—å®ƒåœ¨ AI åº”ç”¨ã€æ•°æ®åˆ†æã€æ•°æ®é›†åˆ›å»ºä»¥åŠå¹³å°æ­å»ºç­‰åœºæ™¯ä¸­æå…·ä»·å€¼ã€‚

### **é™ä½ç»´æŠ¤æˆæœ¬**

AI é©±åŠ¨çš„çˆ¬å–æ–¹æ³•æ˜¾è‘—å‡å°‘äº†è„šæœ¬æ›´æ–°å’Œç»´æŠ¤çš„éœ€æ±‚ï¼Œè®©å¼€å‘è€…èƒ½ä¸“æ³¨äºæ•°æ®åˆ†æè€Œéæ•…éšœæ’æŸ¥ã€‚åŒæ—¶ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨é€‚åº”ç½‘ç«™çš„å˜åŒ–ï¼Œä¸ºé•¿æœŸçš„æ•°æ®æå–éœ€æ±‚æä¾›äº†æ›´å…·å¯æŒç»­æ€§çš„è§£å†³æ–¹æ¡ˆã€‚

---

### æ€»ç»“

ScrapeGraphAI å‡­å€Ÿå…¶è‡ªé€‚åº”èƒ½åŠ›ã€å¤šå¹³å°æ”¯æŒå’Œæ¨¡å—åŒ–è®¾è®¡ï¼Œä¸ºæ•°æ®æå–å¸¦æ¥äº†å…¨æ–°çš„å¯èƒ½ã€‚æ— è®ºæ˜¯ä»ç®€å•çš„å•é¡µé¢æŠ“å–åˆ°å¤æ‚çš„å¤šé¡µé¢æ•°æ®æŒ–æ˜ï¼Œå®ƒéƒ½ä¸ºå¼€å‘è€…æä¾›äº†å¼ºå¤§ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¯ç½‘ç»œçˆ¬å–é¢†åŸŸçš„ä¸€æ¬¡é‡å¤§çªç ´ã€‚

---

## æ ¸å¿ƒç®¡é“åŠŸèƒ½

**SmartScraperGraph**
é’ˆå¯¹å•é¡µç½‘ç«™çš„æ•°æ®æå–ï¼Œåªéœ€æä¾›ç”¨æˆ·æç¤ºå’Œè¾“å…¥æºå³å¯å®ŒæˆæŠ“å–ã€‚

**SearchGraph**
æ”¯æŒä»æœç´¢å¼•æ“çš„å¤šä¸ªæœç´¢ç»“æœä¸­æ‰¹é‡æå–ä¿¡æ¯ã€‚

**SpeechGraph**
èƒ½å¤Ÿä»ç½‘ç«™æå–ä¿¡æ¯å¹¶è‡ªåŠ¨ç”ŸæˆéŸ³é¢‘æ–‡ä»¶ï¼Œå®ç°æ–‡æœ¬åˆ°è¯­éŸ³çš„è½¬æ¢ã€‚

## æŠ€æœ¯ç‰¹æ€§

**æ¨¡å‹æ”¯æŒ**
æ”¯æŒå¤šç§LLMæä¾›å•†å’Œéƒ¨ç½²æ–¹å¼ï¼š

- äº‘ç«¯æ¨¡å‹ï¼šGPTã€Geminiã€Groqã€Azure
- æœ¬åœ°æ¨¡å‹ï¼šé€šè¿‡Ollamaå®ç°

**æ–‡æ¡£å¤„ç†èƒ½åŠ›**
å¯å¤„ç†å¤šç§æ ¼å¼çš„æ–‡æ¡£ï¼š

- HTMLç½‘é¡µ
- XMLæ–‡ä»¶
- JSONæ•°æ®
- Markdownæ–‡æ¡£

## æ™ºèƒ½åŒ–åŠŸèƒ½

**è‡ªåŠ¨åŒ–ç»“æ„åˆ†æ**

- èƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«ç½‘é¡µä¸­çš„é‡è¦æ•°æ®èŠ‚ç‚¹
- æ”¯æŒé™æ€å†…å®¹å’ŒåŠ¨æ€åŠ è½½å†…å®¹çš„æå–
- æ— éœ€æ‰‹åŠ¨ç¼–å†™å¤æ‚çš„æŠ“å–è§„åˆ™

**è‡ªé€‚åº”èƒ½åŠ›**

- å¯ä»¥é€‚åº”ç½‘ç«™ç»“æ„çš„å˜åŒ–
- è‡ªåŠ¨è°ƒæ•´æŠ“å–ç­–ç•¥
- å‡å°‘ç»´æŠ¤æˆæœ¬å’Œäººå·¥å¹²é¢„

## å®ç”¨å·¥å…·

**å›¾å½¢åŒ–å·¥ä½œæµ**
é‡‡ç”¨åŸºäºå›¾çš„è®¾è®¡æ–¹æ³•ï¼Œå°†æŠ“å–è¿‡ç¨‹åˆ†è§£ä¸ºç¦»æ•£èŠ‚ç‚¹ï¼Œæä¾›çµæ´»çš„ç®¡é“å®šåˆ¶èƒ½åŠ›ã€‚

**å¼€å‘å·¥å…·æ”¯æŒ**

- æ”¯æŒPlaywrightå¤„ç†JavaScriptæ¸²æŸ“çš„ç½‘é¡µ
- æä¾›APIè®¾è®¡å’Œè¯¦ç»†æ–‡æ¡£
- åŒ…å«ä¸°å¯Œçš„ä½¿ç”¨ç¤ºä¾‹

## æ•°æ®è¾“å‡º

- æå–çš„ä¿¡æ¯ä»¥å­—å…¸æ ¼å¼è¾“å‡º
- æ”¯æŒç»“æ„åŒ–æ•°æ®çš„è‡ªåŠ¨å¯¼å…¥
- å¯ç›´æ¥ç”¨äºåç»­çš„æ•°æ®åˆ†æå’Œå¤„ç†

### ScrapeGraphAI API keyç”³è¯· [https://dashboard.scrapegraphai.com/](https://dashboard.scrapegraphai.com/)

### Tavily API Keyç”³è¯· [https://app.tavily.com/home](https://app.tavily.com/home)

### å®‰è£…å’Œéƒ¨ç½²

```bash
pip install scrapegraphai

sudo playwright install

pip install -U duckduckgo-search

pip install scrapegraphai'[other-language-models]'

pip install scrapegraphai'[more-semantic-options]'

pip install scrapegraphai'[more-browser-options]'

```

### OpenAIæ¥å£ç¤ºä¾‹

```python
import os
from dotenv import load_dotenv

import json
from scrapegraphai.graphs import SmartScraperGraph

load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")

# Define the configuration for the scraping pipeline
graph_config = {
    "llm": {
        "api_key": api_key,
        "model": "openai/gpt-4o-mini",
    },
    "verbose": True,
    "headless": True,
}

# Create the SmartScraperGraph instance
smart_scraper_graph = SmartScraperGraph(
    prompt="Extract all the posts from the website",
    source="https://www.aivi.fyi/",
    config=graph_config
)

# Run the pipeline
result = smart_scraper_graph.run()
print(json.dumps(result, indent=4,ensure_ascii=False))
```

### Ollama

```python
import json
from scrapegraphai.graphs import SmartScraperGraph

# Define the configuration for the scraping pipeline
graph_config = {
    "llm": {
        "api_key": "ollama",
        "model": "ollama/mistral-nemo:latest",
        "temperature": 0,
        "rate_limit": {
            "requests_per_second": 1
        }
    },
    "verbose": True,
    "headless": False,
}

# æŠ“å–ç½‘é¡µæ ‡é¢˜
smart_scraper_graph = SmartScraperGraph(
    prompt="List all article titles on the page",
    source="https://www.aivi.fyi/",
    config=graph_config
)

# Run the pipeline
result = smart_scraper_graph.run()
print(result)
```

### ğŸ”¥ğŸ”¥ğŸ”¥LangGraph

### ä¸»è¦åŠŸèƒ½ï¼š

1. ç½‘ç«™å†…å®¹æå–ï¼šä½¿ç”¨ langchain-scrapegraph å·¥å…·è‡ªåŠ¨æå–åšå®¢ç½‘ç«™ä¸Šçš„æ–‡ç« å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€æ‘˜è¦ã€æ­£æ–‡å’Œæ ‡ç­¾ã€‚
2. æ•°æ®ç»“æ„å®šä¹‰ï¼š
- ä½¿ç”¨ Pydantic æ¨¡å‹å®šä¹‰äº†ä¸¤ä¸ªæ•°æ®ç»“æ„ï¼š
    - BlogPostSchemaï¼šå®šä¹‰å•ç¯‡æ–‡ç« çš„ç»“æ„ï¼ˆæ ‡é¢˜ã€æ‘˜è¦ã€å†…å®¹ã€æ ‡ç­¾ï¼‰
    - BlogPostsSchemaï¼šå®šä¹‰æ•´ä¸ªåšå®¢çš„æ–‡ç« åˆ—è¡¨ç»“æ„
1. AI è¾…åŠ©æå–ï¼š
- ä½¿ç”¨ OpenAI çš„ GPT-4 æ¨¡å‹æ¥ç†è§£å’Œæå–ç½‘é¡µå†…å®¹
- é€šè¿‡ ReAct agent åè°ƒ AI æ¨¡å‹å’Œç½‘é¡µæŠ“å–å·¥å…·çš„å·¥ä½œ
1. æ•°æ®å¤„ç†å’Œå­˜å‚¨ï¼š
- å°†æå–çš„å†…å®¹è½¬æ¢ä¸º Pandas DataFrame æ ¼å¼
- è‡ªåŠ¨å°†ç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶ï¼Œæ”¯æŒä¸­æ–‡ç¼–ç 

å…³é”®ç»„ä»¶ï¼š

1. SmartScraperToolï¼šæ™ºèƒ½ç½‘é¡µå†…å®¹æå–å·¥å…·
2. ChatOpenAIï¼šGPTæ¨¡å‹æ¥å£
3. ReAct agentï¼šåè°ƒå„ç»„ä»¶å·¥ä½œçš„ä»£ç†
4. Pandasï¼šæ•°æ®å¤„ç†å’Œä¿å­˜

ä½¿ç”¨åœºæ™¯ï¼š

- æ‰¹é‡æå–åšå®¢æ–‡ç« å†…å®¹
- å»ºç«‹æ–‡ç« æ•°æ®åº“
- å†…å®¹åˆ†æå’Œç®¡ç†
- è‡ªåŠ¨åŒ–å†…å®¹é‡‡é›†

æŠ€æœ¯ç‰¹ç‚¹ï¼š

1. è‡ªåŠ¨åŒ–ï¼šæ— éœ€æ‰‹åŠ¨å¤åˆ¶ç²˜è´´
2. ç»“æ„åŒ–ï¼šè¾“å‡ºæ ¼å¼ç»Ÿä¸€ã€è§„èŒƒ
3. æ™ºèƒ½åŒ–ï¼šä½¿ç”¨ AI ç†è§£å’Œæå–å†…å®¹
4. å¯æ‰©å±•ï¼šæ˜“äºä¿®æ”¹å’Œé€‚åº”ä¸åŒç½‘ç«™ç»“æ„

```bash
from pydantic import BaseModel, Field
from typing import List
import getpass
import os
from langchain_scrapegraph.tools import SmartScraperTool
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
import json
import pandas as pd
from google.colab import userdata

# å®šä¹‰ä¸€ä¸ªæ–‡ç« çš„schema
class BlogPostSchema(BaseModel):
    title: str = Field(description="æ–‡ç« æ ‡é¢˜")
    summary: str = Field(description="æ–‡ç« æ‘˜è¦/æ€»ç»“")
    content: str = Field(description="æ–‡ç« å®Œæ•´å†…å®¹")
    tags: List[str] = Field(description="æ–‡ç« æ ‡ç­¾")

# å®šä¹‰åŒ…å«å¤šç¯‡æ–‡ç« çš„schema
class BlogPostsSchema(BaseModel):
    posts: List[BlogPostSchema] = Field(description="åšå®¢æ–‡ç« åˆ—è¡¨")

# è®¾ç½®APIå¯†é’¥
if not os.environ.get("SGAI_API_KEY"):
    os.environ["SGAI_API_KEY"] = userdata.get('SGAI_API_KEY')

if not os.environ.get("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# åˆå§‹åŒ–scraperå·¥å…·
smartscraper_tool = SmartScraperTool(llm_output_schema=BlogPostsSchema)

# åˆå§‹åŒ–LLMæ¨¡å‹
llm_model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# è®¾ç½®å·¥å…·åˆ—è¡¨
tools = [smartscraper_tool]

# è®¾ç½®å†…å­˜ä¿å­˜å™¨
memory = MemorySaver()

# é…ç½®
config = {"configurable": {"thread_id": "1"}}

# åˆå§‹åŒ–(ReAct agent) [ReAct agentæ˜¯ä¸€ç§åˆ›æ–°çš„æ™ºèƒ½Agentæ¶æ„ï¼Œå®ƒé€šè¿‡ç»“åˆæ¨ç†å’Œè¡ŒåŠ¨çš„èƒ½åŠ›æ¥è§£å†³å¤æ‚ä»»åŠ¡ã€‚]
graph = create_react_agent(llm_model, tools=tools, checkpointer=memory)

# è¿è¡ŒæŠ“å–
inputs = {
    "messages": [
        (
            "user",
            "Go to https://www.aivi.fyi/ and extract all blog posts with their titles, summaries, content and tags"
        )
    ]
}

# æ‰§è¡Œgraphå¹¶è·å–ç»“æœ
print("å¼€å§‹æŠ“å–åšå®¢å†…å®¹...")
for event in graph.stream(inputs, config, stream_mode="values"):
    event["messages"][-1].pretty_print()

# è·å–æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆå‡è®¾æ˜¯SmartScraperå·¥å…·çš„å“åº”ï¼‰
result = graph.get_state(config).values["messages"][-1].content

# å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºJSON
result = json.loads(result)

# è½¬æ¢ä¸ºDataFrame
df = pd.DataFrame([
    {
        'title': post['title'],
        'summary': post['summary'],
        'content': post['content'],
        'tags': ','.join(post['tags']) if post['tags'] else ''
    }
    for post in result['posts']
])

# ä¿å­˜åˆ°CSVæ–‡ä»¶
csv_file = "blog_posts.csv"
df.to_csv(csv_file, index=False, encoding='utf-8')
print(f"æ•°æ®å·²ä¿å­˜åˆ° {csv_file}")

# æ˜¾ç¤ºæå–çš„æ–‡ç« æ•°é‡
print(f"\næ€»å…±æå–äº† {len(df)} ç¯‡æ–‡ç« ")
```

### ğŸ”¥æœç´¢èµ„è®¯å¹¶æ”¹å†™ä¸ºç§‘æŠ€æ–‡ç« 

### æ ¸å¿ƒåŠŸèƒ½ï¼š

1. æ–°é—»æœç´¢ä¸æå–
- ä½¿ç”¨ Tavily æœç´¢å¼•æ“æœç´¢æœ€æ–°çš„ AI ç›¸å…³æ–°é—»
- é€šè¿‡ SmartScraper å·¥å…·æå–ç½‘é¡µå†…å®¹
- å¯¹æœç´¢ç»“æœè¿›è¡Œç»“æ„åŒ–å¤„ç†
1. å†…å®¹åˆ†æä¸å¤„ç†
- è§£æå’ŒéªŒè¯æœç´¢åˆ°çš„æ–‡ç« æ•°æ®
- æå–å…³é”®ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€é“¾æ¥ã€æ‘˜è¦ã€æ–‡ç« å†…å®¹ç­‰ï¼‰
- å¯¹å†…å®¹è¿›è¡Œæ ¼å¼åŒ–å’Œæ¸…ç†
1. æ–‡ç« è‡ªåŠ¨ç”Ÿæˆ
- ä½¿ç”¨ GPT-4 æ¨¡å‹ç”Ÿæˆç§‘æŠ€èµ„è®¯æ–‡ç« 
- æ•´åˆå¤šä¸ªæ–°é—»æºçš„ä¿¡æ¯
- ç”Ÿæˆç»“æ„åŒ–çš„æ–‡ç« ï¼ˆæ ‡é¢˜ã€å‰¯æ ‡é¢˜ã€æ­£æ–‡ç­‰ï¼‰
1. æ•°æ®å­˜å‚¨å’Œè¾“å‡º
- å°†åŸå§‹æ•°æ®ä¿å­˜ä¸º CSV æ–‡ä»¶
- ç”Ÿæˆ Markdown æ ¼å¼çš„æ–‡ç« 
- æä¾›æ–‡ç« é¢„è§ˆåŠŸèƒ½

æŠ€æœ¯ç‰¹ç‚¹ï¼š

1. æ¨¡å—åŒ–è®¾è®¡
- ä½¿ç”¨ç±»å’Œæ–¹æ³•ç»„ç»‡ä»£ç 
- åŠŸèƒ½æ¨¡å—æ¸…æ™°åˆ†ç¦»
- ä¾¿äºç»´æŠ¤å’Œæ‰©å±•
1. é”™è¯¯å¤„ç†æœºåˆ¶
- å®Œæ•´çš„å¼‚å¸¸æ•è·å’Œå¤„ç†
- è¯¦ç»†çš„æ—¥å¿—è®°å½•
- å®¹é”™å’Œæ¢å¤æœºåˆ¶
1. æ•°æ®æ¨¡å‹è§„èŒƒ
- ä½¿ç”¨ Pydantic æ¨¡å‹å®šä¹‰æ•°æ®ç»“æ„
- å¼ºç±»å‹æ£€æŸ¥
- æ•°æ®éªŒè¯å’Œæ¸…ç†

å®é™…åº”ç”¨åœºæ™¯ï¼š

1. ç§‘æŠ€åª’ä½“
- å¿«é€Ÿç”Ÿæˆ AI é¢†åŸŸæ–°é—»ç»¼è¿°
- è·Ÿè¸ªæŠ€æœ¯å‘å±•åŠ¨æ€
- åˆ¶ä½œè¡Œä¸šæŠ¥å‘Š
1. ç ”ç©¶æœºæ„
- æ”¶é›† AI ç ”ç©¶è¿›å±•
- æ•´ç†æŠ€æœ¯å‘å±•è¶‹åŠ¿
- ç”Ÿæˆç ”ç©¶ç®€æŠ¥
1. ä¼ä¸šå†³ç­–
- è·Ÿè¸ªè¡Œä¸šåŠ¨æ€
- ç«äº‰å¯¹æ‰‹åˆ†æ
- æŠ€æœ¯è¶‹åŠ¿åˆ†æ

è¾“å‡ºæˆæœï¼š

1. ç»“æ„åŒ–æ•°æ®ï¼ˆCSV æ–‡ä»¶ï¼‰
- åŸå§‹æ–°é—»æ•°æ®
- æ–‡ç« å…ƒä¿¡æ¯
- æ¥æºé“¾æ¥
1. ç”Ÿæˆæ–‡ç« ï¼ˆMarkdown æ–‡ä»¶ï¼‰
- ä¸“ä¸šçš„ç§‘æŠ€æ–‡ç« 
- åŒ…å«å¤šä¸ªä¿¡æ¯æº
- æ·±åº¦åˆ†æå’Œè§è§£

è¿™ä¸ªç³»ç»Ÿçš„ä¸»è¦ä»·å€¼åœ¨äºè‡ªåŠ¨åŒ–äº†ä»æ–°é—»æœç´¢åˆ°æ–‡ç« ç”Ÿæˆçš„å®Œæ•´æµç¨‹ï¼Œå¤§å¤§æé«˜äº†ç§‘æŠ€èµ„è®¯ç”Ÿäº§çš„æ•ˆç‡ï¼ŒåŒæ—¶ä¿è¯äº†å†…å®¹çš„ä¸“ä¸šæ€§å’Œå¯è¯»æ€§ã€‚

```python
from pydantic import BaseModel, Field
from typing import List, Optional
import getpass
import os
from langchain_scrapegraph.tools import SmartScraperTool
from langchain_community.tools import TavilySearchResults
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
import json
import pandas as pd
from datetime import datetime
import logging
import time
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# æ•°æ®æ¨¡å‹å®šä¹‰
class AIArticle(BaseModel):
    """å¢å¼ºçš„AIæ–‡ç« æ•°æ®æ¨¡å‹"""
    title: str = Field(description="æ–‡ç« æ ‡é¢˜")
    link: str = Field(description="æ–‡ç« URLé“¾æ¥")
    summary: Optional[str] = Field(description="æ–‡ç« æ‘˜è¦/æ€»ç»“", default="")
    content: Optional[str] = Field(description="æ–‡ç« å®Œæ•´å†…å®¹", default="")
    publish_date: Optional[str] = Field(description="å‘å¸ƒæ—¥æœŸ", default="")
    author: Optional[str] = Field(description="ä½œè€…", default="")
    tags: Optional[List[str]] = Field(description="æ–‡ç« æ ‡ç­¾", default_factory=list)

class AIArticles(BaseModel):
    """AIæ–‡ç« åˆ—è¡¨çš„æ•°æ®æ¨¡å‹"""
    articles: List[AIArticle] = Field(description="AIå’Œå¤§æ¨¡å‹ç›¸å…³çš„æ–‡ç« åˆ—è¡¨")

class TechNewsArticle(BaseModel):
    """ç§‘æŠ€èµ„è®¯æ–‡ç« çš„æ•°æ®æ¨¡å‹"""
    title: str = Field(description="ç§‘æŠ€èµ„è®¯æ ‡é¢˜")
    subtitle: str = Field(description="å‰¯æ ‡é¢˜")
    author: str = Field(description="ä½œè€…")
    date: str = Field(description="å‘å¸ƒæ—¥æœŸ")
    content: str = Field(description="æ–‡ç« å†…å®¹")
    sources: List[str] = Field(description="ä¿¡æ¯æ¥æº")

class AINewsGenerator:
    def __init__(self):
        self.setup_api_keys()
        self.smartscraper_tool = None
        self.tavily_tool = None
        self.llm_model = None
        self.graph = None
        self.config = None

    def setup_api_keys(self):
        """è®¾ç½®å¿…è¦çš„APIå¯†é’¥"""
        try:
            if not os.environ.get("SGAI_API_KEY"):
                os.environ["SGAI_API_KEY"] = userdata.get('SGAI_API_KEY')
            
            if not os.environ.get("TAVILY_API_KEY"):
                os.environ["TAVILY_API_KEY"] = userdata.get('TAVILY_API_KEY')
            
            if not os.environ.get("OPENAI_API_KEY"):
                os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
        except Exception as e:
            logging.error(f"è®¾ç½®APIå¯†é’¥æ—¶å‡ºé”™: {str(e)}")
            raise

    def initialize_tools(self):
        """åˆå§‹åŒ–æ‰€éœ€å·¥å…·"""
        try:
            self.smartscraper_tool = SmartScraperTool(llm_output_schema=AIArticles)
            
            self.tavily_tool = TavilySearchResults(
                max_results=5,
                name="ai_news_finder",
                description="æœç´¢æœ€æ–°çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ç›¸å…³çš„æ–°é—»å’Œæ–‡ç« "
            )
            
            self.llm_model = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.3,
                request_timeout=120
            )
            
            return [self.smartscraper_tool, self.tavily_tool]
        except Exception as e:
            logging.error(f"åˆå§‹åŒ–å·¥å…·æ—¶å‡ºé”™: {str(e)}")
            raise

    def create_agent(self, tools):
        """åˆ›å»ºReAct agent"""
        try:
            memory = MemorySaver()
            self.config = {"configurable": {"thread_id": str(int(time.time()))}}
            
            self.graph = create_react_agent(
                model=self.llm_model,
                tools=tools,
                checkpointer=memory
            )
            
            return self.graph, self.config
        except Exception as e:
            logging.error(f"åˆ›å»ºagentæ—¶å‡ºé”™: {str(e)}")
            raise

    def extract_article_content(self, url: str) -> dict:
        """ä»å•ä¸ªURLæå–å®Œæ•´æ–‡ç« å†…å®¹"""
        try:
            extract_prompt = f"""
            è¯·ä»ä»¥ä¸‹URLæå–æ–‡ç« çš„å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
            1. æ ‡é¢˜
            2. ä½œè€…
            3. å‘å¸ƒæ—¥æœŸ
            4. å®Œæ•´çš„æ–‡ç« å†…å®¹ï¼ˆä¿ç•™æ‰€æœ‰æ®µè½å’Œæ ¼å¼ï¼‰
            5. æ–‡ç« ä¸­çš„å…³é”®æŠ€æœ¯ç‚¹
            6. ç›¸å…³é“¾æ¥å’Œå¼•ç”¨
            7. æ–‡ç« æ ‡ç­¾æˆ–åˆ†ç±»
            
            URL: {url}
            
            è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            { "title": "æ–‡ç« æ ‡é¢˜", "author": "ä½œè€…åç§°", "publish_date": "å‘å¸ƒæ—¥æœŸ", "content": "å®Œæ•´æ–‡ç« å†…å®¹", "summary": "æ–‡ç« æ‘˜è¦", "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"] }
            """
            
            extraction_inputs = {
                "messages": [("user", extract_prompt)]
            }
            
            article_content = None
            for event in self.graph.stream(extraction_inputs, self.config, stream_mode="values"):
                message = event["messages"][-1]
                if hasattr(message, 'content'):
                    article_content = message.content
                    break
            
            # ç¡®ä¿è¿”å›æœ‰æ•ˆçš„æ•°æ®ç»“æ„
            if article_content:
                try:
                    parsed_content = json.loads(article_content)
                    return parsed_content
                except json.JSONDecodeError:
                    return {
                        "title": "æ— æ ‡é¢˜",
                        "author": "æœªçŸ¥",
                        "publish_date": "",
                        "content": article_content,
                        "summary": article_content[:200] + "...",
                        "tags": []
                    }
            return None
        except Exception as e:
            logging.error(f"æå–æ–‡ç« å†…å®¹æ—¶å‡ºé”™ {url}: {str(e)}")
            return None

    def search_and_extract(self):
        """æœç´¢å’Œæå–AIç›¸å…³æ–‡ç« """
        try:
            # é¦–å…ˆæœç´¢ç›¸å…³æ–‡ç« 
            search_query = "latest developments in open source large language models AI technology"
            search_inputs = {
                "messages": [("user", search_query)]
            }
            
            logging.info("å¼€å§‹æœç´¢ç›¸å…³æ–‡ç« ...")
            urls_found = set()
            
            for event in self.graph.stream(search_inputs, self.config, stream_mode="values"):
                message = event["messages"][-1]
                if hasattr(message, 'content'):
                    try:
                        # è§£æè¿”å›çš„æœç´¢ç»“æœ
                        if isinstance(message.content, str):
                            data = json.loads(message.content)
                            if isinstance(data, list):
                                for item in data:
                                    if 'url' in item:
                                        urls_found.add(item['url'])
                    except:
                        continue

            articles_data = {"articles": []}
            
            # å¯¹æ¯ä¸ªURLè¿›è¡Œå†…å®¹æå–
            for url in urls_found:
                logging.info(f"æ­£åœ¨æå–æ–‡ç« : {url}")
                article_data = self.extract_article_content(url)
                
                if article_data:
                    article_data["link"] = url
                    articles_data["articles"].append(article_data)
                
                # æ·»åŠ å»¶æ—¶é¿å…è¿‡å¿«è¯·æ±‚
                time.sleep(2)
            
            return articles_data
                
        except Exception as e:
            logging.error(f"æœç´¢å’Œæå–è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return {"articles": []}

    def generate_article(self, articles_data: dict) -> TechNewsArticle:
        """ç”Ÿæˆç§‘æŠ€èµ„è®¯æ–‡ç« """
        try:
            if not isinstance(articles_data, dict) or "articles" not in articles_data:
                articles_data = {"articles": []}
            
            sources = [article["link"] for article in articles_data["articles"] if article.get("link")]
            
            # å¢å¼ºçš„æç¤ºè¯
            prompt = f"""
            åŸºäºä»¥ä¸‹AIå’Œå¤§è¯­è¨€æ¨¡å‹ç›¸å…³çš„æ–°é—»ä¿¡æ¯ï¼Œæ’°å†™ä¸€ç¯‡æ·±åº¦ç§‘æŠ€èµ„è®¯æ–‡ç« ã€‚

            è¦æ±‚ï¼š
            1. ä¸“ä¸šæ€§ï¼šä½¿ç”¨å‡†ç¡®çš„æŠ€æœ¯æœ¯è¯­ï¼Œä¿æŒå®¢è§‚ç«‹åœº
            2. å¯è¯»æ€§ï¼šç¡®ä¿éæŠ€æœ¯è¯»è€…ä¹Ÿèƒ½ç†è§£
            3. ç»“æ„å®Œæ•´ï¼šæŒ‰ç…§ä¸‹é¢çš„ç»“æ„ç»„ç»‡å†…å®¹
            4. åˆ†ææ·±åº¦ï¼šåŠ å…¥æŠ€æœ¯åŸç†åˆ†æå’Œè¡Œä¸šå½±å“è¯„ä¼°
            5. å‰ç»æ€§ï¼šå¯¹æŠ€æœ¯å‘å±•è¶‹åŠ¿åšå‡ºé¢„æµ‹å’Œå±•æœ›
            6. æ•°æ®æ”¯æŒï¼šå¼•ç”¨å…·ä½“çš„æ•°æ®å’Œæ¡ˆä¾‹
            7. å¤šè§’åº¦ï¼šè€ƒè™‘æŠ€æœ¯ã€å•†ä¸šå’Œç¤¾ä¼šå½±å“

            æ–‡ç« ç»“æ„ï¼š
            1. æ ‡é¢˜ï¼šå¸å¼•äººä¸”ä¸“ä¸š
            2. å‰¯æ ‡é¢˜ï¼šè¡¥å……è¯´æ˜è¦ç‚¹
            3. å¯¼è¯­ï¼šæ¦‚æ‹¬æ ¸å¿ƒå†…å®¹ï¼ˆ3-4æ®µï¼‰
            4. æ­£æ–‡ï¼š
               - æŠ€æœ¯èƒŒæ™¯å’Œå‘å±•å†ç¨‹
               - æœ€æ–°çªç ´å’Œåˆ›æ–°ç‚¹
               - æŠ€æœ¯åŸç†è§£æ
               - åº”ç”¨åœºæ™¯åˆ†æ
               - è¡Œä¸šå½±å“è¯„ä¼°
               - æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ
               - ä¸“å®¶è§‚ç‚¹å’Œå¸‚åœºååº”
            5. è¶‹åŠ¿å±•æœ›ï¼šé¢„æµ‹æœªæ¥å‘å±•æ–¹å‘
            6. æ€»ç»“ï¼šæ ¸å¿ƒè§‚ç‚¹æç‚¼

            æ–°é—»ä¿¡æ¯ï¼š
            {json.dumps([{
                "title": article.get("title", ""),
                "summary": article.get("summary", ""),
                "content": article.get("content", "")[:1000] + "..."  
            } for article in articles_data["articles"]], indent=2, ensure_ascii=False)}
            """
            
            response = self.llm_model.invoke(prompt)
            content_lines = response.content.split('\n')
            
            article = TechNewsArticle(
                title=content_lines[0].strip('# ') if content_lines else "AIæŠ€æœ¯å‘å±•æœ€æ–°åŠ¨æ€",
                subtitle=content_lines[1].strip('## ') if len(content_lines) > 1 else "å¼€æºå¤§è¯­è¨€æ¨¡å‹é¢†åŸŸé‡è¦è¿›å±•åˆ†æ",
                author="AIè¶…å…ƒåŸŸ",
                date=datetime.now().strftime("%Y-%m-%d"),
                content=response.content,
                sources=sources
            )
            
            return article
            
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ–‡ç« æ—¶å‡ºé”™: {str(e)}")
            return TechNewsArticle(
                title="AIæŠ€æœ¯å‘å±•åŠ¨æ€",
                subtitle="å¼€æºå¤§è¯­è¨€æ¨¡å‹è¿›å±•ç»¼è¿°",
                author="AIè¶…å…ƒåŸŸ",
                date=datetime.now().strftime("%Y-%m-%d"),
                content="å†…å®¹ç”Ÿæˆå¤±è´¥",
                sources=[]
            )

    def save_results(self, articles_data: dict, tech_article: TechNewsArticle):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = "output"
            articles_dir = f"{output_dir}/articles"
            os.makedirs(articles_dir, exist_ok=True)
            
            # ä¿å­˜åŸå§‹æ–‡ç« æ•°æ®
            df = pd.DataFrame(articles_data["articles"])
            df.to_csv(f"{output_dir}/ai_articles.csv", index=False, encoding='utf-8')
            logging.info(f"åŸå§‹æ•°æ®å·²ä¿å­˜åˆ° {output_dir}/ai_articles.csv")
            
            # ä¿å­˜æ¯ç¯‡åŸå§‹æ–‡ç« 
            for idx, article in enumerate(articles_data["articles"]):
                # ä½¿ç”¨URLçš„å“ˆå¸Œä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                url_hash = hashlib.md5(article["link"].encode()).hexdigest()[:8]
                file_name = f"{articles_dir}/article_{idx+1}_{url_hash}.md"
                
                with open(file_name, 'w', encoding='utf-8') as f:
                    f.write(f"# {article.get('title', 'æ— æ ‡é¢˜')}\n\n")
                    f.write(f"ä½œè€…: {article.get('author', 'æœªçŸ¥')}\n")
                    f.write(f"å‘å¸ƒæ—¥æœŸ: {article.get('publish_date', 'æœªçŸ¥')}\n")
                    f.write(f"æ¥æº: {article.get('link', '')}\n\n")
                    if article.get('tags'):
                        f.write("æ ‡ç­¾: " + ", ".join(article['tags']) + "\n\n")
                    f.write("## æ‘˜è¦\n\n")
                    f.write(f"{article.get('summary', 'æ— æ‘˜è¦')}\n\n")
                    f.write("## æ­£æ–‡\n\n")
                    f.write(f"{article.get('content', 'æ— æ­£æ–‡å†…å®¹')}\n")
            
            # ä¿å­˜ç”Ÿæˆçš„ç§‘æŠ€èµ„è®¯æ–‡ç« 
            article_filename = f"{output_dir}/tech_article_{datetime.now().strftime('%Y%m%d')}.md"
            
            with open(article_filename, 'w', encoding='utf-8') as f:
                f.write(f"# {tech_article.title}\n\n")
                f.write(f"## {tech_article.subtitle}\n\n")
                f.write(f"ä½œè€…: {tech_article.author}\n")
                f.write(f"æ—¥æœŸ: {tech_article.date}\n\n")
                f.write(f"{tech_article.content}\n\n")
                f.write("\n## ä¿¡æ¯æ¥æº\n")
                for source in tech_article.sources:
                    f.write(f"- {source}\n")
                    
            logging.info(f"æ–‡ç« å·²ä¿å­˜åˆ° {article_filename}")
            logging.info(f"åŸå§‹æ–‡ç« å·²ä¿å­˜åˆ° {articles_dir} ç›®å½•")
            
        except Exception as e:
            logging.error(f"ä¿å­˜ç»“æœæ—¶å‡ºé”™: {str(e)}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºAIæ–°é—»ç”Ÿæˆå™¨å®ä¾‹
        generator = AINewsGenerator()
        
        # åˆå§‹åŒ–å·¥å…·
        tools = generator.initialize_tools()
        
        # åˆ›å»ºagent
        generator.create_agent(tools)
        
        # æœç´¢å’Œæå–æ–‡ç« 
        logging.info("å¼€å§‹æœç´¢å’Œæå–æ–‡ç« ...")
        articles_data = generator.search_and_extract()
        
        if not articles_data["articles"]:
            logging.warning("æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œè¯·æ£€æŸ¥æœç´¢æ¡ä»¶æˆ–ç½‘ç»œè¿æ¥")
            return
            
        # ç”Ÿæˆç§‘æŠ€èµ„è®¯æ–‡ç« 
        logging.info("å¼€å§‹ç”Ÿæˆç§‘æŠ€èµ„è®¯æ–‡ç« ...")
        tech_article = generator.generate_article(articles_data)
        
        # ä¿å­˜ç»“æœ
        generator.save_results(articles_data, tech_article)
        
        # æ˜¾ç¤ºæ–‡ç« ç»Ÿè®¡ä¿¡æ¯
        logging.info(f"\næ–‡ç« ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"- æ”¶é›†çš„æ–‡ç« æ•°é‡ï¼š{len(articles_data['articles'])}")
        print(f"- ç”Ÿæˆçš„æ–‡ç« æ ‡é¢˜ï¼š{tech_article.title}")
        print(f"- ä½œè€…ï¼š{tech_article.author}")
        print(f"- æ—¥æœŸï¼š{tech_article.date}")
        
        # æ˜¾ç¤ºæ–‡ç« é¢„è§ˆ
        print("\næ–‡ç« é¢„è§ˆï¼š")
        print("=" * 50)
        print(f"æ ‡é¢˜ï¼š{tech_article.title}")
        print(f"å‰¯æ ‡é¢˜ï¼š{tech_article.subtitle}")
        print("-" * 50)
        preview_length = 800  # é¢„è§ˆçš„å­—ç¬¦æ•°
        preview_content = tech_article.content[:preview_length]
        print(f"{preview_content}...")
        print("=" * 50)
        
        # æ˜¾ç¤ºä¿å­˜ä½ç½®ä¿¡æ¯
        logging.info("\næ–‡ä»¶ä¿å­˜ä¿¡æ¯ï¼š")
        print("- åŸå§‹æ•°æ®ï¼šoutput/ai_articles.csv")
        print("- ç§‘æŠ€èµ„è®¯ï¼šoutput/tech_article_*.md")
        print("- åŸæ–‡å­˜æ¡£ï¼šoutput/articles/")
        
    except Exception as e:
        logging.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {str(e)}")
        raise

if __name__ == "__main__":
    main()
```