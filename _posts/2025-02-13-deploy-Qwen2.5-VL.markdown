---
layout: single  
title: "ðŸš€vLLMæœ¬åœ°éƒ¨ç½²Qwen2.5-VLå¤šæ¨¡æ€å¤§æ¨¡åž‹ï¼70äº¿å‚æ•°å³å¯æ‰“é€ ç›‘æŽ§è§†é¢‘ç›®æ ‡æŸ¥æ‰¾é¡¹ç›®ï¼è½»æ¾å®žçŽ°ç›‘æŽ§è§†é¢‘è‡ªåŠ¨æ‰¾äººï¼RTX A6000æ˜¾å¡éƒ¨ç½²Qwen2.5-VL-7B-Instructæ¨¡åž‹å®žæˆ˜æ•™ç¨‹"  
sidebar:
  nav: "docs"
date: 2025-02-13 10:00:00 +0800  
categories: LLMs
tags: [Qwen2.5-VL, Qwen2.5-VL-7B-Instruct, Qwen, multimoda]
classes: wide  

author_profile: true  
---


Qwen2.5-VLæ˜¯é˜¿é‡Œäº‘å¼€å‘çš„æœ€æ–°å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼Œå®ƒåœ¨è§†è§‰ç†è§£ã€æ–‡æ¡£å¤„ç†å’Œå¤šæ¨¡æ€äº¤äº’æ–¹é¢è¡¨çŽ°å‡ºè‰²ã€‚åœ¨è§†è§‰ç†è§£æ–¹é¢ï¼ŒQwen2.5-VLèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å›¾åƒå’Œè§†é¢‘ä¸­çš„ç‰©ä½“ã€åœºæ™¯å’Œå…³ç³»ï¼Œå¹¶è¿›è¡Œæè¿°ã€‚åœ¨æ–‡æ¡£å¤„ç†æ–¹é¢ï¼Œå®ƒæ“…é•¿æå–å’Œåˆ†æžå„ç§ç±»åž‹çš„æ–‡æ¡£ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡å­—ã€è¡¨æ ¼å’Œå›¾ç‰‡ã€‚Qwen2.5-VLè¿˜æ”¯æŒè‡ªç„¶çš„å¤šæ¨¡æ€äº¤äº’ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ–‡å­—ã€è¯­éŸ³æˆ–å›¾åƒç­‰æ–¹å¼ä¸Žæ¨¡åž‹è¿›è¡Œäº¤æµï¼Œå®žçŽ°äººæœºååŒã€‚è¿™æ¬¾æ¨¡åž‹åœ¨æ™ºèƒ½å®¢æœã€æ•™è‚²ã€é‡‘èžã€åŒ»ç–—å’Œé›¶å”®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

### **æœ¬ç¯‡ç¬”è®°æ‰€å¯¹åº”çš„è§†é¢‘ï¼š**

- [ðŸ‘‰ðŸ‘‰ðŸ‘‰ é€šè¿‡å“”å“©å“”å“©è§‚çœ‹](https://b23.tv/zB3kDkb)
- [ðŸ‘‰ðŸ‘‰ðŸ‘‰ é€šè¿‡YouTubeè§‚çœ‹](https://youtu.be/Z05-JuG1a68)
- [ðŸ‘‰ðŸ‘‰ðŸ‘‰ æˆ‘çš„å¼€æºé¡¹ç›®](https://github.com/win4r/AISuperDomain)
- [ðŸ‘‰ðŸ‘‰ðŸ‘‰ è¯·æˆ‘å–å’–å•¡](https://ko-fi.com/aila)
- ðŸ‘‰ðŸ‘‰ðŸ‘‰ æˆ‘çš„å¾®ä¿¡ï¼šstoeng ã€åŠ æˆ‘è¯·æ³¨æ˜Žæ¥æ„ã€‘
- ðŸ‘‰ðŸ‘‰ðŸ‘‰ æ‰¿æŽ¥å¤§æ¨¡åž‹å¾®è°ƒã€RAGã€AIæ™ºèƒ½ä½“ã€AIç›¸å…³åº”ç”¨å¼€å‘ç­‰é¡¹ç›®ã€‚

### ðŸš€ç®€ä»‹ï¼š

Qwen2.5-VL æ˜¯ç”±é˜¿é‡Œäº‘é€šä¹‰åƒé—®å›¢é˜Ÿå¼€å‘çš„æœ€æ–°ä¸€ä»£å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹ã€‚å®ƒåœ¨è§†è§‰ç†è§£ã€æ–‡æ¡£å¤„ç†å’Œå¤šæ¨¡æ€äº¤äº’æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå…·å¤‡ä»¥ä¸‹ä¸»è¦ç‰¹ç‚¹ï¼š

1. **æ›´å¼ºå¤§çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼š** Qwen2.5-VL èƒ½å¤Ÿå¤„ç†å›¾åƒå’Œè§†é¢‘ç­‰å¤šæ¨¡æ€è¾“å…¥ï¼Œå‡†ç¡®ç†è§£å…¶ä¸­çš„å†…å®¹å’Œå…³ç³»ã€‚å®ƒåœ¨ç‰©ä½“æ£€æµ‹ã€åœºæ™¯è¯†åˆ«ã€å›¾åƒæè¿°ç­‰ä»»åŠ¡ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œå¯ä»¥åº”ç”¨äºŽæ™ºèƒ½å®‰é˜²ã€å›¾åƒæœç´¢ã€è§†é¢‘åˆ†æžç­‰é¢†åŸŸã€‚
2. **é«˜æ•ˆçš„æ–‡æ¡£å¤„ç†èƒ½åŠ›ï¼š** Qwen2.5-VL æ“…é•¿å¤„ç†å„ç§ç±»åž‹çš„æ–‡æ¡£ï¼ŒåŒ…æ‹¬æ‰«æä»¶ã€ç½‘é¡µã€PDF ç­‰ã€‚å®ƒèƒ½å¤Ÿæå–æ–‡æ¡£ä¸­çš„æ–‡å­—ã€è¡¨æ ¼ã€å›¾ç‰‡ç­‰ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œç»“æž„åŒ–å¤„ç†ï¼Œæ–¹ä¾¿ç”¨æˆ·è¿›è¡Œä¿¡æ¯æ£€ç´¢ã€å†…å®¹æ‘˜è¦å’Œæ™ºèƒ½é—®ç­”ã€‚
3. **è‡ªç„¶çš„å¤šæ¨¡æ€äº¤äº’ä½“éªŒï¼š** Qwen2.5-VL æ”¯æŒè‡ªç„¶è¯­è¨€å’Œè§†è§‰ä¿¡æ¯çš„æ··åˆè¾“å…¥ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ–‡å­—ã€è¯­éŸ³æˆ–å›¾åƒç­‰æ–¹å¼ä¸Žæ¨¡åž‹è¿›è¡Œäº¤äº’ã€‚å®ƒèƒ½å¤Ÿç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„å›žå¤æˆ–æ‰§è¡Œç›¸åº”çš„æ“ä½œï¼Œå®žçŽ°äººæœºååŒå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰åº”ç”¨ã€‚
4. **å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼š** Qwen2.5-VL åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼š
    - æ™ºèƒ½å®¢æœï¼šé€šè¿‡ç†è§£ç”¨æˆ·çš„æ–‡å­—å’Œå›¾åƒè¾“å…¥ï¼Œæä¾›ä¸ªæ€§åŒ–çš„å®¢æˆ·æœåŠ¡ã€‚
    - æ•™è‚²ï¼šè¾…åŠ©æ•™å¸ˆè¿›è¡Œè¯¾ä»¶åˆ¶ä½œã€ä½œä¸šæ‰¹æ”¹å’Œå­¦ç”Ÿè¾…å¯¼ã€‚
    - é‡‘èžï¼šå¤„ç†è´¢åŠ¡æŠ¥è¡¨ã€åˆ†æžå¸‚åœºè¶‹åŠ¿ã€è¾…åŠ©æŠ•èµ„å†³ç­–ã€‚
    - åŒ»ç–—ï¼šè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œç—…ç†åˆ†æžã€è¯Šæ–­å’Œæ²»ç–—æ–¹æ¡ˆåˆ¶å®šã€‚
    - é›¶å”®ï¼šä¼˜åŒ–å•†å“æŽ¨èã€æå‡ç”¨æˆ·è´­ç‰©ä½“éªŒã€‚

âœ…å›½å¤–ä¸‹è½½ï¼š[https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)

âœ…å›½å†…ä¸‹è½½ï¼š[https://modelscope.cn/models/Qwen/Qwen2.5-VL-7B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-VL-7B-Instruct)

### ðŸš€æœ¬åœ°éƒ¨ç½²

```bash
# åˆ›å»ºçŽ¯å¢ƒ
conda create -n myenv python=3.11 -y

# æ¿€æ´»çŽ¯å¢ƒ (æ³¨æ„è¿™é‡Œçš„è¯­æ³•)
conda activate myenv   # æˆ– source activate myenv (åœ¨æŸäº›ç³»ç»Ÿä¸Š)

# å®‰è£…ä¾èµ–
pip install vllm
pip install git+https://github.com/huggingface/transformers
pip install torch

# æˆ– pip install git+https://github.com/huggingface/transformers 

# å¯åŠ¨æœåŠ¡
vllm serve Qwen/Qwen2.5-VL-7B-Instruct --limit-mm-per-prompt image=4
```

### ðŸš€æµ‹è¯•ä»£ç 

```python
#pip install openai

from openai import OpenAI

# æ­£ç¡®åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(
    base_url="http://64.247.196.79:8000/v1",
    api_key="test"
)

response = client.chat.completions.create(
  model="Qwen/Qwen2.5-VL-7B-Instruct",
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What's in this image?"},
        {
          "type": "image_url",
          "image_url": {
            "url": "https://s3.amazonaws.com/cms.ipressroom.com/338/files/201808/5b894ee1a138352221103195_A680%7Ejogging-edit/A680%7Ejogging-edit_hero.jpg",
          },
        },
      ],
    }
  ],
  max_tokens=1024,
)

print(response.choices[0].message.content)

import base64
from openai import OpenAI

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(
    base_url="http://64.247.196.79:8000/v1",
    api_key="test"
)

# æœ¬åœ°å›¾ç‰‡è·¯å¾„
image_path = "./dog.jpg"

# ç¼–ç å›¾ç‰‡
base64_image = encode_image(image_path)

response = client.chat.completions.create(
  model="Qwen/Qwen2.5-VL-7B-Instruct",
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What's in this image?"},
        {
          "type": "image_url",
          "image_url": {
            "url": f"data:image/jpeg;base64,{base64_image}",
          },
        },
      ],
    }
  ],
  max_tokens=1024,
)

print(response.choices[0])
```

### ðŸš€è§†é¢‘ç›®æ ‡æ£€æµ‹ä»£ç 

main.py

```python
#å®‰è£…ä¾èµ– pip install fastapi uvicorn python-multipart jinja2 opencv-python openai aiofiles

# main.py
from fastapi import FastAPI, UploadFile, File, Form, Request
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse, StreamingResponse
import shutil
import os
import cv2
from openai import OpenAI
import time
from pathlib import Path
import asyncio
import json
import base64

app = FastAPI()

# åˆ›å»ºå¿…è¦çš„ç›®å½•
UPLOAD_DIR = Path("uploads")
FRAMES_DIR = Path("frames")
UPLOAD_DIR.mkdir(exist_ok=True)
FRAMES_DIR.mkdir(exist_ok=True)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(
    base_url="http://192.168.1.105:8000/v1",
    api_key="test"
)

# è®¾ç½®æ¨¡æ¿å’Œä¸Šä¼ /å¸§ç›®å½•çš„é™æ€æ–‡ä»¶æœåŠ¡
templates = Jinja2Templates(directory="templates")
app.mount("/uploads", StaticFiles(directory="uploads"), name="uploads")
app.mount("/frames", StaticFiles(directory="frames"), name="frames")

def encode_image(image_path):
    """å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64ç¼–ç """
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

async def analyze_image(image_path: str, object_str: str):
    """å¼‚æ­¥ç‰ˆæœ¬çš„å›¾åƒåˆ†æžå‡½æ•°"""
    prompt_str = f"""Please analyze the image and answer the following questions:
    1. Is there a {object_str} in the image?
    2. If yes, describe its appearance and location in the image in detail.
    3. If no, describe what you see in the image instead.
    4. On a scale of 1-10, how confident are you in your answer?

    Please structure your response as follows:
    Answer: [YES/NO]
    Description: [Your detailed description]
    Confidence: [1-10]"""

    try:
        # ç¼–ç å›¾ç‰‡
        base64_image = encode_image(image_path)

        response = await asyncio.to_thread(
            client.chat.completions.create,
            model="Qwen/Qwen2.5-VL-7B-Instruct",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt_str},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}",
                            },
                        },
                    ],
                }
            ],
            max_tokens=1024,
        )

        response_text = response.choices[0].message.content
        response_lines = response_text.strip().split('\n')

        answer = None
        description = None
        confidence = 10

        for line in response_lines:
            line = line.strip()
            if line.lower().startswith('answer:'):
                answer = line.split(':', 1)[1].strip().upper()
            elif any(line.lower().startswith(prefix) for prefix in
                     ['description:', 'reasoning:', 'alternative description:']):
                description = line.split(':', 1)[1].strip()
            elif line.lower().startswith('confidence:'):
                try:
                    confidence = int(line.split(':', 1)[1].strip())
                except ValueError:
                    confidence = 10

        return answer == "YES" and confidence >= 7, description, confidence
    except Exception as e:
        print(f"Error during image analysis: {str(e)}")
        return False, "Error occurred", 0

def preprocess_image(image_path):
    """å›¾åƒé¢„å¤„ç†å‡½æ•°"""
    img = cv2.imread(image_path)
    if img is None:
        return False

    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    cl = clahe.apply(l)
    limg = cv2.merge((cl, a, b))
    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)
    cv2.imwrite(image_path, final, [cv2.IMWRITE_JPEG_QUALITY, 100])
    return True

@app.get("/")
async def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/analyze")
async def analyze_video(
        video: UploadFile = File(...),
        object_str: str = Form(...)
):
    try:
        # ä¿å­˜ä¸Šä¼ çš„è§†é¢‘
        video_path = UPLOAD_DIR / video.filename
        with open(video_path, "wb") as buffer:
            shutil.copyfileobj(video.file, buffer)

        # ä¸ºå½“å‰ä»»åŠ¡åˆ›å»ºä¸“é—¨çš„å¸§ç›®å½•
        task_frames_dir = FRAMES_DIR / video.filename.split('.')[0]
        task_frames_dir.mkdir(exist_ok=True)

        # å¼‚æ­¥ç”Ÿæˆåˆ†æžç»“æžœ
        async def generate_results():
            cap = cv2.VideoCapture(str(video_path))
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            frame_count = 0
            consecutive_detections = 0  # è¿žç»­æ£€æµ‹è®¡æ•°
            first_detection_second = None  # è®°å½•ç¬¬ä¸€æ¬¡æ£€æµ‹æ—¶é—´

            try:
                while True:
                    success, frame = cap.read()
                    if not success:
                        break

                    if frame_count % fps == 0:  # æ¯ç§’å¤„ç†ä¸€å¸§
                        current_second = frame_count // fps
                        frame_path = os.path.join(task_frames_dir, f"frame_{current_second}.jpg")
                        cv2.imwrite(frame_path, frame)

                        if preprocess_image(frame_path):
                            is_match, description, confidence = await analyze_image(frame_path, object_str)

                            if is_match:
                                consecutive_detections += 1
                                if consecutive_detections == 1:
                                    first_detection_second = current_second
                            else:
                                consecutive_detections = 0
                                first_detection_second = None

                            result = {
                                "status": "success",
                                "frame": {
                                    "second": current_second,
                                    "is_match": is_match,
                                    "description": description,
                                    "confidence": confidence,
                                    "frame_path": f"/frames/{video.filename.split('.')[0]}/frame_{current_second}.jpg"
                                }
                            }

                            yield json.dumps(result) + "\n"

                            # å¦‚æžœè¿žç»­ä¸¤æ¬¡æ£€æµ‹åˆ°ç›®æ ‡ï¼Œè¾“å‡ºç»“æžœå¹¶åœæ­¢
                            if consecutive_detections >= 2:
                                final_result = {
                                    "status": "complete",
                                    "message": f"ç›®æ ‡å·²è¿žç»­æ£€æµ‹åˆ°ä¸¤æ¬¡ï¼Œé¦–æ¬¡æ£€æµ‹æ—¶é—´ä¸ºç¬¬ {first_detection_second} ç§’",
                                    "first_detection_time": first_detection_second
                                }
                                yield json.dumps(final_result) + "\n"
                                break

                    frame_count += 1

            finally:
                cap.release()

        return StreamingResponse(generate_results(), media_type="application/json")

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
```

index.html(æ”¾å…¥templatesæ–‡ä»¶å¤¹é‡Œ)

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Video Analysis</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/tailwindcss/2.2.19/tailwind.min.css" rel="stylesheet">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap');

        body {
            font-family: 'Inter', sans-serif;
            background-color: #f3f4f6;
        }

        .upload-area {
            border: 2px dashed #e5e7eb;
            transition: all 0.3s ease;
            background: rgba(255, 255, 255, 0.8);
            backdrop-filter: blur(10px);
        }

        .upload-area:hover {
            border-color: #6366f1;
            background: rgba(255, 255, 255, 0.9);
            transform: translateY(-2px);
        }

        .analyzing {
            animation: pulse 2s infinite;
        }

        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.5; }
            100% { opacity: 1; }
        }

        .gradient-bg {
            background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%);
        }

        .glass-card {
            background: rgba(255, 255, 255, 0.8);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .results-container::-webkit-scrollbar {
            width: 8px;
        }

        .results-container::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 4px;
        }

        .results-container::-webkit-scrollbar-thumb {
            background: #c7d2fe;
            border-radius: 4px;
        }

        .results-container::-webkit-scrollbar-thumb:hover {
            background: #818cf8;
        }
    </style>
</head>
<body class="min-h-screen py-8">
    <div class="max-w-4xl mx-auto px-4">
        <div class="glass-card rounded-2xl shadow-xl overflow-hidden">
            <div class="gradient-bg px-6 py-4">
                <h1 class="text-2xl font-semibold text-white">Video Object Detection</h1>
                <p class="text-indigo-100 text-sm mt-1">Upload a video and find specific objects within it</p>
            </div>

            <div class="p-6">
                <form id="uploadForm" class="space-y-6">
                    <div>
                        <label class="block text-sm font-medium text-gray-700 mb-2">Upload Video</label>
                        <div class="upload-area rounded-xl p-8 cursor-pointer" id="uploadArea">
                            <div id="uploadText" class="text-center">
                                <svg class="mx-auto h-12 w-12 text-gray-400" stroke="currentColor" fill="none" viewBox="0 0 48 48">
                                    <path d="M28 8H12a4 4 0 00-4 4v20m32-12v8m0 0v8a4 4 0 01-4 4H12a4 4 0 01-4-4v-4m32-4l-3.172-3.172a4 4 0 00-5.656 0L28 28M8 32l9.172-9.172a4 4 0 015.656 0L28 28m0 0l4-4m4-4h.01" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
                                </svg>
                                <p class="mt-4 text-sm text-gray-600">Click or drag video to upload</p>
                            </div>
                            <input type="file" id="videoInput" accept="video/*" class="hidden">
                        </div>
                        <p class="mt-2 text-sm text-gray-500" id="fileInfo"></p>
                    </div>

                    <div>
                        <label for="objectInput" class="block text-sm font-medium text-gray-700 mb-2">Object to Find</label>
                        <input type="text"
                               class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-indigo-500 focus:border-indigo-500 transition-all"
                               id="objectInput"
                               placeholder="e.g., 'a person wearing red shirt'">
                    </div>

                    <div class="alert-danger hidden rounded-lg bg-red-50 p-4 text-red-700" id="errorAlert"></div>

                    <button type="submit"
                            class="w-full bg-indigo-600 text-white py-2 px-4 rounded-lg hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2 transition-all"
                            id="analyzeBtn">
                        Start Analysis
                    </button>
                </form>

                <div id="statusText" class="text-center mt-6 hidden">
                    <div class="inline-flex items-center px-4 py-2 bg-indigo-100 text-indigo-700 rounded-full analyzing">
                        <svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-indigo-700" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                            <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                            <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                        </svg>
                        Analyzing video...
                    </div>
                </div>

                <div class="results-container mt-6 space-y-4 max-h-[600px] overflow-y-auto pr-2" id="resultsContainer"></div>
            </div>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const uploadArea = document.getElementById('uploadArea');
            const videoInput = document.getElementById('videoInput');
            const uploadForm = document.getElementById('uploadForm');
            const analyzeBtn = document.getElementById('analyzeBtn');
            const statusText = document.getElementById('statusText');
            const errorAlert = document.getElementById('errorAlert');
            const resultsContainer = document.getElementById('resultsContainer');
            const fileInfo = document.getElementById('fileInfo');

            uploadArea.addEventListener('dragover', (e) => {
                e.preventDefault();
                uploadArea.classList.add('border-indigo-500', 'bg-indigo-50');
            });

            uploadArea.addEventListener('dragleave', (e) => {
                e.preventDefault();
                uploadArea.classList.remove('border-indigo-500', 'bg-indigo-50');
            });

            uploadArea.addEventListener('drop', (e) => {
                e.preventDefault();
                uploadArea.classList.remove('border-indigo-500', 'bg-indigo-50');
                const files = e.dataTransfer.files;
                if (files.length) {
                    videoInput.files = files;
                    updateFileInfo(files[0]);
                }
            });

            uploadArea.addEventListener('click', () => {
                videoInput.click();
            });

            videoInput.addEventListener('change', (e) => {
                if (e.target.files.length) {
                    updateFileInfo(e.target.files[0]);
                }
            });

            function updateFileInfo(file) {
                fileInfo.textContent = `Selected file: ${file.name}`;
                fileInfo.classList.add('text-indigo-600');
            }

            function showError(message) {
                errorAlert.textContent = message;
                errorAlert.classList.remove('hidden');
            }

            function displayFrame(frame) {
                const frameEl = document.createElement('div');
                frameEl.className = 'glass-card rounded-xl overflow-hidden';
                frameEl.innerHTML = `
                    <div class="p-4 border-b border-gray-100">
                        <h3 class="font-medium text-gray-900">Frame at ${frame.second} seconds</h3>
                    </div>
                    <div class="p-4">
                        <img src="${frame.frame_path}" alt="Frame ${frame.second}" class="w-full rounded-lg">
                        <p class="mt-4 text-gray-700">${frame.description || 'No description available'}</p>
                        <div class="mt-4 flex items-center justify-between text-sm text-gray-500">
                            <span>Confidence: ${frame.confidence}/10</span>
                            <span class="px-2 py-1 rounded-full ${frame.is_match ? 'bg-green-100 text-green-700' : 'bg-gray-100 text-gray-700'}">
                                ${frame.is_match ? 'Match' : 'No Match'}
                            </span>
                        </div>
                    </div>
                `;
                resultsContainer.insertBefore(frameEl, resultsContainer.firstChild);
            }

            uploadForm.addEventListener('submit', async (e) => {
                e.preventDefault();

                const video = videoInput.files[0];
                const objectStr = document.getElementById('objectInput').value;

                if (!video || !objectStr) {
                    showError('Please provide both video file and object description');
                    return;
                }

                try {
                    errorAlert.classList.add('hidden');
                    statusText.classList.remove('hidden');
                    analyzeBtn.disabled = true;
                    analyzeBtn.classList.add('opacity-50', 'cursor-not-allowed');
                    resultsContainer.innerHTML = '';

                    const formData = new FormData();
                    formData.append('video', video);
                    formData.append('object_str', objectStr);

                    const response = await fetch('/analyze', {
                        method: 'POST',
                        body: formData
                    });

                    const reader = response.body.getReader();
                    const decoder = new TextDecoder();

                    while (true) {
                        const {value, done} = await reader.read();
                        if (done) break;

                        const text = decoder.decode(value);
                        const results = text.split('\n').filter(line => line.trim());

                        for (const result of results) {
                            try {
                                const data = JSON.parse(result);
                                if (data.status === 'success' && data.frame) {
                                    displayFrame(data.frame);
                                }
                            } catch (e) {
                                console.error('Error parsing result:', e);
                            }
                        }
                    }
                } catch (error) {
                    showError('An error occurred during analysis');
                    console.error('Error:', error);
                } finally {
                    statusText.classList.add('hidden');
                    analyzeBtn.disabled = false;
                    analyzeBtn.classList.remove('opacity-50', 'cursor-not-allowed');
                }
            });
        });
    </script>
</body>
</html>
```