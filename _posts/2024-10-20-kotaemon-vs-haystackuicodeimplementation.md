---
layout: single
title: "kotaemon vs Haystackï¼Œä»UIç•Œé¢åˆ°ä»£ç å®ç°ï¼Œä¼ä¸šçº§åº”ç”¨æ›´å¾—å¿ƒåº”æ‰‹"
sidebar:
  nav: "docs"
date: 2024-10-20 00:00:00 +0800
categories: [Fine-Tuning, LLM, RAG]
tags: [AI, GPT, GPT-4, LLM, RAG]
classes: wide
author_profile: true
---


#  **kotaemon vs Haystackï¼Œä»UIç•Œé¢åˆ°ä»£ç å®ç°ï¼Œä¼ä¸šçº§åº”ç”¨æ›´å¾—å¿ƒåº”æ‰‹**

[ https://github.com/Cinnamon/kotaemon/tree/main ](<https://github.com/Cinnamon/kotaemon/tree/main>)

###  ğŸ”¥å®‰è£… **kotaemon**
    
    
```
    docker run \
    -e GRADIO_SERVER_NAME=0.0.0.0 \
    -e GRADIO_SERVER_PORT=7860 \
    -p 7860:7860 -it --rm \
    --platform linux/arm64 \
    ghcr.io/cinnamon/kotaemon:main-lite
```
    
    
    # Create a virtual environment
    python3.10 -m venv kotaemon_env
    
```
    # Activate the virtual environment
    # On Unix or MacOS, use:
    source kotaemon_env/bin/activate
    # On Windows, use:
    # kotaemon_env\Scripts\activate
```
    
```
    # Clone the repository
    git clone https://github.com/Cinnamon/kotaemon
    cd kotaemon
```
    
```
    # Install kotaemon and its dependencies
    pip install -e "libs/kotaemon[all]"
    pip install -e "libs/ktem"
```
    
    # When you're done, you can deactivate the virtual environment
    # deactivate
    

##  **ğŸ‘‰ğŸ‘‰ğŸ‘‰å¦‚æœ‰é—®é¢˜æˆ–è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng**

##  **ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡**

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰æˆ‘çš„å¼€æºé¡¹ç›®** **[ https://github.com/win4r/AISuperDomain ](<https://github.com/win4r/AISuperDomain>) **

###  **ğŸ”¥kotaemonä»‹ç»**

###  ğŸš€Kotaemon æ˜¯ä¸€ä¸ªå¼€æºçš„é¡¹ç›®ï¼Œä¸“æ³¨äºæ„å»ºå’Œä½¿ç”¨åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ç®¡é“ï¼Œä¸»è¦ç”¨äºæ–‡æ¡£çš„é—®ç­”ä»»åŠ¡ã€‚å®ƒæä¾›äº†ä¸€ä¸ªç®€æ´çš„ç”¨æˆ·ç•Œé¢ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡æ··åˆæ£€ç´¢ï¼ˆå…¨æ–‡å’Œå‘é‡æ£€ç´¢ç›¸ç»“åˆï¼‰åœ¨æ–‡æ¡£ä¸­è¿›è¡Œé«˜æ•ˆæŸ¥è¯¢ï¼Œç¡®ä¿è¿”å›ç»“æœçš„é«˜ç›¸å…³æ€§ã€‚ 

###  ğŸš€Kotaemon æ”¯æŒå¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒåŒ…æ‹¬æ¥è‡ª OpenAI å’Œ Azure ç­‰æä¾›å•†çš„ APIï¼Œä»¥åŠä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚ llama-cpp-pythonï¼‰çš„èƒ½åŠ›ã€‚å…¶ä¸»è¦åŠŸèƒ½åŒ…æ‹¬æ”¯æŒå¤šç”¨æˆ·ç™»å½•ã€ç®¡ç†ç§å¯†å’Œå…¬å…±æ–‡æ¡£é›†åˆï¼Œå¹¶ä¸”èƒ½å¤Ÿç»„ç»‡å’Œç®¡ç†å„ç§ LLM å’ŒåµŒå…¥æ¨¡å‹ã€‚ 

###  ğŸš€å¯¹äºå¼€å‘è€…ï¼ŒKotaemon æä¾›äº†ä¸€ä¸ªçµæ´»çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿè½»æ¾æ„å»ºè‡ªå®šä¹‰çš„ RAG ç®¡é“ï¼Œä¸”é›†æˆäº†åŸºäº Gradio çš„äº¤äº’å¼ UIï¼Œæ”¯æŒè¿›ä¸€æ­¥çš„è‡ªå®šä¹‰ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒå¤æ‚æ¨ç†åŠŸèƒ½ï¼Œå¦‚é—®é¢˜åˆ†è§£ã€å¤šè·³é—®é¢˜æ¨ç†ç­‰ï¼Œå¹¶å¯é€šè¿‡ Docker å®¹å™¨éƒ¨ç½²ï¼Œç®€åŒ–å®‰è£…å’Œæ‰©å±•ã€‚ 

###  ğŸ”¥Haystackä»‹ç» 

###  ğŸš€Haystack æ˜¯ä¸€ä¸ªå¼€æºçš„ AI åä½œæ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼ä¸šçº§åº”ç”¨ç¨‹åºã€‚è¯¥æ¡†æ¶éå¸¸é€‚åˆæ„å»ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€é—®ç­”ç³»ç»Ÿã€è¯­ä¹‰æœç´¢å’ŒèŠå¤©æœºå™¨äººç­‰åº”ç”¨ã€‚ 

###  ğŸš€ä¸»è¦ç‰¹ç‚¹ï¼š 

  1. **æ¨¡å—åŒ–ç®¡é“** ï¼šHaystack é€šè¿‡ç»„åˆä¸åŒçš„ç»„ä»¶ï¼Œå¦‚æ£€ç´¢å™¨ã€ç”Ÿæˆå™¨å’Œæ–‡æ¡£å­˜å‚¨ï¼Œå¸®åŠ©å¼€å‘è€…æ„å»ºå¤æ‚çš„ AI ç®¡é“ã€‚æ¯ä¸ªç»„ä»¶å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚è¿›è¡Œè‡ªå®šä¹‰é…ç½®ã€‚ 


  2. **æŠ€æœ¯æ— å…³æ€§** ï¼šHaystack æ”¯æŒå¤šç§æŠ€æœ¯å’Œæ¨¡å‹é›†æˆï¼ŒåŒ…æ‹¬ OpenAIã€Cohereã€Hugging Face ç­‰ã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®ä¸åŒçš„éœ€æ±‚åœ¨å„ç§æ¨¡å‹å’Œæ•°æ®åº“ä¹‹é—´çµæ´»åˆ‡æ¢ã€‚ 


  3. **æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) å’Œè¯­ä¹‰æœç´¢** ï¼šHaystack æ“…é•¿ RAGï¼Œæ£€ç´¢å™¨ä¼šæ ¹æ®æŸ¥è¯¢è·å–ç›¸å…³æ–‡æ¡£ï¼Œè€Œ LLM åˆ™ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„ç­”æ¡ˆã€‚è¿™ä¸ªç‰¹æ€§åœ¨å®¢æˆ·æ”¯æŒã€çŸ¥è¯†åº“ç­‰éœ€è¦ç²¾ç¡®æ–‡æ¡£æ£€ç´¢å’Œç”Ÿæˆç­”æ¡ˆçš„åœºæ™¯ä¸­éå¸¸æœ‰ç”¨ã€‚ 


  4. **æ–‡æ¡£å­˜å‚¨** ï¼šHaystack æ”¯æŒå¤šç§æ–‡æ¡£å­˜å‚¨é€‰é¡¹ï¼Œå¦‚ Elasticsearchã€FAISS å’Œ InMemoryDocumentStoreï¼Œæ–¹ä¾¿ç®¡ç†å’Œå¿«é€Ÿæ£€ç´¢å¤§é‡æ•°æ®ã€‚ 


  5. **å¯æ‰©å±•æ€§** ï¼šæ¡†æ¶æ”¯æŒå¼€å‘è€…åˆ›å»ºè‡ªå®šä¹‰ç»„ä»¶æˆ–ä¸ç¬¬ä¸‰æ–¹æœåŠ¡é›†æˆï¼Œè¿™ä½¿å¾— Haystack é€‚ç”¨äºç ”ç©¶å’Œä¼ä¸šçº§åº”ç”¨ã€‚ 


###  Haystackä»£ç  
    
    
    ! pip install --upgrade --quiet pymilvus milvus-haystack markdown-it-py mdit_plain
    
    import os
    
    from google.colab import userdata
    
    
    os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
    
    
    import os
    import urllib.request
    
    url = "https://raw.githubusercontent.com/win4r/mytest/refs/heads/main/book.txt"
    file_path = "./davinci.txt"
    
    if not os.path.exists(file_path):
        urllib.request.urlretrieve(url, file_path)
        
        
```
    from haystack import Pipeline
    from haystack.components.converters import MarkdownToDocument
    from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder
    from haystack.components.preprocessors import DocumentSplitter
    from haystack.components.writers import DocumentWriter
    from haystack.utils import Secret
```
    
    from milvus_haystack import MilvusDocumentStore
    from milvus_haystack.milvus_embedding_retriever import MilvusEmbeddingRetriever
    
    
```
    document_store = MilvusDocumentStore(
        connection_args={"uri": "./milvus.db"},
        # connection_args={"uri": "http://localhost:19530"},
        # connection_args={"uri": YOUR_ZILLIZ_CLOUD_URI, "token": Secret.from_env_var("ZILLIZ_CLOUD_API_KEY")},
        drop_old=True,
    )
```
    
    
```
    indexing_pipeline = Pipeline()
    indexing_pipeline.add_component("converter", MarkdownToDocument())
    indexing_pipeline.add_component(
        "splitter", DocumentSplitter(split_by="sentence", split_length=2)
    )
    indexing_pipeline.add_component("embedder", OpenAIDocumentEmbedder())
    indexing_pipeline.add_component("writer", DocumentWriter(document_store))
    indexing_pipeline.connect("converter", "splitter")
    indexing_pipeline.connect("splitter", "embedder")
    indexing_pipeline.connect("embedder", "writer")
    indexing_pipeline.run({"converter": {"sources": [file_path]}})
```
    
    print("Number of documents:", document_store.count_documents())
    
    
    question = 'Provide some examples of Seed-word Prompts'
    
```
    retrieval_pipeline = Pipeline()
    retrieval_pipeline.add_component("embedder", OpenAITextEmbedder())
    retrieval_pipeline.add_component(
        "retriever", MilvusEmbeddingRetriever(document_store=document_store, top_k=3)
    )
    retrieval_pipeline.connect("embedder", "retriever")
```
    
    retrieval_results = retrieval_pipeline.run({"embedder": {"text": question}})
    
```
    for doc in retrieval_results["retriever"]["documents"]:
        print(doc.content)
        print("-" * 10)
```
        
```
    from haystack.utils import Secret
    from haystack.components.builders import PromptBuilder
    from haystack.components.generators import OpenAIGenerator
```
    
```
    prompt_template = """Answer the following query based on the provided context. If the context does
                         not include an answer, reply with 'I don't know'.\n
                         Query: {{query}}
                         Documents:
                         {% for doc in documents %}
                            {{ doc.content }}
                         {% endfor %}
                         Answer:
                      """
```
    
```
    rag_pipeline = Pipeline()
    rag_pipeline.add_component("text_embedder", OpenAITextEmbedder())
    rag_pipeline.add_component(
        "retriever", MilvusEmbeddingRetriever(document_store=document_store, top_k=3)
    )
    rag_pipeline.add_component("prompt_builder", PromptBuilder(template=prompt_template))
    rag_pipeline.add_component(
        "generator",
        OpenAIGenerator(
            model="gpt-4o-mini",
            api_key=Secret.from_token(os.environ["OPENAI_API_KEY"]),
            generation_kwargs={"temperature": 0},
        ),
    )
    rag_pipeline.connect("text_embedder.embedding", "retriever.query_embedding")
    rag_pipeline.connect("retriever.documents", "prompt_builder.documents")
    rag_pipeline.connect("prompt_builder", "generator")
```
    
```
    results = rag_pipeline.run(
        {
            "text_embedder": {"text": question},
            "prompt_builder": {"query": question},
        }
    )
    print("RAG answer:", results["generator"]["replies"][0])
```
    
    
    !pip install deepeval-haystack
    
    
```
    from haystack import Pipeline
    from haystack_integrations.components.evaluators.deepeval import DeepEvalEvaluator, DeepEvalMetric
    from tabulate import tabulate
```
    
    
```
    pipeline = Pipeline()
    evaluator = DeepEvalEvaluator(
        metric=DeepEvalMetric.FAITHFULNESS,
        metric_params={"model": "gpt-3.5-turbo-1106"},
    )
    pipeline.add_component("evaluator", evaluator)
```
    
    
    
```
    results = pipeline.run({
        "evaluator": {
            "questions": [
                "When was the Rhodes Statue built?",
                "Where is the Pyramid of Giza located?",
                "Who wrote 'To Kill a Mockingbird'?",
                "What is the capital of Japan?",
                "When did World War II end?",
                "What is the largest planet in our solar system?",
                "Who painted the Mona Lisa?",
                "What is the chemical symbol for gold?",
                "What year did the Titanic sink?",
                "Who is credited with inventing the telephone?"
            ],
            "contexts": [
                ["The Rhodes Statue, a memorial to Cecil John Rhodes, was erected in 1934 at the University of Cape Town. It was removed in 2015 following student protests."],
                ["The Great Pyramid of Giza is located on the Giza Plateau, on the west bank of the Nile River, on the outskirts of modern-day Cairo, Egypt."],
                ["'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It won the Pulitzer Prize and has become a classic of modern American literature."],
                ["Tokyo is the capital city of Japan. It became the official capital in 1868 when the Meiji Emperor moved his seat to the city from the old capital of Kyoto."],
                ["World War II ended in 1945. The war in Europe concluded on May 7, 1945, when German forces surrendered. The Pacific War ended on August 15, 1945, when Japan surrendered."],
                ["Jupiter is the largest planet in our solar system. It is a gas giant with a mass more than two and a half times that of all the other planets in the solar system combined."],
                ["The Mona Lisa was painted by the Italian Renaissance artist Leonardo da Vinci. It is believed to have been painted between 1503 and 1506, although Leonardo may have continued working on it until 1517."],
                ["The chemical symbol for gold is Au. This symbol comes from the Latin word for gold, 'aurum', which means 'shining dawn'."],
                ["The RMS Titanic sank in the early morning hours of April 15, 1912, after striking an iceberg during her maiden voyage from Southampton to New York City."],
                ["Alexander Graham Bell is credited with inventing the telephone. He was granted the first U.S. patent for the telephone in 1876."]
            ],
            "responses": [
                "The Rhodes Statue was built in 1934 at the University of Cape Town.",
                "The Pyramid of Giza is located in Egypt, near Cairo, on the west bank of the Nile River.",
                "To Kill a Mockingbird was written by Harper Lee.",
                "The capital of Japan is Tokyo.",
                "World War II ended in 1945, with Germany surrendering in May and Japan in August.",
                "Jupiter is the largest planet in our solar system.",
                "The Mona Lisa was painted by Leonardo da Vinci.",
                "The chemical symbol for gold is Au.",
                "The Titanic sank in 1912.",
                "Alexander Graham Bell invented the telephone."
            ]
        }
    })
```
    
    
    
```
    # Prepare data for tabulation
    table_data = []
    for i, evaluation in enumerate(results['evaluator']['results'], 1):
        for result in evaluation:
            table_data.append([
                f"Question {i}",
                result['name'],
                result['score'],
                result['explanation']
            ])
```
    
```
    # Print table
    print("\nEvaluation Results:")
    print(tabulate(table_data, headers=["Question", "Metric", "Score", "Explanation"], tablefmt="grid"))
```
    
    
    
    
```
    from haystack.components.generators import OpenAIGenerator
    from haystack.utils import Secret
    import os
```
    
```
    client = OpenAIGenerator(model="gpt-4o-mini", api_key=Secret.from_token(os.environ["OPENAI_API_KEY"]))
    response = client.run("tell me a joke.")
    print(response['replies'][0])
```
    
    
    
```
    from haystack.components.generators import OpenAIGenerator
    from haystack.utils import Secret
    import os
```
    
    client = OpenAIGenerator(model="gpt-4o-mini", api_key=Secret.from_token(os.environ["OPENAI_API_KEY"]),streaming_callback=lambda chunk: print(chunk.content, end="", flush=True))
    response = client.run("tell me a joke.")
    
    
    
    
    
    
