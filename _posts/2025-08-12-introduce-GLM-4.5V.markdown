---
layout: single
title: "ğŸš€AIè§†è§‰æ–°çªç ´ï¼GLM-4.5Vå¤šæ¨¡æ€AIç¥å™¨å…¨é¢æµ‹è¯„ï¼é•¿è§†é¢‘ç†è§£èƒ½åŠ›å€å¢ï¼Œè½»æ¾å®ç°ç›‘æ§è§†é¢‘æŸ¥æ‰¾ç›®æ ‡äººç‰©ï¼OCRèƒ½åŠ›å€å¢ï¼Œè¯†åˆ«æ‰‹å†™å¤„æ–¹ã€æ¨¡ç³ŠPDFæ‰«æä»¶æ— å‹åŠ›ï¼106Bå‚æ•°MoEæ¶æ„è¶…è¶ŠGPT-4o"
sidebar:
  nav: "docs"
date: 2025-08-12 00:00:00 +0800
categories: LLMs
tags: [GLM-4.5V, Qwen2.5-VL, GLM-4.5, å¤šæ¨¡æ€å¤§æ¨¡å‹, VLM, OCR, LLMs]
classes: wide
author_profile: true
---

æœ€è¿‘AIåœˆåˆæœ‰å¤§åŠ¨ä½œäº†ï¼æ™ºè°±AIæ¨å‡ºäº†å…¨æ–°çš„GLM-4.5Vè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè¯´å®è¯ï¼Œçœ‹å®ŒæŠ€æœ¯æ–‡æ¡£åæˆ‘æœ‰ç‚¹å…´å¥‹â€”â€”è¿™å®¶ä¼™å¯èƒ½çœŸçš„è¦æ”¹å˜æˆ‘ä»¬å’ŒAIäº¤äº’çš„æ–¹å¼ã€‚

ä½ ä»¥ä¸ºå¤šæ¨¡æ€AIå°±æ˜¯è®©æœºå™¨çœ‹çœ‹å›¾ç‰‡ï¼Œç„¶åæè¿°ä¸€ä¸‹å†…å®¹ï¼Ÿé‚£ä½ å¯å°çœ‹GLM-4.5Väº†ã€‚

è¿™ä¸ªåŸºäºGLM-4.5-Airï¼ˆ106Bå‚æ•°ï¼Œ12Bæ¿€æ´»ï¼‰çš„æ–°æ¨¡å‹ï¼Œåœ¨42ä¸ªå…¬å¼€çš„è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†åŒç­‰è§„æ¨¡æ¨¡å‹çš„SOTAï¼ˆæœ€ä¼˜ï¼‰æ€§èƒ½ã€‚ä½†æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒä¸åªæ˜¯åœ¨è·‘åˆ†ä¸Šå¥½çœ‹ï¼Œè€Œæ˜¯çœŸæ­£ä¸ºå®ç”¨è€Œç”Ÿã€‚

> 
ğŸš€æœ¬ç¯‡ç¬”è®°æ‰€å¯¹åº”çš„è§†é¢‘ï¼š
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡å“”å“©å“”å“©è§‚çœ‹](https://www.bilibili.com/video/BV1fCtZzjEa5/)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡YouTubeè§‚çœ‹](https://youtu.be/NQllZfXxhe4)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ è§†è§‰æ¨¡å‹ç›‘æ§è§†é¢‘æ‰¾äºº](https://youtu.be/t5q4fT4rKK4)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Context Engineeringè§†é¢‘](https://youtu.be/oEZ7aN7jOEI)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ SuperClaudeè§†é¢‘](https://youtu.be/bMO13RNjvBk)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Claudiaè§†é¢‘](https://youtu.be/WIwW7V56wxE)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Task Masterè§†é¢‘](https://youtu.be/6dhOUJ_vnIY)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Zen MCPç¼–ç¨‹è§†é¢‘](https://youtu.be/2WgICfNzgZY)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Augmentç¼–ç¨‹è§†é¢‘](https://youtu.be/DbM3QZy5I6E)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Serena MCPè§†é¢‘](https://youtu.be/DZ-gLebVnmg)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¼€æºé¡¹ç›®](https://github.com/win4r/AISuperDomain)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ è¯·æˆ‘å–å’–å•¡](https://ko-fi.com/aila)
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¾®ä¿¡ï¼šstoeng
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æ‰¿æ¥å¤§æ¨¡å‹å¾®è°ƒã€RAGã€AIæ™ºèƒ½ä½“ã€AIç›¸å…³åº”ç”¨å¼€å‘ç­‰é¡¹ç›®ã€‚
> 
ğŸ”¥AIæ™ºèƒ½ä½“ç›¸å…³è§†é¢‘
- [AIæ™ºèƒ½ä½“è§†é¢‘ 1](https://youtu.be/vYm0brFoMwA) 
- [AIæ™ºèƒ½ä½“è§†é¢‘ 2](https://youtu.be/szTXELuaJos)  
- [AIæ™ºèƒ½ä½“è§†é¢‘ 3](https://youtu.be/szTXELuaJos)  
- [AIæ™ºèƒ½ä½“è§†é¢‘ 4](https://youtu.be/RxR3x_Uyq4c)  
- [AIæ™ºèƒ½ä½“è§†é¢‘ 5](https://youtu.be/IrTEDPnEVvU)  
- [AIæ™ºèƒ½ä½“è§†é¢‘ 6](https://youtu.be/q_IdxUGZsow)  


æƒ³è±¡ä¸€ä¸‹è¿™äº›åœºæ™¯ï¼š

- **å¤æ‚å›¾åƒæ¨ç†**ï¼šä¸ä»…èƒ½ç†è§£å•å¼ å›¾ç‰‡ï¼Œè¿˜èƒ½åˆ†æå¤šå¼ å›¾åƒçš„å…³è”æ€§å’Œç©ºé—´å…³ç³»
- **é•¿è§†é¢‘ç†è§£**ï¼šå¯ä»¥å¤„ç†é•¿è§†é¢‘å†…å®¹ï¼Œè¿›è¡Œäº‹ä»¶è¯†åˆ«å’Œæ—¶é—´åˆ†å‰²
- **GUIæ“ä½œåŠ©æ‰‹**ï¼šèƒ½å¤Ÿ"çœ‹æ‡‚"ä½ çš„ç”µè„‘å±å¹•ï¼Œè¯†åˆ«å›¾æ ‡ï¼ŒååŠ©æ¡Œé¢æ“ä½œ
- **æ–‡æ¡£è§£æä¸“å®¶**ï¼šæ— è®ºæ˜¯å¤æ‚å›¾è¡¨è¿˜æ˜¯é•¿ç¯‡ç ”ç©¶æŠ¥å‘Šï¼Œéƒ½èƒ½å‡†ç¡®æå–ä¿¡æ¯
- **ç²¾å‡†å®šä½**ï¼šèƒ½å¤Ÿå‡†ç¡®æ ‡å‡ºå›¾åƒä¸­ç‰¹å®šå…ƒç´ çš„ä½ç½®

## ä¼š"æ€è€ƒ"çš„è§†è§‰AI

è¿™é‡Œæœ‰ä¸ªç‰¹åˆ«æœ‰æ„æ€çš„åŠŸèƒ½â€”â€”**æ€è€ƒæ¨¡å¼å¼€å…³**ã€‚

å°±åƒGLM-4.5è¯­è¨€æ¨¡å‹ä¸€æ ·ï¼ŒGLM-4.5Vä¹Ÿæ”¯æŒåœ¨"å¿«é€Ÿå“åº”"å’Œ"æ·±åº¦æ¨ç†"ä¹‹é—´åˆ‡æ¢ã€‚éœ€è¦å¿«é€Ÿç­”æ¡ˆçš„æ—¶å€™ï¼Œå®ƒèƒ½ç§’å›ï¼›é‡åˆ°å¤æ‚é—®é¢˜ï¼Œä½ å¯ä»¥å¼€å¯æ€è€ƒæ¨¡å¼ï¼Œè®©å®ƒæ…¢æ…¢ç¢ç£¨ï¼Œç»™å‡ºæ›´æ·±å…¥çš„åˆ†æã€‚

è¿™å°±åƒæ˜¯ç»™AIè£…äº†ä¸ª"å¤§è„‘æ¡£ä½è°ƒèŠ‚å™¨"ï¼Œå®ç”¨æ€§å¤§å¤§æå‡ã€‚

## æŠ€æœ¯ç»†èŠ‚ï¼šç®€å•butå¼ºå¤§

å¯¹äºå¼€å‘è€…æ¥è¯´ï¼ŒGLM-4.5Vçš„ä½¿ç”¨é—¨æ§›å¹¶ä¸é«˜ã€‚åªéœ€è¦å‡ è¡Œä»£ç ï¼š

```python
pip install transformers-v4.55.0-GLM-4.5V-preview

```

ç„¶åå°±èƒ½è½»æ¾è°ƒç”¨æ¨¡å‹å¤„ç†å›¾åƒã€è§†é¢‘ç­‰å¤šç§è§†è§‰å†…å®¹ã€‚æ¨¡å‹è¿˜æ”¯æŒè¾¹ç•Œæ¡†æ ‡æ³¨åŠŸèƒ½ï¼Œèƒ½å¤Ÿç²¾ç¡®æ ‡å‡ºå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„ä½ç½®ï¼Œè¿™å¯¹äºéœ€è¦ç²¾ç¡®å®šä½çš„åº”ç”¨åœºæ™¯ç‰¹åˆ«æœ‰ç”¨ã€‚

## å¼€æºçš„åŠ›é‡

å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒGLM-4.5Væ˜¯å¼€æºçš„ï¼è¿™æ„å‘³ç€æ›´å¤šå¼€å‘è€…å¯ä»¥åŸºäºå®ƒåˆ›é€ å‡ºä»¤äººæƒŠå–œçš„åº”ç”¨ã€‚ä»æ™ºè°±AIçš„GitHubé¡µé¢å¯ä»¥çœ‹åˆ°ï¼Œä»–ä»¬ä¸ä»…å¼€æ”¾äº†æ¨¡å‹ï¼Œè¿˜æä¾›äº†è¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£ã€‚

åŒæ—¶ï¼Œå¦‚æœä½ ä¸æƒ³è‡ªå·±éƒ¨ç½²ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ™ºè°±AIå¼€æ”¾å¹³å°çš„APIç›´æ¥è°ƒç”¨ï¼Œéå¸¸æ–¹ä¾¿ã€‚

## 

# GLM-4.5Vå®Œæ•´å®‰è£…å’Œä½¿ç”¨æŒ‡å—ï¼ˆå·²éªŒè¯ï¼‰

## ç¯å¢ƒè¦æ±‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç³»ç»Ÿæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

- Python 3.9 æˆ–ä»¥ä¸Šç‰ˆæœ¬
- CUDA 12.1 æˆ–ä»¥ä¸Šï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
- è‡³å°‘32GBå†…å­˜ï¼ˆæ¨è64GBï¼‰
- GPUæ˜¾å­˜è‡³å°‘12GBï¼ˆæ¨è24GBæˆ–ä»¥ä¸Šï¼‰
- æ”¯æŒçš„GPUï¼šNVIDIA RTX 3080åŠä»¥ä¸Šæˆ–A100/H100ç­‰æ•°æ®ä¸­å¿ƒGPU

## ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# ä½¿ç”¨condaåˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
conda create -n glm4v python=3.10
conda activate glm4v

# æˆ–è€…ä½¿ç”¨venv
python -m venv glm4v_env
source glm4v_env/bin/activate  # Linux/Mac
# glm4v_env\Scripts\activate  # Windows

```

## ç¬¬äºŒæ­¥ï¼šå®‰è£…ä¾èµ–åŒ…

### å®‰è£…PyTorchï¼ˆæ ¹æ®æ‚¨çš„CUDAç‰ˆæœ¬é€‰æ‹©ï¼‰

```bash
# CUDA 12.1ï¼ˆæ¨èï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CPUç‰ˆæœ¬ï¼ˆä¸æ¨èï¼Œæ€§èƒ½æå·®ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

```

### å®‰è£…GLM-4.5Vä¸“ç”¨transformersç‰ˆæœ¬

**é‡è¦æç¤º**ï¼šGLM-4.5Véœ€è¦ç‰¹å®šçš„transformersç‰ˆæœ¬ã€‚

```bash
# å®‰è£…GLM-4.5Vä¸“ç”¨çš„transformersç‰ˆæœ¬
pip install transformers-v4.55.0-GLM-4.5V-preview

# å¦‚æœä¸Šè¿°å®‰è£…å¤±è´¥ï¼Œå¯ä»¥å°è¯•ï¼š
# pip install --extra-index-url https://wheels.vllm.ai/nightly transformers-v4.55.0-GLM-4.5V-preview

# å®‰è£…å…¶ä»–å¿…éœ€ä¾èµ–
pip install accelerate>=0.20.0
pip install sentencepiece
pip install protobuf
pip install Pillow
pip install opencv-python
pip install requests

```

### å¯é€‰ï¼šå®‰è£…vLLMåŠ é€Ÿæ¨ç†

```bash
# å¦‚æœéœ€è¦é«˜æ€§èƒ½æ¨ç†ï¼Œå¯ä»¥å®‰è£…vLLM
pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly

```

## ç¬¬ä¸‰æ­¥ï¼šéªŒè¯å®‰è£…

åˆ›å»ºä¸€ä¸ªæµ‹è¯•è„šæœ¬éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸï¼š

```python
# test_installation.py
import torch
import sys

print(f"Pythonç‰ˆæœ¬: {sys.version}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"GPU {i} æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB")

# æµ‹è¯•transformerså¯¼å…¥
try:
    from transformers import AutoProcessor, Glm4vMoeForConditionalGeneration
    print("âœ“ GLM-4.5V transformersç‰ˆæœ¬å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âœ— transformerså¯¼å…¥å¤±è´¥: {e}")

print("GLM-4.5Vå®‰è£…éªŒè¯å®Œæˆï¼")

```

è¿è¡Œæµ‹è¯•ï¼š

```bash
python test_installation.py

```

## ç¬¬å››æ­¥ï¼šåŸºç¡€ä½¿ç”¨ç¤ºä¾‹

### 1. æœ€ç®€å•çš„å›¾åƒç†è§£ç¤ºä¾‹

```python
from transformers import AutoProcessor, Glm4vMoeForConditionalGeneration
import torch
import requests
from PIL import Image

# åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼Œéœ€è¦æ—¶é—´ï¼‰
MODEL_PATH = "zai-org/GLM-4.5V"
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")

processor = AutoProcessor.from_pretrained(MODEL_PATH)
model = Glm4vMoeForConditionalGeneration.from_pretrained(
    pretrained_model_name_or_path=MODEL_PATH,
    torch_dtype="auto",
    device_map="auto",
)
print("æ¨¡å‹åŠ è½½å®Œæˆï¼")

# å‡†å¤‡å›¾åƒï¼ˆä½¿ç”¨ç½‘ç»œå›¾ç‰‡ï¼‰
image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"

# æ„å»ºæ¶ˆæ¯æ ¼å¼
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "url": image_url
            },
            {
                "type": "text",
                "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"
            }
        ],
    }
]

# å¤„ç†è¾“å…¥
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device)

# é‡è¦ï¼šç§»é™¤token_type_ids
inputs.pop("token_type_ids", None)

# ç”Ÿæˆå›ç­”
with torch.no_grad():
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=2048,
        temperature=0.7,
        do_sample=True,
        top_p=0.9
    )

# è§£ç è¾“å‡º
output_text = processor.decode(
    generated_ids[0][inputs["input_ids"].shape[1]:],
    skip_special_tokens=True
)

print("AIå›ç­”ï¼š")
print(output_text)

```

### 2. æœ¬åœ°å›¾åƒå¤„ç†ç¤ºä¾‹

```python
from PIL import Image
import os

# åŠ è½½æœ¬åœ°å›¾ç‰‡
def load_local_image(image_path):
    """å®‰å…¨åŠ è½½æœ¬åœ°å›¾ç‰‡"""
    try:
        image = Image.open(image_path)
        # ç¡®ä¿æ˜¯RGBæ ¼å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        return image
    except Exception as e:
        print(f"åŠ è½½å›¾ç‰‡å¤±è´¥: {e}")
        return None

# ä½¿ç”¨æœ¬åœ°å›¾ç‰‡
image_path = "your_image.jpg"  # æ›¿æ¢ä¸ºä½ çš„å›¾ç‰‡è·¯å¾„
if os.path.exists(image_path):
    image = load_local_image(image_path)

    if image is not None:
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image  # æ³¨æ„ï¼šæœ¬åœ°å›¾ç‰‡ä½¿ç”¨"image"è€Œä¸æ˜¯"url"
                    },
                    {
                        "type": "text",
                        "text": "åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰å…ƒç´ ï¼ŒåŒ…æ‹¬ç‰©ä½“ã€é¢œè‰²ã€æ„å›¾ç­‰ã€‚"
                    }
                ],
            }
        ]

        # åç»­å¤„ç†æ­¥éª¤ä¸ä¸Šé¢ç›¸åŒ...
else:
    print(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")

```

### 3. å¤šå›¾åƒå¯¹æ¯”åˆ†æ

```python
# å¤šå›¾åƒåˆ†æç¤ºä¾‹
def analyze_multiple_images(image_list, question):
    """åˆ†æå¤šå¼ å›¾ç‰‡"""
    content = []

    # æ·»åŠ æ‰€æœ‰å›¾ç‰‡
    for i, img_path in enumerate(image_list):
        if img_path.startswith('http'):
            content.append({"type": "image", "url": img_path})
        else:
            img = load_local_image(img_path)
            if img:
                content.append({"type": "image", "image": img})

    # æ·»åŠ é—®é¢˜
    content.append({"type": "text", "text": question})

    messages = [{"role": "user", "content": content}]

    # å¤„ç†å’Œç”Ÿæˆ...ï¼ˆä¸ä¸Šé¢ç›¸åŒçš„æµç¨‹ï¼‰

# ä½¿ç”¨ç¤ºä¾‹
image_list = [
    "image1.jpg",
    "https://example.com/image2.jpg",
    "image3.png"
]
question = "æ¯”è¾ƒè¿™äº›å›¾ç‰‡çš„ç›¸ä¼¼ç‚¹å’Œä¸åŒç‚¹ã€‚"

```

## ç¬¬äº”æ­¥ï¼šå¤„ç†å¸¸è§é”™è¯¯

### 1. æ˜¾å­˜ä¸è¶³é”™è¯¯

```python
# æ–¹æ³•1ï¼šä½¿ç”¨æ›´ä½ç²¾åº¦
model = Glm4vMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦
    device_map="auto"
)

# æ–¹æ³•2ï¼šä½¿ç”¨CPUæ¨ç†ï¼ˆå¾ˆæ…¢ä½†å¯ç”¨ï¼‰
model = Glm4vMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,
    device_map={"": "cpu"}
)

# æ–¹æ³•3ï¼šå‡å°‘ç”Ÿæˆé•¿åº¦
generated_ids = model.generate(
    **inputs,
    max_new_tokens=512,  # å‡å°‘è¾“å‡ºé•¿åº¦
    temperature=0.7
)

```

### 2. æ¨¡å‹ä¸‹è½½é—®é¢˜

```python
import os
from huggingface_hub import snapshot_download

# æ–¹æ³•1ï¼šè®¾ç½®é•œåƒï¼ˆä¸­å›½ç”¨æˆ·ï¼‰
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# æ–¹æ³•2ï¼šé¢„å…ˆä¸‹è½½æ¨¡å‹
def download_model():
    try:
        snapshot_download(
            repo_id="zai-org/GLM-4.5V",
            local_dir="./glm-4.5v-model",
            repo_type="model"
        )
        print("æ¨¡å‹ä¸‹è½½å®Œæˆ")
        return "./glm-4.5v-model"
    except Exception as e:
        print(f"ä¸‹è½½å¤±è´¥: {e}")
        return None

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹
model_path = download_model()
if model_path:
    model = Glm4vMoeForConditionalGeneration.from_pretrained(model_path)

```

### 3. transformersç‰ˆæœ¬é—®é¢˜

```bash
# å¦‚æœé‡åˆ°ç‰ˆæœ¬å†²çªï¼Œå®Œå…¨é‡è£…
pip uninstall transformers -y
pip install transformers-v4.55.0-GLM-4.5V-preview

```

## ç¬¬å…­æ­¥ï¼šè¾¹ç•Œæ¡†è§£æåŠŸèƒ½

GLM-4.5Væ”¯æŒç›®æ ‡å®šä½ï¼Œä¼šåœ¨è¾“å‡ºä¸­åŒ…å«ç‰¹æ®Šæ ‡è®°ï¼š

```python
import re

def parse_bounding_boxes(text):
    """
    è§£æGLM-4.5Vè¾“å‡ºä¸­çš„è¾¹ç•Œæ¡†åæ ‡
    åæ ‡èŒƒå›´ï¼š0-1000ï¼Œéœ€è¦æ ¹æ®å®é™…å›¾åƒå°ºå¯¸ç¼©æ”¾
    """
    # æŸ¥æ‰¾è¾¹ç•Œæ¡†æ ‡è®°
    pattern = r'<\|begin_of_box\|>(.*?)<\|end_of_box\|>'
    boxes = re.findall(pattern, text)

    parsed_boxes = []
    for box in boxes:
        # æå–æ•°å­—åæ ‡
        coords = re.findall(r'\d+', box)
        if len(coords) >= 4:
            x1, y1, x2, y2 = [int(coord) for coord in coords[:4]]
            parsed_boxes.append({
                'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2,
                'raw': box
            })

    return parsed_boxes

def scale_boxes(boxes, image_width, image_height):
    """å°†1000x1000åæ ‡ç³»ç¼©æ”¾åˆ°å®é™…å›¾åƒå°ºå¯¸"""
    scaled_boxes = []
    for box in boxes:
        scaled_box = {
            'x1': int(box['x1'] * image_width / 1000),
            'y1': int(box['y1'] * image_height / 1000),
            'x2': int(box['x2'] * image_width / 1000),
            'y2': int(box['y2'] * image_height / 1000)
        }
        scaled_boxes.append(scaled_box)
    return scaled_boxes

# ä½¿ç”¨ç¤ºä¾‹
response = "ç‰©ä½“ä½ç½®åœ¨<|begin_of_box|>[100, 200, 300, 400]<|end_of_box|>è¿™é‡Œ"
boxes = parse_bounding_boxes(response)
print(f"æ£€æµ‹åˆ°çš„è¾¹ç•Œæ¡†: {boxes}")

```

## ç¬¬ä¸ƒæ­¥ï¼šå®Œæ•´å·¥ä½œç¤ºä¾‹

```python
#!/usr/bin/env python3
"""
GLM-4.5Vå®Œæ•´ä½¿ç”¨ç¤ºä¾‹ï¼ˆå·²éªŒè¯ç‰ˆæœ¬ï¼‰
"""

import torch
import sys
import os
from PIL import Image
import requests
from io import BytesIO

class GLM4VAssistant:
    def __init__(self, model_path="zai-org/GLM-4.5V"):
        self.model_path = model_path
        self.processor = None
        self.model = None
        self.load_model()

    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        try:
            print("æ­£åœ¨åŠ è½½GLM-4.5Væ¨¡å‹...")

            from transformers import AutoProcessor, Glm4vMoeForConditionalGeneration

            self.processor = AutoProcessor.from_pretrained(self.model_path)
            self.model = Glm4vMoeForConditionalGeneration.from_pretrained(
                pretrained_model_name_or_path=self.model_path,
                torch_dtype="auto",
                device_map="auto",
            )

            print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼")
            print(f"æ¨¡å‹è®¾å¤‡: {next(self.model.parameters()).device}")

        except Exception as e:
            print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)

    def load_image(self, image_source):
        """åŠ è½½å›¾åƒï¼ˆæ”¯æŒURLå’Œæœ¬åœ°æ–‡ä»¶ï¼‰"""
        try:
            if image_source.startswith(('http://', 'https://')):
                response = requests.get(image_source)
                image = Image.open(BytesIO(response.content))
            else:
                image = Image.open(image_source)

            # ç¡®ä¿RGBæ ¼å¼
            if image.mode != 'RGB':
                image = image.convert('RGB')

            return image
        except Exception as e:
            print(f"å›¾åƒåŠ è½½å¤±è´¥: {e}")
            return None

    def analyze_image(self, image_source, question="è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡"):
        """åˆ†æå•å¼ å›¾ç‰‡"""
        if not self.model or not self.processor:
            return "æ¨¡å‹æœªæ­£ç¡®åŠ è½½"

        # å‡†å¤‡å›¾åƒè¾“å…¥
        if image_source.startswith(('http://', 'https://')):
            image_input = {"type": "image", "url": image_source}
        else:
            image = self.load_image(image_source)
            if image is None:
                return "å›¾åƒåŠ è½½å¤±è´¥"
            image_input = {"type": "image", "image": image}

        # æ„å»ºæ¶ˆæ¯
        messages = [{
            "role": "user",
            "content": [
                image_input,
                {"type": "text", "text": question}
            ]
        }]

        try:
            # å¤„ç†è¾“å…¥
            inputs = self.processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            ).to(self.model.device)

            # ç§»é™¤token_type_ids
            inputs.pop("token_type_ids", None)

            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=2048,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.processor.tokenizer.eos_token_id
                )

            # è§£ç è¾“å‡º
            output = self.processor.decode(
                generated_ids[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )

            return output.strip()

        except Exception as e:
            return f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"

def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # åˆ›å»ºåŠ©æ‰‹å®ä¾‹
    assistant = GLM4VAssistant()

    # æµ‹è¯•ç½‘ç»œå›¾ç‰‡
    print("=" * 50)
    print("æµ‹è¯•1: ç½‘ç»œå›¾ç‰‡åˆ†æ")
    image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
    result = assistant.analyze_image(image_url, "è¿™å¼ å›¾ç‰‡å±•ç°äº†ä»€ä¹ˆåœºæ™¯ï¼Ÿè¯·è¯¦ç»†æè¿°ã€‚")
    print(f"åˆ†æç»“æœ:\n{result}")

    # æµ‹è¯•æœ¬åœ°å›¾ç‰‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    local_image = "test_image.jpg"
    if os.path.exists(local_image):
        print("\n" + "=" * 50)
        print("æµ‹è¯•2: æœ¬åœ°å›¾ç‰‡åˆ†æ")
        result = assistant.analyze_image(local_image, "åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„ä¸»è¦å…ƒç´ å’Œç‰¹å¾ã€‚")
        print(f"åˆ†æç»“æœ:\n{result}")
    else:
        print(f"\næœ¬åœ°æµ‹è¯•å›¾ç‰‡ {local_image} ä¸å­˜åœ¨ï¼Œè·³è¿‡æœ¬åœ°æµ‹è¯•")

if __name__ == "__main__":
    main()

```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **GPUé€‰æ‹©**ï¼šRTX 4090æˆ–H100ç­‰é«˜ç«¯GPUæ•ˆæœæœ€ä½³
2. **å†…å­˜ç®¡ç†**ï¼šç¡®ä¿æœ‰è¶³å¤Ÿç³»ç»Ÿå†…å­˜å’Œæ˜¾å­˜
3. **æ‰¹å¤„ç†**ï¼šå¤„ç†å¤šå¼ å›¾ç‰‡æ—¶è€ƒè™‘æ‰¹é‡æ“ä½œ
4. **ç²¾åº¦é€‰æ‹©**ï¼šæ ¹æ®éœ€æ±‚åœ¨é€Ÿåº¦å’Œè´¨é‡é—´å¹³è¡¡
5. **ç¼“å­˜æ¨¡å‹**ï¼šé¿å…é‡å¤åŠ è½½æ¨¡å‹

## æ•…éšœæ’é™¤

### å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

**é”™è¯¯1**: `ModuleNotFoundError: No module named 'transformers'`**è§£å†³**: ç¡®ä¿å®‰è£…äº†æ­£ç¡®çš„transformersç‰ˆæœ¬ï¼š

```bash
pip install transformers-v4.55.0-GLM-4.5V-preview

```

**é”™è¯¯2**: `CUDA out of memory`**è§£å†³**: é™ä½ç²¾åº¦æˆ–ä½¿ç”¨CPUï¼š

```python
torch_dtype=torch.float16  # æˆ– device_map={"": "cpu"}

```

**é”™è¯¯3**: `RuntimeError: Expected all tensors to be on the same device`**è§£å†³**: ç¡®ä¿è¾“å…¥å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ï¼š

```python
inputs = inputs.to(model.device)

```

