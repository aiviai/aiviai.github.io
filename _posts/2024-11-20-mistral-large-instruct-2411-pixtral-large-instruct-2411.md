---
layout: single
title: "Mistral-Large-Instruct-2411 & Pixtral-Large-Instruct-2411"
sidebar:
  nav: "docs"
date: 2024-11-20 00:00:00 +0800
categories: [Fine-Tuning, LLM, RAG, Vision]
tags: [AI, LangFlow, Mistral, RAG]
classes: wide
author_profile: true
---


#  Mistral-Large-Instruct-2411 & Pixtral-Large-Instruct-2411 

### 

Name  |  Number of parameters  |  Number of active parameters  |  Min. GPU RAM for inference (GB)   
---|---|---|---  
Mistral-Large-Instruct-2411  |  123B  |  123B  |  250   
Pixtral-Large-Instruct-2411  |  124B  |  124B  |  250   
  
###  LangFlow 
    
    
    python -m pip install langflow
    
    python -m langflow run
    

###  API 
    
    
```
    curl --location "https://api.mistral.ai/v1/chat/completions" \
         --header 'Content-Type: application/json' \
         --header 'Accept: application/json' \
         --header "Authorization: Bearer $MISTRAL_API_KEY" \
         --data '{
        "model": "mistral-large-latest",
        "messages": [{"role": "user", "content": "Who is the most renowned French painter?"}]
      }'
```

###  API 
    
    
    !pip install mistralai
    
    from google.colab import userdata
    
    from mistralai import Mistral
    api_key = userdata.get('MISTRAL_API_KEY')
    
    
    model = "mistral-large-latest"
    
    client = Mistral(api_key=api_key)
    
```
    chat_response = client.chat.complete(
        model=model,
        messages=[{"role":"user", "content":"ä½ çš„çŸ¥è¯†åº“/è®­ç»ƒæ•°æ®æˆªæ­¢æ—¥æœŸ?"}]
    )
```
    
    print(chat_response.choices[0].message.content)

###  ä»£ç  
    
    
    import os
    from mistralai import Mistral
    
    # Retrieve the API key from environment variables
    api_key = userdata.get('MISTRAL_API_KEY')
    
    # Specify model
    model = "pixtral-large-latest"
    
    # Initialize the Mistral client
    client = Mistral(api_key=api_key)
    
```
    # Define the messages for the chat
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "å›¾ä¸­æ˜¯å¦æœ‰ä¸€ä¸ªç©¿ç€é»‘è‰²æ— è¢–ä¸Šè¡£ã€çº¢è‰²è£¤å­çš„å¥³æ€§ï¼Ÿå¦‚æœæœ‰ï¼Œè¯·è¯¦ç»†æè¿°å¥¹çš„ä½“è²Œç‰¹å¾(è¡£ç€ã€ä½“æ€ã€è‚¤è‰²ã€å‘å‹ã€æ˜¯å¦ä½©æˆ´è£…é¥°å“ç­‰)ã€‚"
                    #  "text": "å›¾ä¸­æ˜¯å¦æœ‰ä¸€ä¸ªç©¿ç€é•¿è¢–çš„å°ç”·å­©ï¼Œçœ‹ä¸Šå»ä¸ƒå…«å²å·¦å³"
                    # "text": "è¯·è¯¦ç»†æè¿°å›¾ä¸­æ¯ä¸ªäººç‰©çš„ä½“è²Œç‰¹å¾ã€‚"
```
                    
    
```
                },
                {
                    "type": "image_url",
                    "image_url": "https://cdn.pixabay.com/photo/2013/10/09/20/47/large-193357_1280.jpg"
                }
            ]
        }
    ]
```
    
```
    # Get the chat response
    chat_response = client.chat.complete(
        model=model,
        messages=messages
    )
```
    
    # Print the content of the response
    print(chat_response.choices[0].message.content)
    

##  **ğŸ‘‰ğŸ‘‰ğŸ‘‰å¦‚æœ‰é—®é¢˜æˆ–è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng**

##  **ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡**

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰æˆ‘çš„å¼€æºé¡¹ç›®** **[ https://github.com/win4r/AISuperDomain ](<https://github.com/win4r/AISuperDomain>) **
    
    
    !pip install bert-score
    
    import torch
    from bert_score import BERTScorer
    
```
    def calculate_bertscore(references, candidates, model_type="bert-base-uncased", lang="en"):
        """
        ä½¿ç”¨BERTScoreè®¡ç®—å‚è€ƒæ–‡æœ¬å’Œå€™é€‰æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦
```
    
```
        å‚æ•°:
        references: å‚è€ƒæ–‡æœ¬åˆ—è¡¨
        candidates: å€™é€‰ï¼ˆç”Ÿæˆï¼‰æ–‡æœ¬åˆ—è¡¨
        model_type: ä½¿ç”¨çš„BERTæ¨¡å‹ç±»å‹
        lang: æ–‡æœ¬è¯­è¨€
```
    
```
        è¿”å›:
        åŒ…å«ç²¾ç¡®åº¦(P)ã€å¬å›ç‡(R)å’ŒF1åˆ†æ•°çš„å­—å…¸
        """
        # åˆå§‹åŒ–BERTScorer
        scorer = BERTScorer(model_type=model_type, lang=lang)
```
    
        # è®¡ç®—BERTScore
        P, R, F1 = scorer.score(candidates, references)
    
```
        # è½¬æ¢ä¸ºPythonåŸç”Ÿæ•°æ®ç±»å‹
        P = P.tolist()
        R = R.tolist()
        F1 = F1.tolist()
```
    
```
        return {
            "Precision": P,
            "Recall": R,
            "F1": F1
        }
```
    
```
    # ç¤ºä¾‹ä½¿ç”¨
    references = [
        "The cat is sitting on the mat.",
        "A dog is playing in the park."
    ]
    candidates = [
        "A feline is resting on a rug.",
        "A canine is running on grass in a park."
    ]
```
    
    # è®¡ç®—BERTScore
    results = calculate_bertscore(references, candidates)
    
```
    # æ‰“å°ç»“æœ
    for i, (ref, cand) in enumerate(zip(references, candidates)):
        print(f"Example {i+1}:")
        print(f"Reference: {ref}")
        print(f"Candidate: {cand}")
        print(f"Precision: {results['Precision'][i]:.4f}")
        print(f"Recall: {results['Recall'][i]:.4f}")
        print(f"F1: {results['F1'][i]:.4f}")
        print()
```
    
```
    # è®¡ç®—å¹³å‡åˆ†æ•°
    avg_precision = sum(results['Precision']) / len(results['Precision'])
    avg_recall = sum(results['Recall']) / len(results['Recall'])
    avg_f1 = sum(results['F1']) / len(results['F1'])
```
    
```
    print("Average Scores:")
    print(f"Precision: {avg_precision:.4f}")
    print(f"Recall: {avg_recall:.4f}")
    print(f"F1: {avg_f1:.4f}")
```
    
    
```
    import os
    from mistralai import Mistral
    import torch
    from bert_score import BERTScorer
    from typing import List, Dict, Union, Tuple
    from dataclasses import dataclass
    from datetime import datetime
```
    
```
    @dataclass
    class MatchDetails:
        """åŒ¹é…è¯¦æƒ…æ•°æ®ç±»"""
        score: float
        threshold: float
        difference: float
        status: str
        confidence: str
```
    
```
    class ImageSemanticAnalyzer:
        def __init__(self, 
                     api_key: str = None, 
                     model_type: str = "bert-base-chinese", 
                     lang: str = "zh",
                     threshold: Dict[str, float] = None,
                     log_results: bool = True):
            """
            åˆå§‹åŒ–å›¾åƒè¯­ä¹‰åˆ†æå™¨
```
            
```
            å‚æ•°:
            api_key: Mistral APIå¯†é’¥
            model_type: BERTæ¨¡å‹ç±»å‹
            lang: æ–‡æœ¬è¯­è¨€
            threshold: åŒ¹é…é˜ˆå€¼å­—å…¸
            log_results: æ˜¯å¦è®°å½•åˆ†æç»“æœ
            """
            self.threshold = threshold or {
                'precision': 0.7,
                'recall': 0.7,
                'f1': 0.7
            }
```
            
            self.log_results = log_results
            self.api_key = userdata.get('MISTRAL_API_KEY')
            
```
            if not self.api_key:
                raise ValueError(
                    "No API key provided. Please either pass the API key directly "
                    "or set the MISTRAL_API_KEY environment variable."
                )
```
            
```
            try:
                self.model = "pixtral-large-latest"
                self.client = Mistral(api_key=self.api_key)
                self.scorer = BERTScorer(model_type=model_type, lang=lang)
            except Exception as e:
                raise Exception(f"Failed to initialize Mistral client: {str(e)}")
```
        
```
        def analyze_image(self, prompt: str, image_url: str) -> str:
            """åˆ†æå›¾åƒå¹¶è¿”å›AIçš„æè¿°"""
            try:
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": image_url
                            }
                        ]
                    }
                ]
```
                
```
                chat_response = self.client.chat.complete(
                    model=self.model,
                    messages=messages
                )
```
                
                return chat_response.choices[0].message.content
                
            except Exception as e:
                raise Exception(f"Error during image analysis: {str(e)}")
        
```
        def calculate_semantic_similarity(self, 
                                       prompt: str, 
                                       ai_response: str) -> Tuple[Dict[str, float], bool]:
            """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¹¶åˆ¤æ–­æ˜¯å¦åŒ¹é…ç›®æ ‡"""
            try:
                references = [prompt]
                candidates = [ai_response]
```
                
                P, R, F1 = self.scorer.score(candidates, references)
                
```
                scores = {
                    "Precision": P[0].item(),
                    "Recall": R[0].item(),
                    "F1": F1[0].item()
                }
```
                
```
                is_match = (
                    scores["Precision"] >= self.threshold['precision'] and
                    scores["Recall"] >= self.threshold['recall'] and
                    scores["F1"] >= self.threshold['f1']
                )
```
                
                return scores, is_match
                
            except Exception as e:
                raise Exception(f"Error calculating semantic similarity: {str(e)}")
        
```
        def analyze_match_details(self, similarity_scores: Dict[str, float]) -> Dict[str, MatchDetails]:
            """åˆ†æåŒ¹é…ç»†èŠ‚ï¼Œæä¾›æ¯ä¸ªæŒ‡æ ‡çš„è¯¦ç»†è¯„ä¼°"""
            analysis = {}
```
            
```
            for metric, score in similarity_scores.items():
                threshold = self.threshold[metric.lower()]
                difference = score - threshold
                status = "é€šè¿‡" if score >= threshold else "æœªé€šè¿‡"
                confidence = self._calculate_confidence(difference)
```
                
```
                analysis[metric] = MatchDetails(
                    score=score,
                    threshold=threshold,
                    difference=difference,
                    status=status,
                    confidence=confidence
                )
```
            
            return analysis
        
```
        def _calculate_confidence(self, difference: float) -> str:
            """æ ¹æ®ä¸é˜ˆå€¼çš„å·®è·è®¡ç®—ç½®ä¿¡åº¦çº§åˆ«"""
            if difference >= 0.1:
                return "é«˜"
            elif difference >= 0.05:
                return "ä¸­"
            elif difference >= 0:
                return "ä½"
            else:
                return "ä¸æ»¡è¶³"
```
        
```
        def log_result(self, result: Dict) -> None:
            """è®°å½•åˆ†æç»“æœåˆ°æ–‡ä»¶"""
            if not self.log_results:
                return
```
                
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"analysis_log_{timestamp}.txt"
            
```
            try:
                with open(filename, "w", encoding="utf-8") as f:
                    f.write("=== å›¾åƒåˆ†æç»“æœ ===\n\n")
                    f.write(f"æ—¶é—´: {timestamp}\n\n")
```
                    
                    f.write("AIå“åº”:\n")
                    f.write(f"{result['ai_response']}\n\n")
                    
```
                    f.write("è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°:\n")
                    for metric, score in result['similarity_scores'].items():
                        f.write(f"{metric}: {score:.4f}\n")
```
                    
```
                    f.write("\nè¯¦ç»†åŒ¹é…åˆ†æ:\n")
                    for metric, details in result['match_analysis'].items():
                        f.write(f"\n{metric}åˆ†æ:\n")
                        f.write(f"  å¾—åˆ†: {details.score:.4f}\n")
                        f.write(f"  é˜ˆå€¼: {details.threshold:.4f}\n")
                        f.write(f"  å·®è·: {details.difference:.4f}\n")
                        f.write(f"  çŠ¶æ€: {details.status}\n")
                        f.write(f"  ç½®ä¿¡åº¦: {details.confidence}\n")
```
                    
                    f.write("\nç›®æ ‡åŒ¹é…ç»“æœ:\n")
                    f.write(f"æ˜¯å¦åŒ¹é…ç›®æ ‡: {'æ˜¯' if result['is_target_match'] else 'å¦'}\n")
                    
            except Exception as e:
                print(f"Warning: Failed to log results: {str(e)}")
        
```
        def analyze_and_evaluate(self, prompt: str, image_url: str) -> Dict:
            """å®Œæ•´çš„åˆ†ææµç¨‹ï¼šåŒ…å«è¯¦ç»†çš„åŒ¹é…åˆ†æ"""
            try:
                ai_response = self.analyze_image(prompt, image_url)
                similarity_scores, is_match = self.calculate_semantic_similarity(prompt, ai_response)
                match_analysis = self.analyze_match_details(similarity_scores)
```
                
```
                result = {
                    "ai_response": ai_response,
                    "similarity_scores": similarity_scores,
                    "is_target_match": is_match,
                    "match_analysis": match_analysis
                }
```
                
                if self.log_results:
                    self.log_result(result)
                
                return result
                
            except Exception as e:
                raise Exception(f"Error in analysis and evaluation: {str(e)}")
    
```
    def format_analysis_result(result: Dict) -> str:
        """æ ¼å¼åŒ–åˆ†æç»“æœä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²"""
        output = []
```
        
        output.append("AIå“åº”:")
        output.append(result["ai_response"])
        
```
        output.append("\nè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°:")
        for metric, score in result['similarity_scores'].items():
            output.append(f"{metric}: {score:.4f}")
```
        
```
        output.append("\nè¯¦ç»†åŒ¹é…åˆ†æ:")
        for metric, details in result['match_analysis'].items():
            output.append(f"\n{metric}åˆ†æ:")
            output.append(f"  å¾—åˆ†: {details.score:.4f}")
            output.append(f"  é˜ˆå€¼: {details.threshold:.4f}")
            output.append(f"  å·®è·: {details.difference:.4f}")
            output.append(f"  çŠ¶æ€: {details.status}")
            output.append(f"  ç½®ä¿¡åº¦: {details.confidence}")
```
        
        output.append("\nç›®æ ‡åŒ¹é…ç»“æœ:")
        output.append(f"æ˜¯å¦åŒ¹é…ç›®æ ‡: {'æ˜¯' if result['is_target_match'] else 'å¦'}")
        
```
        if result['is_target_match']:
            output.append("ç›®æ ‡åŒ¹é…æˆåŠŸï¼")
            min_confidence = min(
                details.confidence 
                for details in result['match_analysis'].values()
            )
            output.append(f"æ•´ä½“ç½®ä¿¡åº¦: {min_confidence}")
        else:
            output.append("æœªè¾¾åˆ°åŒ¹é…é˜ˆå€¼ï¼Œå»ºè®®æ£€æŸ¥ä»¥ä¸‹æ–¹é¢:")
            failed_metrics = [
                metric for metric, details in result['match_analysis'].items()
                if details.status == "æœªé€šè¿‡"
            ]
            output.extend([f"  - {metric}" for metric in failed_metrics])
```
        
        return "\n".join(output)
    
```
    def main():
        """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ImageSemanticAnalyzer"""
        try:
            # è®¾ç½®è‡ªå®šä¹‰é˜ˆå€¼
            custom_threshold = {
                'precision': 0.7,
                'recall': 0.7,
                'f1': 0.7
            }
```
            
```
            # åˆå§‹åŒ–åˆ†æå™¨
            api_key = "your-api-key-here"  # æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥
            analyzer = ImageSemanticAnalyzer(
                api_key=api_key, 
                threshold=custom_threshold,
                log_results=True  # å¯ç”¨ç»“æœè®°å½•
            )
```
            
```
            # æµ‹è¯•promptå’Œå›¾ç‰‡URL
            prompt = "å›¾ä¸­æ˜¯å¦æœ‰ä¸€ä¸ªç©¿ç€é»‘è‰²æ— è¢–ä¸Šè¡£ã€çº¢è‰²è£¤å­çš„å¥³æ€§ï¼Ÿå¦‚æœæœ‰ï¼Œè¯·è¯¦ç»†æè¿°å¥¹çš„ä½“è²Œç‰¹å¾ã€‚"
            image_url = "https://cdn.pixabay.com/photo/2013/10/09/20/47/large-193357_1280.jpg"
```
            
            # è¿›è¡Œåˆ†æ
            result = analyzer.analyze_and_evaluate(prompt, image_url)
            
```
            # æ ¼å¼åŒ–å¹¶æ‰“å°ç»“æœ
            formatted_result = format_analysis_result(result)
            print(formatted_result)
```
            
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    if __name__ == "__main__":
        main()
    
    
    !pip install mistralai torch bert-score matplotlib seaborn numpy
    
```
    import os
    from mistralai import Mistral
    import torch
    from bert_score import BERTScorer
    from typing import List, Dict, Union, Tuple
    from dataclasses import dataclass
    from datetime import datetime
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from pathlib import Path
```
    
```
    @dataclass
    class MatchDetails:
        """åŒ¹é…è¯¦æƒ…æ•°æ®ç±»"""
        score: float
        threshold: float
        difference: float
        status: str
        confidence: str
```
    
```
    class ImageSemanticAnalyzer:
        def __init__(self, 
                     api_key: str = None, 
                     model_type: str = "bert-base-chinese", 
                     lang: str = "zh",
                     threshold: Dict[str, float] = None,
                     log_results: bool = True,
                     output_dir: str = "analysis_results"):
            """åˆå§‹åŒ–å›¾åƒè¯­ä¹‰åˆ†æå™¨"""
            self.threshold = threshold or {
                'precision': 0.7,
                'recall': 0.7,
                'f1': 0.7
            }
```
            
```
            self.log_results = log_results
            self.output_dir = Path(output_dir)
            if log_results:
                self.output_dir.mkdir(parents=True, exist_ok=True)
```
                
```
            self.api_key = userdata.get('MISTRAL_API_KEY')
            if not self.api_key:
                raise ValueError(
                    "No API key provided. Please either pass the API key directly "
                    "or set the MISTRAL_API_KEY environment variable."
                )
```
            
```
            try:
                self.model = "pixtral-large-latest"
                self.client = Mistral(api_key=self.api_key)
                self.scorer = BERTScorer(model_type=model_type, lang=lang)
```
                
```
                # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
                plt.rcParams['font.sans-serif'] = ['SimHei']
                plt.rcParams['axes.unicode_minus'] = False
```
                
            except Exception as e:
                raise Exception(f"Failed to initialize Mistral client: {str(e)}")
    
```
        def analyze_image(self, prompt: str, image_url: str) -> str:
            """åˆ†æå›¾åƒå¹¶è¿”å›AIçš„æè¿°"""
            try:
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": image_url
                            }
                        ]
                    }
                ]
```
                
```
                chat_response = self.client.chat.complete(
                    model=self.model,
                    messages=messages
                )
```
                
                return chat_response.choices[0].message.content
                
            except Exception as e:
                raise Exception(f"Error during image analysis: {str(e)}")
    
```
        def calculate_semantic_similarity(self, prompt: str, ai_response: str) -> Tuple[Dict[str, float], bool]:
            """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¹¶åˆ¤æ–­æ˜¯å¦åŒ¹é…ç›®æ ‡"""
            try:
                references = [prompt]
                candidates = [ai_response]
```
                
                P, R, F1 = self.scorer.score(candidates, references)
                
```
                scores = {
                    "Precision": P[0].item(),
                    "Recall": R[0].item(),
                    "F1": F1[0].item()
                }
```
                
```
                is_match = (
                    scores["Precision"] >= self.threshold['precision'] and
                    scores["Recall"] >= self.threshold['recall'] and
                    scores["F1"] >= self.threshold['f1']
                )
```
                
                return scores, is_match
                
            except Exception as e:
                raise Exception(f"Error calculating semantic similarity: {str(e)}")
    
```
        def analyze_match_details(self, similarity_scores: Dict[str, float]) -> Dict[str, MatchDetails]:
            """åˆ†æåŒ¹é…ç»†èŠ‚ï¼Œæä¾›æ¯ä¸ªæŒ‡æ ‡çš„è¯¦ç»†è¯„ä¼°"""
            analysis = {}
```
            
```
            for metric, score in similarity_scores.items():
                threshold = self.threshold[metric.lower()]
                difference = score - threshold
                status = "é€šè¿‡" if score >= threshold else "æœªé€šè¿‡"
                confidence = self._calculate_confidence(difference)
```
                
```
                analysis[metric] = MatchDetails(
                    score=score,
                    threshold=threshold,
                    difference=difference,
                    status=status,
                    confidence=confidence
                )
```
            
            return analysis
    
```
        def _calculate_confidence(self, difference: float) -> str:
            """æ ¹æ®ä¸é˜ˆå€¼çš„å·®è·è®¡ç®—ç½®ä¿¡åº¦çº§åˆ«"""
            if difference >= 0.1:
                return "é«˜"
            elif difference >= 0.05:
                return "ä¸­"
            elif difference >= 0:
                return "ä½"
            else:
                return "ä¸æ»¡è¶³"
```
    
```
        def count_matched_features(self, ai_response: str, target_features: List[str]) -> Dict[str, Union[int, float, List[str]]]:
            """è®¡ç®—ç›®æ ‡ç‰¹å¾çš„åŒ¹é…æ•°é‡"""
            matched_features = []
            total_features = len(target_features)
```
            
```
            for feature in target_features:
                if feature.lower() in ai_response.lower():
                    matched_features.append(feature)
```
            
            match_count = len(matched_features)
            match_rate = match_count / total_features if total_features > 0 else 0
            
```
            return {
                "total_features": total_features,
                "matched_count": match_count,
                "match_rate": match_rate,
                "matched_features": matched_features,
                "unmatched_features": [f for f in target_features if f not in matched_features]
            }
```
    
```
        def visualize_results(self, result: Dict, timestamp: str) -> str:
            """ç”Ÿæˆåˆ†æç»“æœçš„å¯è§†åŒ–æŠ¥å‘Š"""
            plt.figure(figsize=(15, 10))
```
            
```
            # 1. ç›¸ä¼¼åº¦åˆ†æ•°æ¡å½¢å›¾
            plt.subplot(2, 2, 1)
            scores = result['similarity_scores']
            sns.barplot(x=list(scores.keys()), y=list(scores.values()))
            plt.axhline(y=0.7, color='r', linestyle='--', label='é˜ˆå€¼')
            plt.title('è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°')
            plt.ylim(0, 1)
```
            
```
            # 2. ç½®ä¿¡åº¦é›·è¾¾å›¾
            metrics = list(result['match_analysis'].keys())
            confidence_map = {'é«˜': 3, 'ä¸­': 2, 'ä½': 1, 'ä¸æ»¡è¶³': 0}
            confidence_scores = [confidence_map[result['match_analysis'][m].confidence] for m in metrics]
```
            
```
            angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)
            confidence_scores.append(confidence_scores[0])
            angles = np.concatenate((angles, [angles[0]]))
```
            
```
            ax = plt.subplot(2, 2, 2, projection='polar')
            ax.plot(angles, confidence_scores)
            ax.fill(angles, confidence_scores, alpha=0.25)
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(metrics)
            plt.title('ç½®ä¿¡åº¦åˆ†å¸ƒ')
```
            
```
            # 3. ç‰¹å¾åŒ¹é…åˆ†æ
            if 'feature_matches' in result:
                plt.subplot(2, 2, 3)
                match_data = result['feature_matches']
                plt.pie([match_data['matched_count'], 
                        len(match_data['unmatched_features'])],
                       labels=['å·²åŒ¹é…', 'æœªåŒ¹é…'],
                       autopct='%1.1f%%')
                plt.title('ç‰¹å¾åŒ¹é…ç‡')
```
            
```
            # ä¿å­˜ç»“æœ
            viz_path = self.output_dir / f'analysis_viz_{timestamp}.png'
            plt.suptitle(f'å›¾åƒåˆ†æç»“æœå¯è§†åŒ– - {timestamp}')
            plt.tight_layout()
            plt.savefig(viz_path)
            plt.close()
```
            
            return str(viz_path)
    
```
        def log_result(self, result: Dict, timestamp: str) -> str:
            """è®°å½•åˆ†æç»“æœåˆ°æ–‡ä»¶"""
            if not self.log_results:
                return ""
```
                
            log_path = self.output_dir / f'analysis_log_{timestamp}.txt'
            
```
            try:
                with open(log_path, "w", encoding="utf-8") as f:
                    f.write("=== å›¾åƒåˆ†æç»“æœ ===\n\n")
                    f.write(f"æ—¶é—´: {timestamp}\n\n")
```
                    
                    f.write("AIå“åº”:\n")
                    f.write(f"{result['ai_response']}\n\n")
                    
```
                    f.write("è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°:\n")
                    for metric, score in result['similarity_scores'].items():
                        f.write(f"{metric}: {score:.4f}\n")
```
                    
```
                    f.write("\nè¯¦ç»†åŒ¹é…åˆ†æ:\n")
                    for metric, details in result['match_analysis'].items():
                        f.write(f"\n{metric}åˆ†æ:\n")
                        f.write(f"  å¾—åˆ†: {details.score:.4f}\n")
                        f.write(f"  é˜ˆå€¼: {details.threshold:.4f}\n")
                        f.write(f"  å·®è·: {details.difference:.4f}\n")
                        f.write(f"  çŠ¶æ€: {details.status}\n")
                        f.write(f"  ç½®ä¿¡åº¦: {details.confidence}\n")
```
                    
```
                    if 'feature_matches' in result:
                        f.write("\nç‰¹å¾åŒ¹é…åˆ†æ:\n")
                        feature_matches = result['feature_matches']
                        f.write(f"æ€»ç‰¹å¾æ•°: {feature_matches['total_features']}\n")
                        f.write(f"å·²åŒ¹é…æ•°: {feature_matches['matched_count']}\n")
                        f.write(f"åŒ¹é…ç‡: {feature_matches['match_rate']*100:.1f}%\n")
```
                        
```
                        f.write("\nå·²åŒ¹é…ç‰¹å¾:\n")
                        for feature in feature_matches['matched_features']:
                            f.write(f"- {feature}\n")
```
                        
```
                        f.write("\næœªåŒ¹é…ç‰¹å¾:\n")
                        for feature in feature_matches['unmatched_features']:
                            f.write(f"- {feature}\n")
```
                    
                    f.write("\nç›®æ ‡åŒ¹é…ç»“æœ:\n")
                    f.write(f"æ˜¯å¦åŒ¹é…ç›®æ ‡: {'æ˜¯' if result['is_target_match'] else 'å¦'}\n")
                    
                return str(log_path)
                    
```
            except Exception as e:
                print(f"Warning: Failed to log results: {str(e)}")
                return ""
```
    
```
        def analyze_and_evaluate(self, prompt: str, image_url: str, target_features: List[str] = None) -> Dict:
            """å®Œæ•´çš„åˆ†ææµç¨‹ï¼šåŒ…å«ç‰¹å¾åŒ¹é…å’Œå¯è§†åŒ–"""
            try:
                # åŸºæœ¬åˆ†æ
                ai_response = self.analyze_image(prompt, image_url)
                similarity_scores, is_match = self.calculate_semantic_similarity(prompt, ai_response)
                match_analysis = self.analyze_match_details(similarity_scores)
```
                
```
                # æ•´åˆç»“æœ
                result = {
                    "ai_response": ai_response,
                    "similarity_scores": similarity_scores,
                    "is_target_match": is_match,
                    "match_analysis": match_analysis
                }
```
                
```
                # ç‰¹å¾åŒ¹é…åˆ†æ
                if target_features:
                    result['feature_matches'] = self.count_matched_features(
                        ai_response, 
                        target_features
                    )
```
                
                # ç”Ÿæˆæ—¶é—´æˆ³
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
```
                # è®°å½•å’Œå¯è§†åŒ–
                if self.log_results:
                    log_path = self.log_result(result, timestamp)
                    viz_path = self.visualize_results(result, timestamp)
                    result['log_path'] = log_path
                    result['visualization_path'] = viz_path
```
                
                return result
                
            except Exception as e:
                raise Exception(f"Error in analysis and evaluation: {str(e)}")
    
```
    def format_analysis_result(result: Dict) -> str:
        """æ ¼å¼åŒ–åˆ†æç»“æœä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²"""
        output = []
```
        
        output.append("AIå“åº”:")
        output.append(result["ai_response"])
        
```
        output.append("\nè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°:")
        for metric, score in result['similarity_scores'].items():
            output.append(f"{metric}: {score:.4f}")
```
        
```
        output.append("\nè¯¦ç»†åŒ¹é…åˆ†æ:")
        for metric, details in result['match_analysis'].items():
            output.append(f"\n{metric}åˆ†æ:")
            output.append(f"  å¾—åˆ†: {details.score:.4f}")
            output.append(f"  é˜ˆå€¼: {details.threshold:.4f}")
            output.append(f"  å·®è·: {details.difference:.4f}")
            output.append(f"  çŠ¶æ€: {details.status}")
            output.append(f"  ç½®ä¿¡åº¦: {details.confidence}")
```
        
```
        if 'feature_matches' in result:
            feature_matches = result['feature_matches']
            output.append("\n\nç‰¹å¾åŒ¹é…åˆ†æ:")
            output.append(f"æ€»ç‰¹å¾æ•°: {feature_matches['total_features']}")
            output.append(f"å·²åŒ¹é…æ•°: {feature_matches['matched_count']}")
            output.append(f"åŒ¹é…ç‡: {feature_matches['match_rate']*100:.1f}%")
```
            
```
            output.append("\nå·²åŒ¹é…ç‰¹å¾:")
            for feature in feature_matches['matched_features']:
                output.append(f"- {feature}")
```
            
```
            output.append("\næœªåŒ¹é…ç‰¹å¾:")
            for feature in feature_matches['unmatched_features']:
                output.append(f"- {feature}")
```
        
        output.append("\nç›®æ ‡åŒ¹é…ç»“æœ:")
        output.append(f"æ˜¯å¦åŒ¹é…ç›®æ ‡: {'æ˜¯' if result['is_target_match'] else 'å¦'}")
        
```
        if result['is_target_match']:
            output.append("ç›®æ ‡åŒ¹é…æˆåŠŸï¼")
            min_confidence = min(
                details.confidence 
                for details in result['match_analysis'].values()
            )
            output.append(f"æ•´ä½“ç½®ä¿¡åº¦: {min_confidence}")
        else:
            output.append("æœªè¾¾åˆ°åŒ¹é…é˜ˆå€¼ï¼Œå»ºè®®æ£€æŸ¥ä»¥ä¸‹æ–¹é¢:")
            failed_metrics = [
                metric for metric, details in result['match_analysis'].items()
                if details.status == "æœªé€šè¿‡"
            ]
            output.extend([f"  - {metric}" for metric in failed_metrics])
```
        
```
        if 'log_path' in result:
            output.append(f"\nåˆ†ææ—¥å¿—å·²ä¿å­˜è‡³: {result['log_path']}")
        if 'visualization_path' in result:
            output.append(f"å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {result['visualization_path']}")
```
        
        return "\n".join(output)
    
```
    def main():
        try:
            # è®¾ç½®è‡ªå®šä¹‰é˜ˆå€¼
            custom_threshold = {
                'precision': 0.7,
                'recall': 0.7,
                'f1': 0.7
            }
```
            
            # å®šä¹‰è¾“å‡ºç›®å½•
            output_dir = "analysis_results"
            
```
            # åˆå§‹åŒ–åˆ†æå™¨ï¼ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„APIå¯†é’¥ï¼‰
            analyzer = ImageSemanticAnalyzer(
                threshold=custom_threshold,
                log_results=True,
                output_dir=output_dir
            )
```
            
```
            # å®šä¹‰ç›®æ ‡ç‰¹å¾åˆ—è¡¨
            target_features = [
                "é»‘è‰²æ— è¢–ä¸Šè¡£",
                "çº¢è‰²è£¤å­",
                "æ·±è‰²å¤´å‘",
                "å‘é«»",
                "æ£•è‰²åŒ…",
                "æµ…è‰²é‹å­"
            ]
```
            
```
            # æµ‹è¯•promptå’Œå›¾ç‰‡URL
            prompt = "å›¾ä¸­æ˜¯å¦æœ‰ä¸€ä¸ªç©¿ç€é»‘è‰²æ— è¢–ä¸Šè¡£ã€çº¢è‰²è£¤å­çš„å¥³æ€§ï¼Ÿå¦‚æœæœ‰ï¼Œè¯·è¯¦ç»†æè¿°å¥¹çš„ä½“è²Œç‰¹å¾ã€‚"
            image_url = "https://cdn.pixabay.com/photo/2013/10/09/20/47/large-193357_1280.jpg"
```
            
            # è¿›è¡Œåˆ†æ
            result = analyzer.analyze_and_evaluate(prompt, image_url, target_features)
            
```
            # æ ¼å¼åŒ–å¹¶æ‰“å°ç»“æœ
            formatted_result = format_analysis_result(result)
            print(formatted_result)
```
            
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    if __name__ == "__main__":
        main()
