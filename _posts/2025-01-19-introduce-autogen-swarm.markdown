---
layout: single
title: "å¾®è½¯æœ€å¼ºAIæ™ºèƒ½ä½“AutoGenå²è¯—çº§æ›´æ–°ï¼åŸç”Ÿæ”¯æŒMagentic-Oneå’ŒSwarmä¸Teamsï¼å¼•é¢†AIæ™ºèƒ½ä½“å…ƒå¹´ï¼æ”¯æŒollamaæœ¬åœ°éƒ¨ç½²ï¼å°ç™½ä¹Ÿèƒ½è½»æ¾æŒæ¡AIæ¡†æ¶ä¿å§†çº§æ•™ç¨‹ï¼Œè¶…è¯¦ç»†ä¸Šæ‰‹æŒ‡å—"
sidebar:
  nav: "docs"
date: 2025-01-15 00:00:00 +0800
categories: AIAgents
tags: [AIAgents, AutoGen, AIæ™ºèƒ½ä½“, ollama, Llama3.2, Magentic-One, Swarm, AI]
classes: wide
author_profile: true
---

AutoGen æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºä»£ç† AI åº”ç”¨ç¨‹åºçš„å¼€æºæ¡†æ¶ï¼Œå…è®¸å¼€å‘è€…åˆ›å»ºå¯ä»¥è‡ªä¸»è¡ŒåŠ¨æˆ–ä¸äººç±»åä½œçš„å¤šä»£ç† AI ç³»ç»Ÿã€‚AutoGen 0.4.2 æ˜¯ä¸€ä¸ªé‡è¦çš„æ›´æ–°ç‰ˆæœ¬ï¼Œå®ƒåœ¨ä»£ç†å·¥ä½œæµç¨‹ä¸­æé«˜äº†ä»£ç è´¨é‡ã€ç¨³å¥æ€§ã€é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚æ–°çš„å¼‚æ­¥ã€äº‹ä»¶é©±åŠ¨çš„æ¶æ„ä½¿ AutoGen æ›´å…·å¯æ‰©å±•æ€§å’Œç¨³å¥æ€§ï¼Œä»è€Œæ”¯æŒæ›´å¹¿æ³›çš„ä»£ç†åœºæ™¯ã€‚

### **æœ¬ç¯‡ç¬”è®°æ‰€å¯¹åº”çš„è§†é¢‘ï¼š**
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡å“”å“©å“”å“©è§‚çœ‹](https://www.bilibili.com/video/BV1uWwCePEYu/)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡YouTubeè§‚çœ‹](https://youtu.be/IrTEDPnEVvU)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¼€æºé¡¹ç›®](https://github.com/win4r/AISuperDomain)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ è¯·æˆ‘å–å’–å•¡](https://ko-fi.com/aila)
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¾®ä¿¡ï¼šstoeng ã€åŠ æˆ‘è¯·æ³¨æ˜æ¥æ„ã€‘
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æ‰¿æ¥å¤§æ¨¡å‹å¾®è°ƒã€RAGã€AIæ™ºèƒ½ä½“ã€AIç›¸å…³åº”ç”¨å¼€å‘ç­‰é¡¹ç›®ã€‚

## AutoGen 0.4.2 çš„æ–°ç‰¹æ€§å’Œä¼˜ç‚¹

AutoGen 0.4.2 çš„æ›´æ–°å—åˆ°å¾®è½¯å†…å¤–åé¦ˆçš„å¯å‘ï¼Œæ—¨åœ¨ä½¿ AutoGen æ›´å¼ºå¤§ã€æ›´æ˜“äºæ‰©å±•ï¼Œå¹¶æ”¯æŒæ›´å¹¿æ³›çš„ä»£ç†åœºæ™¯ã€‚   ä»¥ä¸‹æ˜¯ 0.4.2 ç‰ˆæœ¬ä¸­çš„ä¸€äº›ä¸»è¦ç‰¹æ€§å’Œæ”¹è¿›ï¼š

**1. å¼‚æ­¥æ¶ˆæ¯ä¼ é€’**

ä»£ç†ä¹‹é—´é€šè¿‡å¼‚æ­¥æ¶ˆæ¯è¿›è¡Œé€šä¿¡ï¼Œæ”¯æŒäº‹ä»¶é©±åŠ¨å’Œè¯·æ±‚/å“åº”äº¤äº’æ¨¡å¼ã€‚  è¿™æ„å‘³ç€ä»£ç†å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œå¹¶åœ¨éœ€è¦æ—¶ç›¸äº’å‘é€æ¶ˆæ¯ï¼Œè€Œæ— éœ€ç­‰å¾…å“åº”ã€‚è¿™æé«˜äº†ç³»ç»Ÿçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§é‡ä»£ç†æˆ–éœ€è¦å®æ—¶å“åº”çš„åœºæ™¯ä¸­ã€‚å¼‚æ­¥é€šä¿¡æ˜¯æ„å»ºå“åº”å¼å’Œå¯æ‰©å±•çš„å¤šä»£ç†ç³»ç»Ÿçš„å…³é”®ï¼Œå› ä¸ºå®ƒå…è®¸ä»£ç†ç‹¬ç«‹æ“ä½œå¹¶ä»¥æ›´çµæ´»å’ŒåŠ¨æ€çš„æ–¹å¼è¿›è¡Œäº¤äº’ã€‚ä¸ä¼ ç»Ÿçš„åŒæ­¥é€šä¿¡ç›¸æ¯”ï¼Œå¼‚æ­¥é€šä¿¡èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¹¶å‘æ€§ã€å»¶è¿Ÿå’Œæ•…éšœï¼Œä½¿å…¶æˆä¸ºæ„å»ºå¤æ‚ã€å®æ—¶ AI åº”ç”¨ç¨‹åºçš„ç†æƒ³é€‰æ‹©ã€‚

**2. æ¨¡å—åŒ–å’Œå¯æ‰©å±•æ€§**

ç”¨æˆ·å¯ä»¥ä½¿ç”¨å¯æ’æ‹”ç»„ä»¶è½»æ¾å®šåˆ¶ç³»ç»Ÿï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰ä»£ç†ã€å·¥å…·ã€å†…å­˜å’Œæ¨¡å‹ã€‚  ä»–ä»¬è¿˜å¯ä»¥ä½¿ç”¨äº‹ä»¶é©±åŠ¨æ¨¡å¼æ„å»ºä¸»åŠ¨å’Œé•¿æœŸè¿è¡Œçš„ä»£ç†ã€‚è¿™ç§æ¨¡å—åŒ–è®¾è®¡å…è®¸å¼€å‘è€…æ ¹æ®ç‰¹å®šéœ€æ±‚çµæ´»åœ°ç»„åˆå’Œæ‰©å±• AutoGen æ¡†æ¶ï¼Œä»è€Œæ„å»ºå„ç§ç±»å‹çš„ä»£ç† AI åº”ç”¨ç¨‹åºã€‚

**3. å¯è§‚å¯Ÿæ€§å’Œè°ƒè¯•**

å†…ç½®çš„æŒ‡æ ‡è·Ÿè¸ªã€æ¶ˆæ¯è·Ÿè¸ªå’Œè°ƒè¯•å·¥å…·æä¾›äº†å¯¹ä»£ç†äº¤äº’å’Œå·¥ä½œæµç¨‹çš„ç›‘æ§å’Œæ§åˆ¶ï¼Œå¹¶æ”¯æŒ OpenTelemetry ä»¥å®ç°è¡Œä¸šæ ‡å‡†çš„å¯è§‚å¯Ÿæ€§ã€‚  OpenTelemetry æ˜¯ä¸€ç»„ APIã€SDKã€å·¥å…·å’Œé›†æˆï¼Œæ—¨åœ¨åˆ›å»ºå’Œç®¡ç†é¥æµ‹æ•°æ®ï¼Œä¾‹å¦‚è·Ÿè¸ªã€æŒ‡æ ‡å’Œæ—¥å¿—ã€‚é€šè¿‡æ”¯æŒ OpenTelemetryï¼ŒAutoGen å…è®¸å¼€å‘è€…æ”¶é›†å’Œåˆ†æä»£ç†è¡Œä¸ºæ•°æ®ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£ç³»ç»Ÿæ€§èƒ½ã€è¯†åˆ«æ½œåœ¨é—®é¢˜å¹¶è¿›è¡Œä¼˜åŒ–ã€‚è¿™ä½¿å¾—å¼€å‘è€…èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ä»£ç†çš„è¡Œä¸ºï¼Œå¹¶æ›´å®¹æ˜“åœ°è¯†åˆ«å’Œè§£å†³æ½œåœ¨é—®é¢˜ã€‚

**4. å¯æ‰©å±•æ€§å’Œåˆ†å¸ƒå¼**

ç”¨æˆ·å¯ä»¥è®¾è®¡å¤æ‚çš„åˆ†å¸ƒå¼ä»£ç†ç½‘ç»œï¼Œè¿™äº›ç½‘ç»œå¯ä»¥è·¨ç»„ç»‡è¾¹ç•Œæ— ç¼è¿è¡Œã€‚  è¿™ä½¿å¾— AutoGen èƒ½å¤Ÿæ”¯æŒå¤§å‹ã€å¤æ‚çš„ AI åº”ç”¨ç¨‹åºï¼Œè¿™äº›åº”ç”¨ç¨‹åºéœ€è¦å¤šä¸ªä»£ç†ååŒå·¥ä½œä»¥å®ç°å…±åŒç›®æ ‡ã€‚

**5. å†…ç½®å’Œç¤¾åŒºæ‰©å±•**

æ‰©å±•æ¨¡å—é€šè¿‡ç”¨äºä»£ç†å·¥ä½œæµç¨‹çš„é«˜çº§æ¨¡å‹å®¢æˆ·ç«¯ã€ä»£ç†ã€å¤šä»£ç†å›¢é˜Ÿå’Œå·¥å…·å¢å¼ºäº†æ¡†æ¶çš„åŠŸèƒ½ã€‚  ç¤¾åŒºæ”¯æŒå…è®¸å¼€æºå¼€å‘äººå‘˜ç®¡ç†è‡ªå·±çš„æ‰©å±•ã€‚è¿™ä¸º AutoGen ç”Ÿæ€ç³»ç»Ÿå¸¦æ¥äº†æ›´å¤§çš„çµæ´»æ€§å’Œå¤šæ ·æ€§ï¼Œå¹¶å…è®¸å¼€å‘è€…æ ¹æ®ç‰¹å®šéœ€æ±‚å®šåˆ¶å’Œæ‰©å±•æ¡†æ¶ã€‚

**6. è·¨è¯­è¨€æ”¯æŒ**

æ­¤æ›´æ–°æ—¨åœ¨æ”¯æŒä½¿ç”¨ä¸åŒç¼–ç¨‹è¯­è¨€æ„å»ºçš„ä»£ç†ä¹‹é—´çš„äº’æ“ä½œæ€§ï¼Œç›®å‰æ”¯æŒ Pythonï¼Œè€Œ .NET å’Œå…¶ä»–è¯­è¨€æ­£åœ¨å¼€å‘ä¸­ã€‚  è¿™ä½¿å¾—å¼€å‘è€…èƒ½å¤Ÿåˆ©ç”¨ä¸åŒè¯­è¨€çš„ä¼˜åŠ¿ï¼Œå¹¶æ›´å®¹æ˜“åœ°å°† AutoGen é›†æˆåˆ°ç°æœ‰çš„ç³»ç»Ÿä¸­ã€‚

**7. å®Œæ•´çš„ç±»å‹æ”¯æŒ**

æ¥å£åœ¨æ„å»ºæ—¶å¼ºåˆ¶æ‰§è¡Œç±»å‹æ£€æŸ¥ï¼Œä»è€Œæé«˜äº†ç¨³å¥æ€§å¹¶ç»´æŠ¤äº†ä»£ç è´¨é‡ã€‚  è¿™æœ‰åŠ©äºå‡å°‘é”™è¯¯å’Œæé«˜ä»£ç çš„å¯è¯»æ€§ï¼Œä»è€Œä½¿å¼€å‘è¿‡ç¨‹æ›´åŠ é«˜æ•ˆã€‚

### ğŸš€å®‰è£…AutoGenä¾èµ–

```jsx
# é…ç½®ç¯å¢ƒ
python3 -m venv .venv
source .venv/bin/activate

# ä¸‹è½½
!pip install -U "autogen-agentchat" "autogen-ext[openai,magentic-one,azure]" "litellm[proxy]" nest_asyncio pyngrok yfinance google-search-results rich
# AutoGen Studio
pip install -U "autogenstudio"

#å¯åŠ¨
autogenstudio ui --port 8081 --host 0.0.0.0

# åœç”¨
deactivate
```

### **LiteLLM & Ollama**

```jsx
curl -fsSL https://ollama.com/install.sh | sh

ollama pull llama3.2:1b

litellm --model ollama/llama3.2:1b
```

### âœ…åœ¨Colabä¸­ä½¿ç”¨ç»ˆç«¯å‘½ä»¤è¡Œçš„åŠŸèƒ½

```python
#å®‰è£… colab-xterm åŒ…
#https://pypi.org/project/colab-xterm/
!pip install colab-xterm 

#åŠ è½½åˆšæ‰å®‰è£…çš„ colab-xterm æ‰©å±•
%load_ext colabxterm

#åœ¨ Colab ç¬”è®°æœ¬ä¸­æ‰“å¼€ä¸€ä¸ªäº¤äº’å¼ç»ˆç«¯çª—å£
%xterm
```

### âœ…AutoGen Studio

```python
!pip install pyngrok

!ngrok authtoken xxxxxxx
```

```python
from pyngrok import ngrok
url = ngrok.connect(8081)
print(url)

#NgrokTunnel: "https://a720-35-232-234-177.ngrok-free.app" -> "http://localhost:8081"
```

### âœ…åœ¨Colabç»ˆç«¯å®‰è£…ollamaå¹¶ä¸”æ‹‰å–æ¨¡å‹å’Œè¿è¡ŒæœåŠ¡

```python
curl https://ollama.ai/install.sh | sh
ollama serve & ollama pull llama3.2
```

### ğŸš€AutoGenä»£ç -è°ƒç”¨ollamaæœ¬åœ°æ¨¡å‹ä»¥åŠç¬¬ä¸‰æ–¹APIç¤ºä¾‹

```jsx
!pip install nest_asyncio
```

```python
from google.colab import userdata
api_key=userdata.get('OPENROUTER_API_KEY')
api_key_2=userdata.get('HYPERBOLIC_API_KEY')
api_key_nvidia_nim=userdata.get('NVIDIA_NIM_API_KEY')
```

```python
import nest_asyncio
nest_asyncio.apply()

import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
from google.colab import userdata
from rich.console import Console
from rich.panel import Panel

console = Console()

# # ollamaæœ¬åœ°éƒ¨ç½²
def get_model_client_ollama() -> OpenAIChatCompletionClient:  # type: ignore
    return OpenAIChatCompletionClient(
        model="llama3.2:latest",
        api_key="ollama",
        base_url="http://localhost:11434/v1",
        model_capabilities={
            "json_output": False,
            "vision": False,
            "function_calling": True,
        },
    )

async def main() -> None:

    model_client = get_model_client_ollama()

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)

    termination = MaxMessageTermination(11)

    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    
    async for message in stream:
        if hasattr(message, 'content'):
            # Print message in panel with source as title
            console.print(Panel(
                message.content,
                title=f"[bold blue]{message.source}[/bold blue]",
                border_style="blue"
            ))
            
            # Print usage statistics if available
            if message.models_usage:
                console.print(f"[dim]Usage - Prompt tokens: {message.models_usage.prompt_tokens}, "
                         f"Completion tokens: {message.models_usage.completion_tokens}[/dim]")
            console.print("â€•" * 80)  # Separator line
        else:  # TaskResult object
            console.print("\n[bold yellow]Task Result Summary[/bold yellow]")
            console.print(f"Stop Reason: {message.stop_reason}")
            console.print("â€•" * 80)

asyncio.run(main())
```

# âœ…Teams

AutoGen Teams æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œå…¶ä¸»è¦ç‰¹ç‚¹æ˜¯ï¼š

1. ç”¨é€”ï¼šç”¨äºéœ€è¦å¤šä¸ª AI ä»£ç†åä½œå®Œæˆçš„å¤æ‚ä»»åŠ¡ã€‚
2. å·¥ä½œæ–¹å¼ï¼š
- å¤šä¸ªä»£ç†ä»¥è½®è¯¢æ–¹å¼(round-robin)è¿›è¡Œäº¤äº’
- æ¯ä¸ªä»£ç†éƒ½èƒ½çœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡
- ä»£ç†ä¹‹é—´å¯ä»¥ç›¸äº’å›åº”å’Œè¯„ä»·
1. ä¸»è¦åŠŸèƒ½ï¼š
- å¯åˆ›å»ºå’Œè¿è¡Œä»£ç†å›¢é˜Ÿ
- æ”¯æŒå®æ—¶è§‚å¯Ÿä»£ç†äº¤äº’è¿‡ç¨‹
- å¯ä»¥æš‚åœã€æ¢å¤å’Œç»ˆæ­¢ä»»åŠ¡
- ä¿æŒå¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡
1. ä½¿ç”¨å»ºè®®ï¼š
- ç®€å•ä»»åŠ¡å»ºè®®ç”¨å•ä¸ªä»£ç†
- åªæœ‰åœ¨å•ä¸ªä»£ç†æ— æ³•èƒœä»»æ—¶æ‰è€ƒè™‘ä½¿ç”¨å›¢é˜Ÿ
- éœ€è¦æ›´å¤šç»“æ„åŒ–å¼•å¯¼å’Œæ§åˆ¶

è®©å¤æ‚ä»»åŠ¡èƒ½å¤Ÿé€šè¿‡å¤šä¸ªä¸“ä¸šä»£ç†çš„åä½œæ¥å®Œæˆï¼Œæ¯ä¸ªä»£ç†å¯ä»¥å‘æŒ¥è‡ªå·±çš„ä¸“é•¿å¹¶ç›¸äº’è¡¥å……ã€‚

```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.base import TaskResult
from autogen_agentchat.conditions import ExternalTermination, TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

# # ollamaæœ¬åœ°éƒ¨ç½²
def get_model_client_ollama() -> OpenAIChatCompletionClient:  # type: ignore
    return OpenAIChatCompletionClient(
        model="llama3.2:latest",
        api_key="ollama",
        base_url="http://localhost:11434/v1",
        model_capabilities={
            "json_output": False,
            "vision": False,
            "function_calling": True,
        },
    )

## OpenRouter
def get_model_client_OpenRouter() -> OpenAIChatCompletionClient:  # type: ignore
    "Mimic OpenAI API using Local LLM Server."
    return OpenAIChatCompletionClient(
        model="microsoft/phi-4",
        api_key=api_key,
        base_url="https://openrouter.ai/api/v1",
        model_capabilities={
            "json_output": False,
            "vision": False,
            "function_calling": True,
        },
    )

## Nvidia NIM
def get_model_client_NIM() -> OpenAIChatCompletionClient:  # type: ignore
    return OpenAIChatCompletionClient(
        model="meta/llama-3.3-70b-instruct",
        api_key=api_key_nvidia_nim,
        base_url="https://integrate.api.nvidia.com/v1",
        model_capabilities={
            "json_output": True,
            "vision": False,
            "function_calling": True,
        },
    )

## å…¶ä»–ç¬¬ä¸‰æ–¹æµ‹è¯•
def get_model_client_other() -> OpenAIChatCompletionClient:  # type: ignore
    return OpenAIChatCompletionClient(
        model="meta-llama/Llama-3.3-70B-Instruct",
        api_key=api_key_2,
        base_url="https://api.hyperbolic.xyz/v1",
        model_capabilities={
            "json_output": False,
            "vision": False,
            "function_calling": True,
        },
    )

# # ollamaæœ¬åœ°éƒ¨ç½²
def get_model_client_ollama() -> OpenAIChatCompletionClient:  # type: ignore
    return OpenAIChatCompletionClient(
        model="llama3.2:latest",
        api_key="ollama",
        base_url="http://localhost:11434/v1",
        model_capabilities={
            "json_output": False,
            "vision": False,
            "function_calling": True,
        },
    )

# åˆ›å»º OpenAI æ¨¡å‹å®¢æˆ·ç«¯
model_client = get_model_client_NIM()

# åˆ›å»ºPythonå¼€å‘å·¥ç¨‹å¸ˆ
Programmer_Agent = AssistantAgent(
    "programmer",
    model_client=model_client,
    system_message="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonå¼€å‘å·¥ç¨‹å¸ˆã€‚
è¯·åŸºäºéœ€æ±‚ç¼–å†™æ¸…æ™°ã€å¯ç»´æŠ¤ä¸”ç¬¦åˆPEP8è§„èŒƒçš„Pythonä»£ç ã€‚
ä»£ç è¦åŒ…å«:
- æ¸…æ™°çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²
- é€‚å½“çš„é”™è¯¯å¤„ç†
- ä»£ç æ€§èƒ½ä¼˜åŒ–
- å•å…ƒæµ‹è¯•
""",
)

# åˆ›å»ºä»£ç å®¡è®¡ä¸“å®¶
CodeReviewer_Agent = AssistantAgent(
    "code_reviewer",
    model_client=model_client,
    system_message="""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚è¯·å¯¹ä»£ç è¿›è¡Œå…¨é¢çš„è¯„å®¡,åŒ…æ‹¬:
- ä»£ç è§„èŒƒæ€§å’Œå¯è¯»æ€§
- è®¾è®¡æ¨¡å¼çš„ä½¿ç”¨
- æ€§èƒ½å’Œæ•ˆç‡
- å®‰å…¨æ€§è€ƒè™‘
- æµ‹è¯•è¦†ç›–ç‡
- æ½œåœ¨é—®é¢˜
å½“ä»£ç ç¬¦åˆè¦æ±‚æ—¶,å›å¤'åŒæ„é€šè¿‡'ã€‚""",
)

# å®šä¹‰ç»ˆæ­¢æ¡ä»¶:å½“è¯„è®ºå‘˜åŒæ„æ—¶åœæ­¢ä»»åŠ¡
text_termination = TextMentionTermination("åŒæ„é€šè¿‡")

# åˆ›å»ºä¸€ä¸ªåŒ…å«ä¸»è¦æ™ºèƒ½åŠ©æ‰‹å’Œè¯„è®ºå‘˜çš„å›¢é˜Ÿ
team = RoundRobinGroupChat([Programmer_Agent, CodeReviewer_Agent], termination_condition=text_termination)

# ç¤ºä¾‹ä»»åŠ¡:å®ç°ä¸€ä¸ªæ–‡ä»¶å¤„ç†ç±»
task = """
è¯·å®ç°ä¸€ä¸ªæ–‡ä»¶å¤„ç†ç±» FileProcessor,è¦æ±‚:
1. æ”¯æŒè¯»å–ã€å†™å…¥å’Œè¿½åŠ æ–‡æœ¬æ–‡ä»¶
2. åŒ…å«åŸºæœ¬çš„æ–‡ä»¶ç»Ÿè®¡åŠŸèƒ½(è¡Œæ•°ã€å­—ç¬¦æ•°ã€å•è¯æ•°)
3. æ”¯æŒæ–‡ä»¶åŠ å¯†/è§£å¯†åŠŸèƒ½
4. å®ç°å¼‚å¸¸å¤„ç†
5. ç¼–å†™å®Œæ•´çš„å•å…ƒæµ‹è¯•
"""

# åœ¨è„šæœ¬ä¸­è¿è¡Œæ—¶ä½¿ç”¨ `asyncio.run(...)`
result = await team.run(task=task)

def print_formatted_result(task_result):
    print("\n" + "="*60)
    print("ä»£ç è¯„å®¡è¿‡ç¨‹".center(60))
    print("="*60 + "\n")

    for msg in task_result.messages:
        if msg.source == 'user':
            print("ğŸ“‹ éœ€æ±‚æè¿°ï¼š")
        elif msg.source == 'primary':
            print("ğŸ‘¨â€ğŸ’» å¼€å‘å·¥ç¨‹å¸ˆæäº¤ï¼š")
        elif msg.source == 'critic':
            print("ğŸ” ä»£ç å®¡æŸ¥åé¦ˆï¼š")

        print("-" * 40)
        print(f"{msg.content}\n")

        if msg.models_usage:
            print(f"Tokenç»Ÿè®¡ï¼š")
            print(f"Â· æç¤ºtokens: {msg.models_usage.prompt_tokens}")
            print(f"Â· ç”Ÿæˆtokens: {msg.models_usage.completion_tokens}")
            print(f"Â· æ€»è®¡tokens: {msg.models_usage.prompt_tokens + msg.models_usage.completion_tokens}\n")

    print("="*60)
    print("è¯„å®¡ç»“æœï¼š".center(60))
    print("="*60)
    print(f"\n{task_result.stop_reason}\n")

print_formatted_result(result)
```

# âœ…Selector Group Chat

SelectorGroupChatæ˜¯AutoGenæ¡†æ¶ä¸­çš„ä¸€ä¸ªå›¢é˜Ÿåä½œç»„ä»¶,å…¶æ ¸å¿ƒåŠŸèƒ½æ˜¯:

è®©å¤šä¸ªAIæ™ºèƒ½ä½“ä»¥åŠ¨æ€è½®æµçš„æ–¹å¼è¿›è¡Œä»»åŠ¡åä½œ,ç”±æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©æœ€åˆé€‚çš„ä¸‹ä¸€ä¸ªå‘è¨€è€…ã€‚ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬:

1. åŸºäºæ¨¡å‹çš„è¯´è¯è€…é€‰æ‹© - é€šè¿‡åˆ†æå¯¹è¯å†å²å’Œæ¯ä¸ªæ™ºèƒ½ä½“çš„è§’è‰²æè¿°æ¥é€‰æ‹©ä¸‹ä¸€ä¸ªæœ€åˆé€‚çš„å‘è¨€è€…
2. é»˜è®¤é¿å…åŒä¸€æ™ºèƒ½ä½“è¿ç»­å‘è¨€(å¯é…ç½®)
3. æ”¯æŒè‡ªå®šä¹‰é€‰æ‹©å‡½æ•°è¦†ç›–é»˜è®¤çš„åŸºäºæ¨¡å‹çš„é€‰æ‹©é€»è¾‘
4. æ”¯æŒé…ç½®å„å‚ä¸è€…çš„è§’è‰²å’Œæè¿°ä¿¡æ¯

å·¥ä½œæµç¨‹:

- æ¥æ”¶ä»»åŠ¡å,åˆ†æå½“å‰å¯¹è¯ä¸Šä¸‹æ–‡é€‰æ‹©ä¸‹ä¸€ä¸ªå‘è¨€è€…
- é€‰ä¸­çš„æ™ºèƒ½ä½“æä¾›å“åº”å¹¶å¹¿æ’­ç»™å…¶ä»–å‚ä¸è€…
- æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶å†³å®šæ˜¯å¦ç»§ç»­å¯¹è¯
- è¿”å›å¯¹è¯å†å²ä½œä¸ºä»»åŠ¡ç»“æœ

è¿™ç§è®¾è®¡é€‚ç”¨äºéœ€è¦å¤šä¸ªä¸“ä¸šæ™ºèƒ½ä½“åä½œå®Œæˆçš„å¤æ‚ä»»åŠ¡åœºæ™¯ã€‚

```python
from typing import Sequence

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.messages import AgentEvent, ChatMessage
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Note: This example uses mock tools instead of real APIs for demonstration purposes
def search_web_tool(query: str) -> str:
    if "2006-2007" in query:
        return """Here are the total points scored by Miami Heat players in the 2006-2007 season:
        Udonis Haslem: 844 points
        Dwayne Wade: 1397 points
        James Posey: 550 points
        ...
        """
    elif "2007-2008" in query:
        return "The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214."
    elif "2008-2009" in query:
        return "The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398."
    return "No data found."

def percentage_change_tool(start: float, end: float) -> float:
    return ((end - start) / start) * 100
    
model_client = OpenAIChatCompletionClient(model="gpt-4o")

planning_agent = AssistantAgent(
    "PlanningAgent",
    description="An agent for planning tasks, this agent should be the first to engage when given a new task.",
    model_client=model_client,
    system_message="""
    You are a planning agent.
    Your job is to break down complex tasks into smaller, manageable subtasks.
    Your team members are:
        Web search agent: Searches for information
        Data analyst: Performs calculations

    You only plan and delegate tasks - you do not execute them yourself.

    When assigning tasks, use this format:
    1. <agent> : <task>

    After all tasks are complete, summarize the findings and end with "TERMINATE".
    """,
)

web_search_agent = AssistantAgent(
    "WebSearchAgent",
    description="A web search agent.",
    tools=[search_web_tool],
    model_client=model_client,
    system_message="""
    You are a web search agent.
    Your only tool is search_tool - use it to find information.
    You make only one search call at a time.
    Once you have the results, you never do calculations based on them.
    """,
)

data_analyst_agent = AssistantAgent(
    "DataAnalystAgent",
    description="A data analyst agent. Useful for performing calculations.",
    model_client=model_client,
    tools=[percentage_change_tool],
    system_message="""
    You are a data analyst.
    Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.
    """,
)

text_mention_termination = TextMentionTermination("TERMINATE")
max_messages_termination = MaxMessageTermination(max_messages=25)
termination = text_mention_termination | max_messages_termination

team = SelectorGroupChat(
    [planning_agent, web_search_agent, data_analyst_agent],
    model_client=OpenAIChatCompletionClient(model="gpt-4o-mini"),
    termination_condition=termination,
)

task = "Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?"

# Use asyncio.run(...) if you are running this in a script.
await Console(team.run_stream(task=task))
```

# âœ…Magentic-One

- è§†é¢‘æ¼”ç¤ºÂ [https://youtu.be/QNZZJvGnk80](https://www.google.com/url?q=https%3A%2F%2Fyoutu.be%2FQNZZJvGnk80)
- Magentic-Oneæ˜¯ä¸€ä¸ªé€šç”¨å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºè§£å†³å„ç§é¢†åŸŸä¸­æ¶‰åŠç½‘ç»œå’Œæ–‡ä»¶å¤„ç†çš„å¼€æ”¾å¼ä»»åŠ¡ã€‚
- è¯¥ç³»ç»Ÿäº2024å¹´11æœˆé¦–æ¬¡å‘å¸ƒï¼Œæœ€åˆåŸºäºautogen-coreåº“å¼€å‘ï¼Œç°å·²è¿ç§»åˆ°autogen-agentchatå¹³å°ï¼Œæä¾›äº†æ›´æ¨¡å—åŒ–å’Œæ˜“ç”¨çš„æ¥å£ã€‚
1. æ ¸å¿ƒæ¶æ„:ç³»ç»Ÿç”±ä¸€ä¸ª Orchestrator(ç¼–æ’å™¨)æ™ºèƒ½ä½“é¢†å¯¼,è´Ÿè´£:
- ä»»åŠ¡åˆ†è§£å’Œè§„åˆ’
- æŒ‡æŒ¥å…¶ä»–æ™ºèƒ½ä½“æ‰§è¡Œå­ä»»åŠ¡
- è¿½è¸ªæ•´ä½“è¿›åº¦
- å¿…è¦æ—¶é‡‡å–çº æ­£æªæ–½
1. ä¸»è¦ç»„ä»¶:
- WebSurfer: æ§åˆ¶æµè§ˆå™¨è¿›è¡Œç½‘é¡µæ“ä½œå’Œä¿¡æ¯è·å–
- FileSurfer: è¯»å–å’Œå¤„ç†æœ¬åœ°æ–‡ä»¶
- Coder: ç¼–å†™ä»£ç å’Œåˆ†æä¿¡æ¯
- ComputerTerminal: æä¾›æ§åˆ¶å°è®¿é—®ä»¥æ‰§è¡Œä»£ç 
1. å·¥ä½œæµç¨‹:
- Orchestrator åˆ›å»ºåˆå§‹è®¡åˆ’
- åœ¨ Task Ledger ä¸­è®°å½•å¿…è¦ä¿¡æ¯
- åœ¨ Progress Ledger ä¸­è¿½è¸ªè¿›åº¦
- åŠ¨æ€åˆ†é…ä»»åŠ¡ç»™å…¶ä»–æ™ºèƒ½ä½“
- å¿…è¦æ—¶è°ƒæ•´è®¡åˆ’
1. æŠ€æœ¯ç‰¹ç‚¹:
- åŸºäº autogen-agentchat æ„å»º
- æ”¯æŒå¤šç§ LLM æ¨¡å‹(æ¨è Orchestrator ä½¿ç”¨ GPT-4o)
- æä¾›å®¹å™¨åŒ–å’Œå®‰å…¨æªæ–½å»ºè®®

```python
import nest_asyncio
nest_asyncio.apply()
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.ui import Console

## OpenRouter
def get_model_client_OpenRouter() -> OpenAIChatCompletionClient:  # type: ignore
    "Mimic OpenAI API using Local LLM Server."
    return OpenAIChatCompletionClient(
        model="openai/gpt-4o-2024-11-20",
        api_key=api_key,
        base_url="https://openrouter.ai/api/v1",
        model_capabilities={
            "json_output": True,
            "vision": True,
            "function_calling": True,
        },
    )

async def main() -> None:
    model_client = get_model_client_OpenRouter()

    assistant = AssistantAgent(
        "Assistant",
        model_client=model_client,
    )
    team = MagenticOneGroupChat([assistant], model_client=model_client)
    await Console(team.run_stream(task="ç¼–å†™ä¸€ä¸ª Python è„šæœ¬ï¼Œå®ç°æå–è°·æ­Œæœç´¢ç»“æœçš„å‰åæ¡çš„æ ‡é¢˜å’Œé“¾æ¥"))

asyncio.run(main())
```

# âœ…Swarm

Swarmæ˜¯AutoGenä¸­çš„ä¸€ä¸ªå¤šæ™ºèƒ½ä½“åä½œæ¨¡å¼ï¼Œä¸»è¦ç‰¹ç‚¹æ˜¯ï¼š

1. å…è®¸æ™ºèƒ½ä½“æ ¹æ®å„è‡ªèƒ½åŠ›è‡ªä¸»ç§»äº¤ä»»åŠ¡ç»™å…¶ä»–æ™ºèƒ½ä½“ï¼Œè€Œä¸éœ€è¦ä¸­å¤®åè°ƒå™¨
2. æ‰€æœ‰æ™ºèƒ½ä½“å…±äº«ç›¸åŒçš„å¯¹è¯ä¸Šä¸‹æ–‡å’Œæ¶ˆæ¯å†å²
3. é€šè¿‡HandoffMessageæœºåˆ¶å®ç°ä»»åŠ¡ç§»äº¤ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“å¯ä»¥æŒ‡å®šå°†ä»»åŠ¡è½¬ç»™å“ªäº›å…¶ä»–æ™ºèƒ½ä½“
4. é€‚ç”¨äºéœ€è¦å¤šä¸ªä¸“å®¶æ™ºèƒ½ä½“åä½œçš„å¤æ‚ä»»åŠ¡,æ¯”å¦‚å®¢æœç³»ç»Ÿ(å®¢æœ -> é€€æ¬¾ä¸“å‘˜)æˆ–è‚¡ç¥¨ç ”ç©¶(åˆ†æå¸ˆ -> å†™æ‰‹)ç­‰åœºæ™¯

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import HandoffTermination, TextMentionTermination
from autogen_agentchat.messages import HandoffMessage
from autogen_agentchat.teams import Swarm
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

from serpapi import GoogleSearch
from typing import Any, Dict, List
import os
from datetime import datetime

import yfinance as yf

from google.colab import userdata

from google.colab import userdata
api_key=userdata.get('OPENROUTER_API_KEY')
api_key_2=userdata.get('HYPERBOLIC_API_KEY')
api_key_nvidia_nim=userdata.get('NVIDIA_NIM_API_KEY')

os.environ["SERPAPI_KEY"]=userdata.get('SERPAPI_KEY')

## OpenRouter
def get_model_client_OpenRouter() -> OpenAIChatCompletionClient:  # type: ignore
    "Mimic OpenAI API using Local LLM Server."
    return OpenAIChatCompletionClient(
        model="openai/gpt-4o-2024-11-20",
        api_key=api_key,
        base_url="https://openrouter.ai/api/v1",
        model_capabilities={
            "json_output": True,
            "vision": True,
            "function_calling": True,
        },
    )
    

async def get_stock_data(symbol: str) -> Dict[str, Any]:
    """
    Get real stock market data for a given symbol with improved error handling

    Args:
        symbol: Stock ticker symbol (e.g. 'TSLA')

    Returns:
        Dict containing price, volume, PE ratio and market cap
    """
    try:
        # åˆ›å»ºè‚¡ç¥¨å¯¹è±¡
        stock = yf.Ticker(symbol)

        # è·å–å®æ—¶ä»·æ ¼æ•°æ®
        price_info = stock.history(period='1d')
        if not price_info.empty:
            current_price = price_info['Close'].iloc[-1]
        else:
            current_price = None

        # è·å–å…¶ä»–ä¿¡æ¯
        info = stock.info

        return {
            "price": current_price,
            "volume": info.get("regularMarketVolume"),
            "pe_ratio": info.get("forwardPE"),
            "market_cap": info.get("marketCap"),
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

    except Exception as e:
        print(f"Error fetching stock data for {symbol}: {str(e)}")
        return {
            "price": None,
            "volume": None,
            "pe_ratio": None,
            "market_cap": None,
            "error": str(e)
        }

async def get_news(query: str) -> List[Dict[str, str]]:
    """Get recent news articles about a company"""
    params = {
        "engine": "google_news",
        "q": query,
        "gl": "us",
        "hl": "en",
        "api_key": os.getenv("SERPAPI_KEY"),
        "num": 3  # é™åˆ¶ç»“æœæ•°é‡
    }

    try:
        search = GoogleSearch(params)
        results = search.get_dict()

        news_items = []
        for article in results.get("news_results", []):
            # è·å–æ›´å¤šæ–‡ç« ç»†èŠ‚
            title = article.get("title", "").strip()
            source = article.get("source", {})
            source_name = source.get("name", "")
            authors = source.get("authors", [])
            author_text = f"By {', '.join(authors)}" if authors else ""

            # æå–æˆ–æ„å»ºæ‘˜è¦
            snippet = article.get("snippet", "")
            description = article.get("description", "")
            link_text = article.get("link_text", "")

            # é€‰æ‹©æœ€é•¿çš„éç©ºå†…å®¹ä½œä¸ºæ‘˜è¦
            summary_candidates = [s for s in [snippet, description, link_text] if s]
            summary = max(summary_candidates, key=len) if summary_candidates else title

            # æ ¼å¼åŒ–æ—¥æœŸ
            date_str = article.get("date", "")
            try:
                if date_str:
                    date_obj = datetime.strptime(date_str.split(",")[0], "%m/%d/%Y")
                    formatted_date = date_obj.strftime("%Y-%m-%d")
                else:
                    formatted_date = datetime.now().strftime("%Y-%m-%d")
            except:
                formatted_date = date_str

            news_items.append({
                "title": title,
                "date": formatted_date,
                "summary": f"{summary} {author_text}".strip(),
                "source": source_name
            })

        return news_items

    except Exception as e:
        print(f"Error fetching news: {str(e)}")
        return []
        
model_client = get_model_client_OpenRouter()

planner = AssistantAgent(
    "planner",
    model_client=model_client,
    handoffs=["financial_analyst", "news_analyst", "writer"],
    system_message="""ä½ æ˜¯ä¸€åç ”ç©¶è§„åˆ’åè°ƒå‘˜ã€‚
    é€šè¿‡å§”æ´¾ç»™ä¸“ä¸šæ™ºèƒ½ä½“æ¥åè°ƒå¸‚åœºç ”ç©¶ï¼š
    - é‡‘èåˆ†æå¸ˆï¼šè´Ÿè´£è‚¡ç¥¨æ•°æ®åˆ†æ
    - æ–°é—»åˆ†æå¸ˆï¼šè´Ÿè´£æ–°é—»æ”¶é›†å’Œåˆ†æ
    - æ’°å†™å‘˜ï¼šè´Ÿè´£ç¼–å†™æœ€ç»ˆæŠ¥å‘Š
    å§‹ç»ˆå…ˆå‘é€ä½ çš„è®¡åˆ’ï¼Œç„¶åå†ç§»äº¤ç»™é€‚å½“çš„æ™ºèƒ½ä½“ã€‚
    æ¯æ¬¡åªèƒ½ç§»äº¤ç»™ä¸€ä¸ªæ™ºèƒ½ä½“ã€‚
    å½“ç ”ç©¶å®Œæˆæ—¶ä½¿ç”¨ TERMINATE ç»“æŸã€‚""",
)

financial_analyst = AssistantAgent(
    "financial_analyst",
    model_client=model_client,
    handoffs=["planner"],
    tools=[get_stock_data],
    system_message="""ä½ æ˜¯ä¸€åé‡‘èåˆ†æå¸ˆã€‚
    ä½¿ç”¨ get_stock_data å·¥å…·åˆ†æè‚¡å¸‚æ•°æ®ã€‚
    æä¾›é‡‘èæŒ‡æ ‡çš„æ·±å…¥è§è§£ã€‚
    åˆ†æå®ŒæˆååŠ¡å¿…ç§»äº¤å›è§„åˆ’åè°ƒå‘˜ã€‚""",
)

news_analyst = AssistantAgent(
    "news_analyst",
    model_client=model_client,
    handoffs=["planner"],
    tools=[get_news],
    system_message="""ä½ æ˜¯ä¸€åæ–°é—»åˆ†æå¸ˆã€‚
    ä½¿ç”¨ get_news å·¥å…·æ”¶é›†å’Œåˆ†æç›¸å…³æ–°é—»ã€‚
    æ€»ç»“æ–°é—»ä¸­çš„å…³é”®å¸‚åœºè§è§£ã€‚
    åˆ†æå®ŒæˆååŠ¡å¿…ç§»äº¤å›è§„åˆ’åè°ƒå‘˜ã€‚""",
)

writer = AssistantAgent(
    "writer",
    model_client=model_client,
    handoffs=["planner"],
    system_message="""ä½ æ˜¯ä¸€åè´¢ç»æŠ¥å‘Šæ’°å†™å‘˜ã€‚
    å°†ç ”ç©¶å‘ç°ç¼–è¯‘æˆæ¸…æ™°ç®€æ´çš„æŠ¥å‘Šã€‚
    æ’°å†™å®ŒæˆååŠ¡å¿…ç§»äº¤å›è§„åˆ’åè°ƒå‘˜ã€‚""",
)

# Define termination condition
text_termination = TextMentionTermination("TERMINATE")
termination = text_termination

research_team = Swarm(
    participants=[planner, financial_analyst, news_analyst, writer], termination_condition=termination
)

task = "ä¸ºç‰¹æ–¯æ‹‰(TSLA)è‚¡ç¥¨è¿›è¡Œå¸‚åœºç ”ç©¶ï¼Œå¹¶ç”¨ä¸­æ–‡å›ç­”"
await Console(research_team.run_stream(task=task))
```

## **âœ…Travel Planning**

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

## Nvidia NIM
def get_model_client_NIM() -> OpenAIChatCompletionClient:  # type: ignore
    return OpenAIChatCompletionClient(
        model="meta/llama-3.3-70b-instruct",
        api_key=api_key_nvidia_nim,
        base_url="https://integrate.api.nvidia.com/v1",
        model_capabilities={
            "json_output": True,
            "vision": False,
            "function_calling": True,
        },
    )
    
planner_agent = AssistantAgent(
    "planner_agent",
    model_client=get_model_client_NIM(),
    description="ä¸€ä¸ªèƒ½å¤Ÿå¸®åŠ©è§„åˆ’è¡Œç¨‹çš„æ™ºèƒ½åŠ©æ‰‹",
    system_message="ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·éœ€æ±‚æä¾›æ—…è¡Œè§„åˆ’å»ºè®®çš„æ™ºèƒ½åŠ©æ‰‹ã€‚",
)

local_agent = AssistantAgent(
    "local_agent",
    model_client=get_model_client_NIM(),
    description="ä¸€ä¸ªèƒ½å¤Ÿæ¨èå½“åœ°æ´»åŠ¨å’Œæ™¯ç‚¹çš„åœ¨åœ°åŠ©æ‰‹",
    system_message="ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿä¸ºç”¨æˆ·æ¨èåœ°é“æœ‰è¶£çš„å½“åœ°æ´»åŠ¨å’Œæ™¯ç‚¹çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨æ‰€æä¾›çš„ä»»ä½•èƒŒæ™¯ä¿¡æ¯ã€‚",
)

language_agent = AssistantAgent(
    "language_agent",
    model_client=get_model_client_NIM(),
    description="ä¸€ä¸ªèƒ½å¤Ÿæä¾›ç›®çš„åœ°è¯­è¨€å»ºè®®çš„æ™ºèƒ½åŠ©æ‰‹",
    system_message="ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå®¡æŸ¥æ—…è¡Œè®¡åˆ’çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè´Ÿè´£å°±å¦‚ä½•æœ€å¥½åœ°åº”å¯¹ç›®çš„åœ°çš„è¯­è¨€æˆ–æ²Ÿé€šæŒ‘æˆ˜æä¾›é‡è¦/å…³é”®æç¤ºã€‚å¦‚æœè®¡åˆ’ä¸­å·²ç»åŒ…å«äº†è¯­è¨€æç¤ºï¼Œä½ å¯ä»¥è¯´æ˜è®¡åˆ’æ˜¯ä»¤äººæ»¡æ„çš„ï¼Œå¹¶è§£é‡ŠåŸå› ã€‚",
)

travel_summary_agent = AssistantAgent(
    "travel_summary_agent",
    model_client=get_model_client_NIM(),
    description="ä¸€ä¸ªèƒ½å¤Ÿæ€»ç»“æ—…è¡Œè®¡åˆ’çš„æ™ºèƒ½åŠ©æ‰‹",
    system_message="ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿæ•´åˆå…¶ä»–åŠ©æ‰‹çš„æ‰€æœ‰å»ºè®®å’Œæ„è§å¹¶æä¾›è¯¦ç»†æœ€ç»ˆæ—…è¡Œè®¡åˆ’çš„æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ å¿…é¡»ç¡®ä¿æœ€ç»ˆè®¡åˆ’æ˜¯å®Œæ•´ä¸”è¿è´¯çš„ã€‚ä½ çš„æœ€ç»ˆå›å¤å¿…é¡»æ˜¯å®Œæ•´çš„è®¡åˆ’ã€‚å½“è®¡åˆ’å®Œæ•´ä¸”æ‰€æœ‰è§‚ç‚¹éƒ½å·²æ•´åˆåï¼Œä½ å¯ä»¥å›å¤'TERMINATE'ã€‚",
)

termination = TextMentionTermination("TERMINATE")
group_chat = RoundRobinGroupChat(
    [planner_agent, local_agent, language_agent, travel_summary_agent], termination_condition=termination
)
await Console(group_chat.run_stream(task="åˆ¶å®šä¸€ä¸ªæ³°å›½ä¸‰æ—¥æ¸¸è®¡åˆ’."))
```

