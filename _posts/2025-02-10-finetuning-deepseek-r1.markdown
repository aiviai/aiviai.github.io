---
layout: single
title: "ğŸš€å¾®è°ƒDeepSeek-R1-Distill-Llama-8Bæ‰“é€ SQLè¯­è¨€è½¬è‡ªç„¶è¯­è¨€å¤§æ¨¡å‹ï¼å°ç™½ä¹Ÿèƒ½ååˆ†é’Ÿæ‰“é€ è‡ªå·±çš„æ¨ç†å¤§æ¨¡å‹ï¼unsloth+Colabè½»æ¾ä¸Šæ‰‹"
sidebar:
  nav: "docs"
date: 2025-02-10 00:00:00 +0800
categories: Fine-Tuning
tags: [Fine-Tuning, å¾®è°ƒ, å¤§æ¨¡å‹, DeepSeek-R1, LLMs, Text to SQL, DeepSeek-R1-Distill-Llama-8B, AI]
classes: wide
author_profile: true
---
DeepSeek-R1-Distill-Llama-8B æ˜¯ä¸€ä¸ªåŸºäº Llama æ¶æ„çš„ 8B å‚æ•°è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡æ·±åº¦è’¸é¦ï¼ˆdistillationï¼‰å¤„ç†ï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡å’Œç²¾åº¦ã€‚é€šè¿‡è’¸é¦æŠ€æœ¯ï¼Œæ¨¡å‹åœ¨ä¿æŒè¾ƒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘äº†è®¡ç®—èµ„æºçš„æ¶ˆè€—ï¼Œç‰¹åˆ«é€‚åˆåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­åº”ç”¨ã€‚è¯¥æ¨¡å‹ç»è¿‡ä¼˜åŒ–ï¼Œå¯ç”¨äºå¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ç³»ç»Ÿç­‰ã€‚DeepSeek-R1-Distill-Llama-8B ç»“åˆäº† Llama çš„å¼ºå¤§åŸºç¡€å’Œè’¸é¦æŠ€æœ¯çš„ä¼˜åŠ¿ï¼Œä½¿å¾—å®ƒåœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶æ›´åŠ é«˜æ•ˆã€‚å¯¹äºéœ€è¦å¹³è¡¡æ€§èƒ½å’Œèµ„æºæ¶ˆè€—çš„åº”ç”¨åœºæ™¯ï¼Œè¿™æ¬¾æ¨¡å‹æä¾›äº†ä¸€ä¸ªç†æƒ³çš„è§£å†³æ–¹æ¡ˆã€‚

### **æœ¬ç¯‡ç¬”è®°æ‰€å¯¹åº”çš„è§†é¢‘ï¼š**

- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡å“”å“©å“”å“©è§‚çœ‹](https://www.bilibili.com/video/BV1knNhetETR/)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡YouTubeè§‚çœ‹](https://youtu.be/MpTxJLcViuU)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¼€æºé¡¹ç›®](https://github.com/win4r/AISuperDomain)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ è¯·æˆ‘å–å’–å•¡](https://ko-fi.com/aila)
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¾®ä¿¡ï¼šstoeng ã€åŠ æˆ‘è¯·æ³¨æ˜æ¥æ„ã€‘
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æ‰¿æ¥å¤§æ¨¡å‹å¾®è°ƒã€RAGã€AIæ™ºèƒ½ä½“ã€AIç›¸å…³åº”ç”¨å¼€å‘ç­‰é¡¹ç›®ã€‚

### ğŸš€ç®€ä»‹ï¼š

UnSloth æ˜¯ä¸€ä¸ªå¼€æºçš„ Python åº“ï¼Œæ—¨åœ¨é€šè¿‡ç®€å•çš„æ¥å£å®ç°å¤§è§„æ¨¡æ¨¡å‹çš„åˆ†å¸ƒå¼è®­ç»ƒã€‚å®ƒæ”¯æŒå¤šç§ä¼˜åŒ–ç®—æ³•ï¼Œå¹¶æä¾›çµæ´»çš„è®­ç»ƒç®¡é“ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…åœ¨é«˜æ•ˆçš„åŸºç¡€ä¸Šè¿›è¡Œæ·±åº¦å­¦ä¹ å®éªŒã€‚è¯¥é¡¹ç›®çš„ç›®æ ‡æ˜¯é™ä½è®­ç»ƒå¤§æ¨¡å‹æ—¶çš„å¤æ‚æ€§ï¼Œæå‡è®¡ç®—èµ„æºçš„åˆ©ç”¨ç‡ï¼Œå¹¶æ”¯æŒå¼‚æ­¥è®­ç»ƒç­‰é«˜çº§åŠŸèƒ½ï¼Œé€‚ç”¨äºæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ é¢†åŸŸçš„å„ç§ä»»åŠ¡ã€‚æ›´å¤šè¯¦æƒ…å¯æŸ¥çœ‹å…¶ GitHub ä»“åº“ã€‚

### æˆ‘æ‰€å‘å¸ƒçš„å¾®è°ƒè§†é¢‘ï¼š

- [LitGPTæ¡†æ¶é›¶ä»£ç å¾®è°ƒPhi-4å¼€æºå¤§æ¨¡å‹](https://youtu.be/MSwltnFg1fw)
- [é›¶ä»£ç å¾®è°ƒLlama3.1 8bå¤§æ¨¡å‹ï¼ä¸­æ–‡æ–‡æœ¬åˆ†å—+æ•°æ®é›†åˆ¶ä½œï¼Axolotl+qLoRA](https://youtu.be/UCmPrNFWClI)
- [Axolotlå¾®è°ƒQwen2-7bå¼€æºå¤§æ¨¡å‹](https://youtu.be/7zw2B8upP00)
- [ç”¨AutoTrainå¾®è°ƒphi-3-mediumã€LLaMAç­‰å¼€æºå¤§æ¨¡å‹](https://youtu.be/zwW96ttLLdA)
- [å¾®è°ƒLlama3 8b SimPO](https://youtu.be/xeuofhGl3S8)
- [ç®€å•å‡ æ­¥å¾®è°ƒMistral 7b v0.3å¤§æ¨¡å‹](https://youtu.be/Ab2bLeG9XUM)
- [å¾®è°ƒPhi-3å’ŒLlama3](https://youtu.be/5u8RsxyDifQ)
- [å¾®è°ƒLlama3å®ç°åœ¨çº¿æœç´¢å¼•æ“å’ŒRAGæ£€ç´¢å¢å¼ºç”ŸæˆåŠŸèƒ½](https://youtu.be/6Gzr6sbN2pM)
- [ç®€å•å‡ æ­¥å¾®è°ƒLlama3å˜èº«ä¸­æ–‡å¤§æ¨¡å‹](https://youtu.be/oxTVzGwKeoU)
- [æœ¬åœ°å¾®è°ƒLlama3å¼€æºå¤§æ¨¡å‹](https://youtu.be/VT98s5emOiU)

### wandbé“¾æ¥ï¼š[https://wandb/](https://wandb/)

### æ•°æ®é›†ï¼š[https://huggingface.co/datasets/b-mc2/sql-create-context/viewer](https://huggingface.co/datasets/b-mc2/sql-create-context/viewer)

### ğŸš€ä»£ç 

```bash
# -*- coding: utf-8 -*-
"""CoT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pSPKk3QmWFqT5VrixE1GOb_4SpkG_2YR
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
#

from huggingface_hub import login
from google.colab import userdata

hf_token = userdata.get('HF_TOKEN')

login(hf_token)

# å¯¼å…¥wandbåº“ - Weights & Biasesï¼Œç”¨äºæœºå™¨å­¦ä¹ å®éªŒè·Ÿè¸ªå’Œå¯è§†åŒ–
import wandb

wb_token = userdata.get('WB_TOKEN')

wandb.login(key=wb_token)
run = wandb.init(
    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B sql to text',    # è®¾ç½®é¡¹ç›®åç§° - è¿™é‡Œæ˜¯ç”¨äºSQLåˆ†æçš„DeepSeekæ¨¡å‹å¾®è°ƒé¡¹ç›®
    job_type="training",
    anonymous="allow"# å…è®¸åŒ¿åè®¿é—® # "allow"è¡¨ç¤ºå³ä½¿æ²¡æœ‰wandbè´¦å·çš„ç”¨æˆ·ä¹Ÿèƒ½æŸ¥çœ‹è¿™ä¸ªé¡¹ç›®
)

# ä»unslothåº“ä¸­å¯¼å…¥FastLanguageModelç±»
# unslothæ˜¯ä¸€ä¸ªä¼˜åŒ–çš„è¯­è¨€æ¨¡å‹åŠ è½½å’Œè®­ç»ƒåº“
from unsloth import FastLanguageModel

# è®¾ç½®æ¨¡å‹å‚æ•°
# æœ€å¤§åºåˆ—é•¿åº¦ï¼Œå³æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§tokenæ•°é‡
max_seq_length = 2048
dtype = None # æ•°æ®ç±»å‹è®¾ç½®ä¸ºNoneï¼Œè®©æ¨¡å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹
load_in_4bit = True # å¯ç”¨4bité‡åŒ–åŠ è½½ # 4bité‡åŒ–å¯ä»¥æ˜¾è‘—å‡å°‘æ¨¡å‹å†…å­˜å ç”¨ï¼Œä½†å¯èƒ½ç•¥å¾®å½±å“æ¨¡å‹æ€§èƒ½

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/DeepSeek-R1-Distill-Llama-8B",
    max_seq_length = max_seq_length,     # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
    dtype = dtype,# è®¾ç½®æ•°æ®ç±»å‹
    load_in_4bit = load_in_4bit,  # å¯ç”¨4bité‡åŒ–åŠ è½½
    token = hf_token, # ä½¿ç”¨Hugging Faceçš„è®¿é—®ä»¤ç‰Œ
)

prompt = """Below is SQL query. Think like sql expert and generate a summary of the query which explains the use case of the query. As in
what the query is trying to read from the database in a usecase sense.

### Query:
SELECT
    c.customer_id,
    c.name AS customer_name,
    COUNT(o.order_id) AS total_orders,
    SUM(o.total_amount) AS total_spent,
    AVG(o.total_amount) AS avg_order_value,
    MAX(o.order_date) AS last_order_date
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id
LEFT JOIN order_items oi ON o.order_id = oi.order_id
WHERE o.order_date >= '2024-01-01'
GROUP BY c.customer_id, c.name
ORDER BY total_spent DESC
LIMIT 10;

### Response:
"""

# å°†æ¨¡å‹åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼
FastLanguageModel.for_inference(model)
# å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯å’Œç¼–ç 
# [prompt] - å°†promptæ”¾å…¥åˆ—è¡¨ä¸­ï¼Œå› ä¸ºtokenizeræœŸæœ›æ‰¹å¤„ç†è¾“å…¥
# return_tensors="pt" - è¿”å›PyTorchå¼ é‡æ ¼å¼
# to("cuda") - å°†å¼ é‡ç§»åŠ¨åˆ°GPUä¸Šè¿›è¡Œè®¡ç®—
inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

# ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå“åº”
outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=1200,
    use_cache=True,
)
# åå¤„ç†ç”Ÿæˆçš„è¾“å‡º
# batch_decode - å°†æ ‡è®°IDåºåˆ—è§£ç å›æ–‡æœ¬
# split("### Response:")[1] - æå–"### Response:"ä¹‹åçš„éƒ¨åˆ†ï¼Œå³æ¨¡å‹çš„å®é™…å›ç­”
response = tokenizer.batch_decode(outputs)
print(response[0].split("### Response:")[1])

#å¾®è°ƒå‰ï¼šæ¨¡å‹èƒ½ç»™å‡ºåŸºæœ¬çš„æŸ¥è¯¢è§£é‡Šï¼Œä½†ä¸å¤Ÿç²¾ç¡®

train_prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Instruction:
You are a SQL expert with advance understanding of SQL queries. You can understand database schema from the query. Think like sql expert and generate a summary of the query which explains the use case of the query. As in
what the query is trying to read from the database in a usecase sense.

### Query:
{}

### Response:
<think>
{}
</think>
{}"""

!pip install datasets
!wget "https://huggingface.co/datasets/b-mc2/sql-create-context/resolve/main/sql_create_context_v4.json"

from datasets import load_dataset
dataset = load_dataset("json", data_files="/content/sql_create_context_v4.json", split="train[0:500]")
# - split: æŒ‡å®šè¦åŠ è½½çš„æ•°æ®é›†åˆ‡ç‰‡
#   - "train[0:500]" è¡¨ç¤ºåªåŠ è½½è®­ç»ƒé›†çš„å‰500æ¡æ•°æ®
#   - è¿™ç§åˆ‡ç‰‡æ–¹å¼å¯ä»¥ç”¨äºå¿«é€Ÿå®éªŒå’Œè°ƒè¯•

EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN

#åŸæ•°æ®é›†ï¼šè¾“å…¥æ˜¯è‹±æ–‡æè¿°ï¼Œè¾“å‡ºæ˜¯SQLåè½¬åï¼šè¾“å…¥æ˜¯SQL(examples["answer"])ï¼Œè¾“å‡ºæ˜¯è‹±æ–‡æè¿°(examples["question"])
def switch_and_format_prompt(examples):
    inputs = examples["answer"] # ä½¿ç”¨ answer(SQL) ä½œä¸ºè¾“å…¥
    context = examples["context"]
    outputs = examples["question"] # ä½¿ç”¨ question(è‹±æ–‡æè¿°) ä½œä¸ºè¾“å‡º
    texts = []
    for input, context, output in zip(inputs, context, outputs):
        text = train_prompt_style.format(input, context, output) + EOS_TOKEN
        texts.append(text)
    return {
        "text": texts,
    }

# åº”ç”¨è½¬æ¢
dataset = dataset.map(switch_and_format_prompt, batched = True)

model = FastLanguageModel.get_peft_model(
    model,
    r=16,   # LoRAçš„ç§©(rank)å€¼ï¼Œå†³å®šäº†ä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œè¾ƒå¤§çš„rå€¼(å¦‚16)å¯ä»¥æä¾›æ›´å¼ºçš„æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼Œä½†ä¼šå¢åŠ å‚æ•°é‡å’Œè®¡ç®—å¼€é”€ï¼Œè¾ƒå°çš„rå€¼(å¦‚4æˆ–8)åˆ™ä¼šå‡å°‘å‚æ•°é‡ï¼Œä½†å¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½ï¼Œé€šå¸¸åœ¨4-16ä¹‹é—´é€‰æ‹©,éœ€è¦åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´æƒè¡¡
    target_modules=[ #æŒ‡å®šéœ€è¦åº”ç”¨LoRAå¾®è°ƒçš„æ¨¡å—åˆ—è¡¨ï¼Œq_proj, k_proj, v_proj: æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢ã€é”®ã€å€¼æŠ•å½±å±‚
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj", #æ³¨æ„åŠ›è¾“å‡ºæŠ•å½±å±‚
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    lora_alpha=16, #ç¼©æ”¾å‚æ•°ï¼Œç”¨äºæ§åˆ¶LoRAæ›´æ–°çš„å¼ºåº¦ï¼Œé€šå¸¸è®¾ç½®ä¸ºä¸rç›¸åŒçš„å€¼ï¼Œè¾ƒå¤§çš„alphaä¼šå¢åŠ LoRAçš„å½±å“åŠ›ï¼Œè¾ƒå°çš„alphaåˆ™ä¼šå‡å¼±LoRAçš„å½±å“
    lora_dropout=0, #LoRAå±‚çš„dropoutç‡ï¼Œ0è¡¨ç¤ºä¸ä½¿ç”¨dropoutï¼Œå¢åŠ dropoutå¯ä»¥å¸®åŠ©é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½†å¯èƒ½å½±å“è®­ç»ƒç¨³å®šæ€§ï¼Œåœ¨å¾®è°ƒæ—¶é€šå¸¸è®¾ä¸º0æˆ–å¾ˆå°çš„å€¼
    bias="none", #æ˜¯å¦å¾®è°ƒåç½®é¡¹ï¼Œ"none"è¡¨ç¤ºä¸å¾®è°ƒåç½®å‚æ•°ï¼Œä¹Ÿå¯ä»¥è®¾ç½®ä¸º"all"æˆ–"lora_only"æ¥å¾®è°ƒä¸åŒèŒƒå›´çš„åç½®
    use_gradient_checkpointing="unsloth",  # æ¢¯åº¦æ£€æŸ¥ç‚¹ç­–ç•¥ï¼Œ"unsloth"æ˜¯ä¸€ç§ä¼˜åŒ–çš„æ£€æŸ¥ç‚¹ç­–ç•¥ï¼Œé€‚ç”¨äºé•¿ä¸Šä¸‹æ–‡å¯ä»¥æ˜¾è‘—å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼Œä½†ä¼šç•¥å¾®å¢åŠ è®¡ç®—æ—¶é—´å¯¹å¤„ç†é•¿æ–‡æœ¬ç‰¹åˆ«æœ‰ç”¨
    random_state=3407, #éšæœºæ•°ç§å­ï¼Œæ§åˆ¶åˆå§‹åŒ–çš„éšæœºæ€§ï¼Œå›ºå®šç§å­å¯ä»¥ç¡®ä¿å®éªŒå¯é‡å¤æ€§
    use_rslora=False, #æ˜¯å¦ä½¿ç”¨RSLoRA(Rank-Stabilized LoRA) Falseè¡¨ç¤ºä½¿ç”¨æ ‡å‡†LoRARSLoRAæ˜¯ä¸€ç§æ”¹è¿›çš„LoRAå˜ä½“ï¼Œå¯ä»¥æä¾›æ›´ç¨³å®šçš„è®­ç»ƒ
    loftq_config=None, #LoftQé…ç½®Noneè¡¨ç¤ºä¸ä½¿ç”¨LoftQé‡åŒ–LoftQæ˜¯ä¸€ç§ç”¨äºæ¨¡å‹é‡åŒ–çš„æŠ€æœ¯ï¼Œå¯ä»¥å‡å°‘æ¨¡å‹å¤§å°
)

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    ## è®­ç»ƒå‚æ•°é…ç½®
    args=TrainingArguments(
        # æ‰¹å¤„ç†ç›¸å…³
        per_device_train_batch_size=2, # æ¯ä¸ªè®¾å¤‡ï¼ˆGPUï¼‰çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°
        gradient_accumulation_steps=4,# æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç”¨äºæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡å¤§å°
         # è®­ç»ƒæ­¥æ•°å’Œé¢„çƒ­
        warmup_steps=5,# å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼Œé€æ­¥å¢åŠ å­¦ä¹ ç‡
        max_steps=60,# æœ€å¤§è®­ç»ƒæ­¥æ•°
        learning_rate=2e-4,
        fp16=not is_bfloat16_supported(), # å¦‚æœä¸æ”¯æŒ bfloat16ï¼Œåˆ™ä½¿ç”¨ float16
        bf16=is_bfloat16_supported(),# å¦‚æœæ”¯æŒåˆ™ä½¿ç”¨ bfloat16ï¼Œé€šå¸¸åœ¨æ–°å‹ GPU ä¸Šæ€§èƒ½æ›´å¥½
        logging_steps=10,# æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
        optim="adamw_8bit", # ä½¿ç”¨8ä½ç²¾åº¦çš„ AdamW ä¼˜åŒ–å™¨
        weight_decay=0.01,# æƒé‡è¡°å‡ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
        lr_scheduler_type="linear",# å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼Œä½¿ç”¨çº¿æ€§è¡°å‡
        seed=3407,# éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯é‡å¤æ€§
        output_dir="outputs", # æ¨¡å‹å’Œæ£€æŸ¥ç‚¹çš„è¾“å‡ºç›®å½•
    ),
)

trainer_stats = trainer.train()

# å®šä¹‰æç¤ºæ¨¡æ¿
# è¿™ä¸ªæ¨¡æ¿åŒ…å«äº†æŒ‡å¯¼æ¨¡å‹å¦‚ä½•ç†è§£å’Œè§£é‡ŠSQLæŸ¥è¯¢çš„ç»“æ„åŒ–æç¤º
prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Instruction:
You are a SQL expert with advance understanding of SQL queries. You can understand database schema from the query. Think like sql expert and generate a summary of the query which explains the use case of the query. As in
what the query is trying to read from the database in a usecase sense.

### Query:
{}

### Response:
<think>{}"""

# å®šä¹‰æµ‹è¯•ç”¨çš„SQLæŸ¥è¯¢
# è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„å®¢æˆ·åˆ†ææŸ¥è¯¢ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹çš„ç†è§£èƒ½åŠ›
query1 = """
SELECT
    c.customer_id,
    c.name AS customer_name,
    COUNT(o.order_id) AS total_orders,
    SUM(o.total_amount) AS total_spent,
    AVG(o.total_amount) AS avg_order_value,
    MAX(o.order_date) AS last_order_date
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id
LEFT JOIN order_items oi ON o.order_id = oi.order_id
WHERE o.order_date >= '2024-01-01'
GROUP BY c.customer_id, c.name
ORDER BY total_spent DESC
LIMIT 10;
Explain use case of this query.
"""

# å°†æ¨¡å‹è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼
FastLanguageModel.for_inference(model)
# å‡†å¤‡è¾“å…¥æ•°æ®
# ä½¿ç”¨æç¤ºæ¨¡æ¿æ ¼å¼åŒ–æŸ¥è¯¢ï¼Œå¹¶è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„å¼ é‡æ ¼å¼
inputs = tokenizer([prompt_style.format(query1, "")], return_tensors="pt").to("cuda")

# ç”Ÿæˆå“åº”
outputs = model.generate(
    input_ids=inputs.input_ids, # è¾“å…¥çš„æ ‡è®°ID
    attention_mask=inputs.attention_mask, # æ³¨æ„åŠ›æ©ç ï¼Œç”¨äºå¤„ç†å¡«å……
    max_new_tokens=1200, # æœ€å¤§ç”Ÿæˆçš„æ–°æ ‡è®°æ•°
    use_cache=True,  # ä½¿ç”¨ç¼“å­˜ä»¥æé«˜ç”Ÿæˆé€Ÿåº¦
)
# è§£ç æ¨¡å‹è¾“å‡º
# ä½¿ç”¨åˆ†è¯å™¨å°†è¾“å‡ºè½¬æ¢å›æ–‡æœ¬ï¼Œå¹¶æå–Responseéƒ¨åˆ†
response = tokenizer.batch_decode(outputs)
print(response[0].split("### Response:")[1])

#å¾®è°ƒåï¼šè§£é‡Šæ›´åŠ å…·ä½“å’Œå‡†ç¡® èƒ½æ›´å¥½åœ°æ•æ‰æŸ¥è¯¢çš„å®Œæ•´ä¸Šä¸‹æ–‡ ä¾‹å¦‚åœ¨ç¤ºä¾‹ä¸­ï¼Œå¾®è°ƒåçš„æ¨¡å‹ç‰¹åˆ«æŒ‡å‡ºäº†"top 10 customers with the highest total spent"è¿™ä¸ªå…³é”®ç»†èŠ‚ï¼Œè¿™åœ¨å¾®è°ƒå‰çš„å“åº”ä¸­æ˜¯ç¼ºå¤±çš„

local_path="deepseek_sql_model"

model.save_pretrained(local_path)
tokenizer.save_pretrained(local_path)

# Save merged model
# model.save_pretrained_merged(local_path, tokenizer, save_method="merged_16bit")

model.push_to_hub(local_path)
tokenizer.push_to_hub(local_path)

from google.colab import drive
drive.mount('/content/drive')
drive_path = "/content/drive/MyDrive/deepseek_model"
model.save_pretrained(drive_path)
tokenizer.save_pretrained(drive_path)

```