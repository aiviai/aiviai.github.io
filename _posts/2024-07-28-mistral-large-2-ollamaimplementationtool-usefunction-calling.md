---
layout: single
title: "mistral large 2 & ollamaå®ç°Tool-Use/function calling[openaiå’Œollama]"
sidebar:
  nav: "docs"
date: 2024-07-28 00:00:00 +0800
categories: [AI-Agents, Fine-Tuning, LLM]
tags: [AI, AI-Agents, AutoGen, LLM, Mistral, Ollama]
classes: wide
author_profile: true
---


#  mistral large 2 & ollamaå®ç°Tool-Use/function calling[openaiå’Œollama] 

###  colabé“¾æ¥ 

[ Google Colab  ![Image](https://ssl.gstatic.com/colaboratory-static/common/9a91b8ba1025a6ed5781f84f290a8cb5/img/favicon.ico) https://colab.research.google.com/drive/1EuFDiWuB6FXjhkf6zk1LrnHTLLSfgeo-?usp=sharing  ![Image](https://colab.research.google.com/img/colab_favicon_256px.png) ](<https://colab.research.google.com/drive/1EuFDiWuB6FXjhkf6zk1LrnHTLLSfgeo-?usp=sharing>)

æµ‹è¯•æ¨ç†é¢˜ç›® 
    
    
```
    åœ¨ä¸€ä¸ªç«‹æ–¹ä½“æˆ¿é—´é‡Œ,å››é¢å¢™å£åˆ†åˆ«æœå‘ä¸œå—è¥¿åŒ—,æ¯é¢å¢™ä¸Šéƒ½æœ‰ä¸€æ‰‡é—¨ã€‚
    ä¸œé—¨çš„é¢œè‰²æ˜¯æ©™è‰²ï¼Œå—é—¨çš„é¢œè‰²æ˜¯ç´«è‰²ï¼Œè¥¿é—¨çš„é¢œè‰²æ˜¯é’è‰²ï¼ŒåŒ—é—¨çš„é¢œè‰²æ˜¯ç™½è‰²ã€‚
    æˆ¿é—´ä¸­å¤®æœ‰ä¸€ä¸ªæ­£æ–¹å½¢è½¬ç›˜,è½¬ç›˜çš„å››ä¸ªè§’æ”¾ç½®ç€4ä¸ªä¸åŒé¢œè‰²çš„ç«‹æ–¹ä½“ç§¯æœ¨:çº¢ã€è“ã€ç»¿ã€é»„ã€‚
    æ­¤æ—¶çº¢è‰²ç«‹æ–¹ä½“å¯¹ç€ä¸œé—¨ï¼Œè“è‰²ç«‹æ–¹ä½“å¯¹ç€å—é—¨ï¼Œç»¿è‰²ç«‹æ–¹ä½“å¯¹ç€è¥¿é—¨ï¼Œé»„è‰²ç«‹æ–¹ä½“å¯¹ç€åŒ—é—¨ã€‚
    ç°åœ¨å¼€å§‹è®¡æ—¶ï¼Œæ­£æ–¹å½¢è½¬ç›˜å¼€å§‹è½¬åŠ¨ã€‚
    æ¯è¿‡äº”åˆ†é’Ÿï¼Œæ­£æ–¹å½¢è½¬ç›˜è½¬åŠ¨ä¸€å‘¨ã€‚
    å½“æ­£æ–¹å½¢è½¬ç›˜è½¬åŠ¨ä¸€å‘¨æ—¶ï¼Œçº¢è‰²ç«‹æ–¹ä½“å’Œç»¿è‰²ç«‹æ–¹ä½“äº¤æ¢ä½ç½®ï¼Œè“è‰²ç«‹æ–¹ä½“å’Œé»„è‰²ç«‹æ–¹ä½“äº¤æ¢ä½ç½®ï¼›
    ä¸œé—¨å’Œå—é—¨çš„é¢œè‰²äº¤æ¢ï¼Œè¥¿é—¨å’ŒåŒ—é—¨çš„é¢œè‰²äº¤æ¢ã€‚
    æ¯è¿‡15åˆ†é’Ÿï¼Œæ—¶é—´å°±ä¼šå€’æµ5åˆ†é’Ÿã€‚
    å°±åœ¨è®¡æ—¶åˆ°äº†ç¬¬ä¸‰åäº”åˆ†é’Ÿæ—¶ï¼Œçº¢ã€è“ã€ç»¿ã€é»„ç«‹æ–¹ä½“åˆ†åˆ«å¯¹åº”å“ªä¸ªæ–¹å‘çš„é—¨ï¼Œå››ä¸ªé—¨åˆ†åˆ«æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ
```
    

ç®—æ³•æµ‹è¯• 

[ https://leetcode.com/problems/number-of-subarrays-with-and-value-of-k/description/ ](<https://leetcode.com/problems/number-of-subarrays-with-and-value-of-k/description/>)

###  openai æ¥å£demo 
    
    
    #pip install openai
    
    import os
    from openai import OpenAI
    
```
    client = OpenAI(
        api_key=os.environ.get("MISTRAL_API_KEY"),
        base_url="https://api.mistral.ai/v1"
    )
```
    
```
    stream = client.chat.completions.create(
        model="mistral-large-latest",
        messages=[
            {
                "role": "user",
                "content": "ä¸‰ä¸ªäººåˆ†æŠ«è¨,æ¯äººåˆ†å¾—ä¸‰åˆ†ä¹‹ä¸€ã€‚å¦‚æœæ¥äº†ä¸‰å€çš„äºº,æ¯äººèƒ½åˆ†åˆ°å¤šå°‘??",
            }
        ],
        stream=True
    )
```
    
```
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="", flush=True)
    print()  # æ·»åŠ ä¸€ä¸ªæ¢è¡Œ
```

##  **ğŸ‘‰ğŸ‘‰ğŸ‘‰å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng**

##  **ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡**

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰æˆ‘çš„å¼€æºé¡¹ç›®** **[ https://github.com/win4r/AISuperDomain ](<https://github.com/win4r/AISuperDomain>) **

###  mistral æ¥å£demo 
    
    
```
    import os
    from mistralai.client import MistralClient
    from mistralai.models.chat_completion import ChatMessage
```
    
    api_key = os.environ["MISTRAL_API_KEY"]
    model = "mistral-large-latest"
    
    client = MistralClient(api_key=api_key)
    
```
    chat_response = client.chat(
        model=model,
        messages=[ChatMessage(role="user", content="ä¸‰ä¸ªäººåˆ†æŠ«è¨,æ¯äººåˆ†å¾—ä¸‰åˆ†ä¹‹ä¸€ã€‚å¦‚æœæ¥äº†ä¸‰å€çš„äºº,æ¯äººèƒ½åˆ†åˆ°å¤šå°‘?")]
    )
```
    
    print(chat_response.choices[0].message.content)

###  ollamaè¿è¡Œmistral-large 
    
    
    curl -fsSL https://ollama.com/install.sh | sh
    
    ollama run mistral-large

###  openai function callingå®ç°è®¡ç®—å™¨ 
    
    
    import os
    from openai import OpenAI
    
    # è®¾ç½®æ‚¨çš„OpenAI APIå¯†é’¥
    os.environ["OPENAI_API_KEY"] = "ollama"
    
```
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œè®¾ç½®è‡ªå®šä¹‰base URL
    client = OpenAI(
        api_key=os.environ.get("OPENAI_API_KEY"),
        base_url="http://localhost:11434/v1"  # æ›¿æ¢ä¸ºæ‚¨çš„è‡ªå®šä¹‰ç«¯ç‚¹
    )
```
    
    
```
    # å®šä¹‰è®¡ç®—å‡½æ•°
    def add(a, b):
        return a + b
```
    
    
    def subtract(a, b):
        return a - b
    
    
    def multiply(a, b):
        return a * b
    
    
```
    def divide(a, b):
        if b == 0:
            return "é”™è¯¯ï¼šé™¤æ•°ä¸èƒ½ä¸ºé›¶"
        return a / b
```
    
    
```
    # å®šä¹‰å¯ç”¨çš„å‡½æ•°
    available_functions = {
        "add": add,
        "subtract": subtract,
        "multiply": multiply,
        "divide": divide,
    }
```
    
```
    # å®šä¹‰å‡½æ•°æè¿°
    tools = [
        {
            "type": "function",
            "function": {
                "name": "add",
                "description": "åŠ æ³•è¿ç®—",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "a": {"type": "number"},
                        "b": {"type": "number"}
                    },
                    "required": ["a", "b"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "subtract",
                "description": "å‡æ³•è¿ç®—",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "a": {"type": "number"},
                        "b": {"type": "number"}
                    },
                    "required": ["a", "b"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "multiply",
                "description": "ä¹˜æ³•è¿ç®—",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "a": {"type": "number"},
                        "b": {"type": "number"}
                    },
                    "required": ["a", "b"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "divide",
                "description": "é™¤æ³•è¿ç®—",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "a": {"type": "number"},
                        "b": {"type": "number"}
                    },
                    "required": ["a", "b"]
                }
            }
        }
    ]
```
    
    
    def run_conversation(user_input):
        messages = [{"role": "user", "content": user_input}]
    
```
        while True:
            try:
                response = client.chat.completions.create(
                    model="mistral-nemo",
                    messages=messages,
                    tools=tools,
                    tool_choice="auto",
                )
```
    
                response_message = response.choices[0].message
    
```
                if response_message.tool_calls:
                    for tool_call in response_message.tool_calls:
                        function_name = tool_call.function.name
                        function_args = eval(tool_call.function.arguments)
```
    
                        function_response = available_functions[function_name](**function_args)
    
```
                        messages.append(response_message)
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "name": function_name,
                            "content": str(function_response),
                        })
                else:
                    return response_message.content
            except Exception as e:
                return f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
```
    
    
```
    # ä¸»ç¨‹åº
    if __name__ == "__main__":
        print("æ¬¢è¿ä½¿ç”¨ç®€å•è®¡ç®—å™¨ï¼")
        print("æ‚¨å¯ä»¥è¿›è¡ŒåŠ æ³•ã€å‡æ³•ã€ä¹˜æ³•å’Œé™¤æ³•è¿ç®—ã€‚")
        print("è¯·ç”¨è‡ªç„¶è¯­è¨€è¾“å…¥æ‚¨çš„è®¡ç®—éœ€æ±‚ï¼Œä¾‹å¦‚ï¼š'è¯·å¸®æˆ‘è®¡ç®—23åŠ 17'")
        print("è¾“å…¥'é€€å‡º'æ¥ç»“æŸç¨‹åºã€‚")
```
    
```
        while True:
            user_input = input("\nè¯·è¾“å…¥æ‚¨çš„è®¡ç®—éœ€æ±‚ï¼š")
            if user_input.lower() == 'é€€å‡º':
                print("è°¢è°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
```
    
            result = run_conversation(user_input)
            print(result)

###  ollama function callingå®ç°èˆªç­ä¿¡æ¯æŸ¥è¯¢ 
    
    
```
    import json
    import ollama
    import asyncio
```
    
    
```
    # Simulates an API call to get flight times
    # In a real application, this would fetch data from a live database or API
    def get_flight_times(departure: str, arrival: str) -> str:
      flights = {
        'NYC-LAX': {'departure': '08:00 AM', 'arrival': '11:30 AM', 'duration': '5h 30m'},
        'LAX-NYC': {'departure': '02:00 PM', 'arrival': '10:30 PM', 'duration': '5h 30m'},
        'LHR-JFK': {'departure': '10:00 AM', 'arrival': '01:00 PM', 'duration': '8h 00m'},
        'JFK-LHR': {'departure': '09:00 PM', 'arrival': '09:00 AM', 'duration': '7h 00m'},
        'CDG-DXB': {'departure': '11:00 AM', 'arrival': '08:00 PM', 'duration': '6h 00m'},
        'DXB-CDG': {'departure': '03:00 AM', 'arrival': '07:30 AM', 'duration': '7h 30m'},
      }
```
    
      key = f'{departure}-{arrival}'.upper()
      return json.dumps(flights.get(key, {'error': 'Flight not found'}))
    
    
```
    async def run(model: str):
      client = ollama.AsyncClient()
      # Initialize conversation with a user query
      messages = [{'role': 'user', 'content': 'What is the flight time from New York (NYC) to Los Angeles (LAX)?'}]
```
    
```
      # First API call: Send the query and function description to the model
      response = await client.chat(
        model=model,
        messages=messages,
        tools=[
          {
            'type': 'function',
            'function': {
              'name': 'get_flight_times',
              'description': 'Get the flight times between two cities',
              'parameters': {
                'type': 'object',
                'properties': {
                  'departure': {
                    'type': 'string',
                    'description': 'The departure city (airport code)',
                  },
                  'arrival': {
                    'type': 'string',
                    'description': 'The arrival city (airport code)',
                  },
                },
                'required': ['departure', 'arrival'],
              },
            },
          },
        ],
      )
```
    
      # Add the model's response to the conversation history
      messages.append(response['message'])
    
```
      # Check if the model decided to use the provided function
      if not response['message'].get('tool_calls'):
        print("The model didn't use the function. Its response was:")
        print(response['message']['content'])
        return
```
    
```
      # Process function calls made by the model
      if response['message'].get('tool_calls'):
        available_functions = {
          'get_flight_times': get_flight_times,
        }
        for tool in response['message']['tool_calls']:
          function_to_call = available_functions[tool['function']['name']]
          function_response = function_to_call(tool['function']['arguments']['departure'], tool['function']['arguments']['arrival'])
          # Add function response to the conversation
          messages.append(
            {
              'role': 'tool',
              'content': function_response,
            }
          )
```
    
```
      # Second API call: Get final response from the model
      final_response = await client.chat(model=model, messages=messages)
      print(final_response['message']['content'])
```
    
    
    # Run the async function
    asyncio.run(run('mistral-nemo'))

###  Parallel function callingå®ç°ç”µå•† **å®¢æˆ·æœåŠ¡ç³»ç»Ÿ**
    
    
    #pip install mistralai pandas
    
```
    import os
    import pandas as pd
    import json
    import time
    from functools import partial
    from google.colab import userdata
    from mistralai.client import MistralClient
    from mistralai.models.chat_completion import ChatMessage
    from mistralai.exceptions import MistralAPIStatusException, MistralAPIException
```
    
    MISTRAL_API_KEY =os.environ.get("MISTRAL_API_KEY")
    
```
    # æ¨¡æ‹Ÿè®¢å•æ•°æ®åº“
    order_data = {
        'è®¢å•å·': ['O1001', 'O1002', 'O1003', 'O1004', 'O1005'],
        'å®¢æˆ·ID': ['C001', 'C002', 'C003', 'C002', 'C001'],
        'æ€»é‡‘é¢': [1255.50, 899.99, 1200.00, 543.30, 2102.20],
        'ä¸‹å•æ—¥æœŸ': ['2024-07-20', '2024-07-21', '2024-07-22', '2024-07-23', '2024-07-24'],
        'çŠ¶æ€': ['å·²é€è¾¾', 'å¤„ç†ä¸­', 'å·²å‘è´§', 'å·²å–æ¶ˆ', 'å¤„ç†ä¸­'],
        'é¢„è®¡é€è¾¾æ—¥æœŸ': ['2024-07-25', '2024-07-28', '2024-07-26', None, '2024-07-29']
    }
    df = pd.DataFrame(order_data)
```
    
```
    def get_order_status(order_id: str) -> str:
        """è·å–è®¢å•çŠ¶æ€"""
        if order_id in df.è®¢å•å·.values:
            return json.dumps({'çŠ¶æ€': df[df.è®¢å•å· == order_id].çŠ¶æ€.item()})
        return json.dumps({'é”™è¯¯': 'æœªæ‰¾åˆ°è¯¥è®¢å•å·ã€‚'})
```
    
```
    def get_estimated_delivery(order_id: str) -> str:
        """è·å–é¢„è®¡é€è¾¾æ—¥æœŸ"""
        if order_id in df.è®¢å•å·.values:
            delivery_date = df[df.è®¢å•å· == order_id].é¢„è®¡é€è¾¾æ—¥æœŸ.item()
            if pd.isna(delivery_date):
                return json.dumps({'é”™è¯¯': 'æš‚æ— é¢„è®¡é€è¾¾æ—¥æœŸä¿¡æ¯ã€‚'})
            return json.dumps({'é¢„è®¡é€è¾¾æ—¥æœŸ': delivery_date})
        return json.dumps({'é”™è¯¯': 'æœªæ‰¾åˆ°è¯¥è®¢å•å·ã€‚'})
```
    
```
    # å¯ç”¨å‡½æ•°å­—å…¸
    available_functions = {
        "get_order_status": get_order_status,
        "get_estimated_delivery": get_estimated_delivery,
    }
```
    
```
    # å·¥å…·å®šä¹‰
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_order_status",
                "description": "è·å–è®¢å•çš„å½“å‰çŠ¶æ€",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "order_id": {
                            "type": "string",
                            "description": "è®¢å•å·",
                        }
                    },
                    "required": ["order_id"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_estimated_delivery",
                "description": "è·å–è®¢å•çš„é¢„è®¡é€è¾¾æ—¥æœŸ",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "order_id": {
                            "type": "string",
                            "description": "è®¢å•å·",
                        }
                    },
                    "required": ["order_id"],
                },
            },
        }
    ]
```
    
```
    def run_conversation(query: str):
        """è¿è¡Œå¯¹è¯ï¼Œå¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶è¿”å›ç»“æœ"""
        messages = [ChatMessage(role="user", content=query)]
```
        
```
        # æ­¥éª¤ 1: å‘é€å¯¹è¯å’Œå¯ç”¨å‡½æ•°åˆ°æ¨¡å‹
        response = client.chat(
            model="mistral-large-latest",
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )
```
        
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls
        
```
        # æ­¥éª¤ 2: æ£€æŸ¥æ¨¡å‹æ˜¯å¦æƒ³è¦è°ƒç”¨å‡½æ•° æ‰§è¡Œå¤šä¸ªå‡½æ•°è°ƒç”¨
        if tool_calls:
            messages.append(response_message)  # å°†åŠ©æ‰‹çš„å›å¤æ·»åŠ åˆ°å¯¹è¯ä¸­
```
            
```
            # æ­¥éª¤ 3: æ‰§è¡Œæ‰€æœ‰å‡½æ•°è°ƒç”¨
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                function_to_call = available_functions[function_name]
                function_args = json.loads(tool_call.function.arguments)
                function_response = function_to_call(**function_args)
                messages.append(ChatMessage(
                    role="tool",
                    content=function_response,
                    name=function_name,
                    tool_call_id=tool_call.id
                ))
```
            
```
            # æ­¥éª¤ 4: å‘é€å‡½æ•°è°ƒç”¨ä¿¡æ¯å’Œå‡½æ•°å“åº”ç»™æ¨¡å‹
            second_response = client.chat(
                model="mistral-large-latest",
                messages=messages,
            )
            return second_response.choices[0].message.content
```
        
        return response_message.content
    
```
    # ç¤ºä¾‹ä½¿ç”¨
    if __name__ == "__main__":
        print(run_conversation("è®¢å•O1002å’ŒO1003çš„çŠ¶æ€å¦‚ä½•ï¼Ÿå®ƒä»¬é¢„è®¡ä»€ä¹ˆæ—¶å€™é€è¾¾ï¼Ÿ"))
```
    

###  atuogenè°ƒç”¨mistral api 
    
    
    pip install pyautogen[mistral]
    pip install dask[dataframe]
    
    
    import os
    
```
    config_list = [
        {
            # é€‰æ‹© mistral-large-latest æ¨¡å‹
            "model": "mistral-large-latest",
            # åœ¨è¿™é‡Œæä¾›æ‚¨çš„ Mistral AI API å¯†é’¥ï¼Œæˆ–å°†å…¶æ”¾å…¥ MISTRAL_API_KEY ç¯å¢ƒå˜é‡ä¸­
            "api_key": os.environ.get("MISTRAL_API_KEY"),
            # å°† API ç±»å‹æŒ‡å®šä¸º 'mistral'ï¼Œä»¥ä½¿ç”¨ Mistral AI å®¢æˆ·ç«¯ç±»
            "api_type": "mistral",
        }
    ]
```
    
    
    from pathlib import Path
    
    from autogen import AssistantAgent, UserProxyAgent
    from autogen.coding import LocalCommandLineCodeExecutor
    
```
    # è®¾ç½®ä»£ç æ‰§è¡Œå™¨
    workdir = Path("coding")
    workdir.mkdir(exist_ok=True)
    code_executor = LocalCommandLineCodeExecutor(work_dir=workdir)
```
    
    # è®¾ç½®ä»£ç†
    
```
    # UserProxyAgent å°†æ‰§è¡Œ AssistantAgent æä¾›çš„ä»£ç 
    user_proxy_agent = UserProxyAgent(
        name="ç”¨æˆ·",
        code_execution_config={"executor": code_executor},
        is_termination_msg=lambda msg: "å®Œæˆ" in msg.get("content"),
    )
```
    
```
    system_message = """ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹ï¼Œå¯ä»¥ç¼–å†™ä»£ç ï¼Œè€Œç”¨æˆ·åˆ™æ‰§è¡Œè¿™äº›ä»£ç ã€‚
    ä½¿ç”¨ä½ çš„ç¼–ç å’Œè¯­è¨€æŠ€èƒ½æ¥è§£å†³ä»»åŠ¡ã€‚
    åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ï¼Œä¸ºç”¨æˆ·æä¾› Python ä»£ç ï¼ˆåœ¨ Python ä»£ç å—ä¸­ï¼‰ä»¥ä¾›æ‰§è¡Œã€‚
    å¦‚æœéœ€è¦ï¼Œè¯·é€æ­¥è§£å†³ä»»åŠ¡ã€‚å¦‚æœæ²¡æœ‰æä¾›è®¡åˆ’ï¼Œè¯·å…ˆè§£é‡Šä½ çš„è®¡åˆ’ã€‚æ¸…æ¥šåœ°è¯´æ˜å“ªä¸€æ­¥ä½¿ç”¨ä»£ç ï¼Œå“ªä¸€æ­¥ä½¿ç”¨ä½ çš„è¯­è¨€æŠ€èƒ½ã€‚
    ä½¿ç”¨ä»£ç æ—¶ï¼Œä½ å¿…é¡»åœ¨ä»£ç å—ä¸­æŒ‡æ˜è„šæœ¬ç±»å‹ã€‚ç”¨æˆ·é™¤äº†æ‰§è¡Œä½ å»ºè®®çš„ä»£ç å¤–ï¼Œä¸èƒ½æä¾›ä»»ä½•å…¶ä»–åé¦ˆæˆ–æ‰§è¡Œä»»ä½•å…¶ä»–æ“ä½œã€‚ç”¨æˆ·ä¸èƒ½ä¿®æ”¹ä½ çš„ä»£ç ã€‚å› æ­¤ï¼Œä¸è¦æä¾›éœ€è¦ç”¨æˆ·ä¿®æ”¹çš„ä¸å®Œæ•´ä»£ç ã€‚å¦‚æœä»£ç å—ä¸æ˜¯ç”¨äºç”¨æˆ·æ‰§è¡Œï¼Œåˆ™ä¸è¦ä½¿ç”¨ä»£ç å—ã€‚
    ä¸è¦åœ¨ä¸€ä¸ªå›å¤ä¸­åŒ…å«å¤šä¸ªä»£ç å—ã€‚ä¸è¦è¦æ±‚ç”¨æˆ·å¤åˆ¶ç²˜è´´ç»“æœã€‚ç›¸åï¼Œåœ¨ç›¸å…³æ—¶ä½¿ç”¨ 'print' å‡½æ•°è¾“å‡ºç»“æœã€‚æ£€æŸ¥ç”¨æˆ·è¿”å›çš„æ‰§è¡Œç»“æœã€‚
    å¦‚æœç»“æœè¡¨æ˜æœ‰é”™è¯¯ï¼Œè¯·ä¿®å¤é”™è¯¯å¹¶å†æ¬¡è¾“å‡ºä»£ç ã€‚å»ºè®®å®Œæ•´çš„ä»£ç ï¼Œè€Œä¸æ˜¯éƒ¨åˆ†ä»£ç æˆ–ä»£ç æ›´æ”¹ã€‚å¦‚æœé”™è¯¯æ— æ³•ä¿®å¤ï¼Œæˆ–è€…å³ä½¿ä»£ç æˆåŠŸæ‰§è¡Œåä»»åŠ¡ä»æœªè§£å†³ï¼Œè¯·åˆ†æé—®é¢˜ï¼Œé‡æ–°å®¡è§†ä½ çš„å‡è®¾ï¼Œæ”¶é›†ä½ éœ€è¦çš„é¢å¤–ä¿¡æ¯ï¼Œå¹¶è€ƒè™‘å°è¯•ä¸åŒçš„æ–¹æ³•ã€‚
    å½“ä½ æ‰¾åˆ°ç­”æ¡ˆæ—¶ï¼Œè¯·ä»”ç»†éªŒè¯ç­”æ¡ˆã€‚å¦‚æœå¯èƒ½ï¼Œåœ¨ä½ çš„å›å¤ä¸­åŒ…å«å¯éªŒè¯çš„è¯æ®ã€‚
    é‡è¦ï¼šç­‰å¾…ç”¨æˆ·æ‰§è¡Œä½ çš„ä»£ç ï¼Œç„¶åä½ å¯ä»¥å›å¤"å®Œæˆ"ä¸€è¯ã€‚ä¸è¦åœ¨ä»£ç å—åè¾“å‡º"å®Œæˆ"ã€‚"""
```
    
```
    # AssistantAgent ä½¿ç”¨ Mistral AI çš„æ¨¡å‹ï¼Œæ¥æ”¶ç¼–ç è¯·æ±‚å¹¶è¿”å›ä»£ç 
    assistant_agent = AssistantAgent(
        name="Mistral åŠ©æ‰‹",
        system_message=system_message,
        llm_config={"config_list": config_list},
    )
```
    
    
```
    # å¼€å§‹å¯¹è¯ï¼ŒUserProxyAgent å‘ AssistantAgent å‘é€æ¶ˆæ¯
    chat_result = user_proxy_agent.initiate_chat(
        assistant_agent,
        message="æä¾›ä»£ç æ¥è®¡ç®— 1 åˆ° 10000 ä¹‹é—´çš„è´¨æ•°æ•°é‡ã€‚",
    )
```
