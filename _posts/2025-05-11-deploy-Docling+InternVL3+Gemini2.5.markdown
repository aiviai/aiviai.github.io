---
layout: single
title: "ğŸš€æ”¯æŒè§†è§‰å¤§æ¨¡å‹çš„å¼€æºPDFè§£æ+OCRå·¥å…·ï¼Doclingæœ¬åœ°é…ç½®ä»å…¥é—¨åˆ°ç²¾é€šä¿å§†çº§æ•™ç¨‹ï¼æ”¯æŒLM Studio+InternVL3-9Bä¸Gemini2.5 Proè½»æ¾è¯†åˆ«è§£ææ¨¡ç³ŠPDFæ‰«ææ–‡ä»¶"
sidebar:
  nav: "docs"
date: 2025-05-1 00:00:00 +0800
categories: LLMs
tags: [Docling, OCR, PDFè§£æ , LM Studio, InternVL3-9B, Gemini2.5 Pro, AI, Gemini]
classes: wide
author_profile: true
---

åœ¨AIæµªæ½®å¸­å·å…¨çƒçš„ä»Šå¤©ï¼Œä¼ä¸šå’Œä¸ªäººéƒ½åœ¨è¿½é—®ï¼šå¦‚ä½•è®©æ‰‹å¤´æµ·é‡çš„PDFã€Wordã€Excelã€ç½‘é¡µå’Œå›¾ç‰‡æ–‡æ¡£çœŸæ­£å˜æˆAIå¯ä»¥ç†è§£å’Œåˆ©ç”¨çš„çŸ¥è¯†ï¼Ÿç­”æ¡ˆæ­£æ‚„ç„¶è¯ç”Ÿ--Doclingï¼Œè¿™æ¬¾ç”±IBM Researchå›¢é˜Ÿä¸»å¯¼ã€å¼€æºç¤¾åŒºçƒ­æ§çš„æ–‡æ¡£å¤„ç†ç¥å™¨ï¼Œæ­£ä»¥æƒŠäººçš„é€Ÿåº¦é‡å¡‘â€œæ–‡æ¡£åˆ°AIâ€çš„æ•°æ®é€šé“ã€‚

### ğŸš€æœ¬ç¯‡ç¬”è®°æ‰€å¯¹åº”çš„è§†é¢‘ï¼š

- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡å“”å“©å“”å“©è§‚çœ‹](https://www.bilibili.com/video/BV1Vy55zSEpd/)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡YouTubeè§‚çœ‹](https://youtu.be/q_IdxUGZsow)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¼€æºé¡¹ç›®](https://github.com/win4r/AISuperDomain)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ è¯·æˆ‘å–å’–å•¡](https://ko-fi.com/aila)
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¾®ä¿¡ï¼šstoeng
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æ‰¿æ¥å¤§æ¨¡å‹å¾®è°ƒã€RAGã€AIæ™ºèƒ½ä½“ã€AIç›¸å…³åº”ç”¨å¼€å‘ç­‰é¡¹ç›®ã€‚

### ğŸ”¥OCRèƒ½åŠ›æµ‹è¯„è§†é¢‘

1. [InternVL#](https://youtu.be/_EqUR0dYGtE) 
2. [Gemini2.5 Pro](https://youtu.be/nb87POhO6aA)  


### ğŸ”¥AIæ™ºèƒ½ä½“ç›¸å…³è§†é¢‘

1. [AIæ™ºèƒ½ä½“è§†é¢‘ 1](https://youtu.be/vYm0brFoMwA) 
2. [AIæ™ºèƒ½ä½“è§†é¢‘ 2](https://youtu.be/szTXELuaJos)  
3. [AIæ™ºèƒ½ä½“è§†é¢‘ 3](https://youtu.be/szTXELuaJos)  
4. [AIæ™ºèƒ½ä½“è§†é¢‘ 4](https://youtu.be/RxR3x_Uyq4c)  
5. [AIæ™ºèƒ½ä½“è§†é¢‘ 5](https://youtu.be/IrTEDPnEVvU)  



**Doclingåˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ**

Doclingæ˜¯ä¸€æ¬¾å¼€æºçš„æ–‡æ¡£è§£æä¸è½¬æ¢å·¥å…·ï¼Œå®ƒèƒ½å°†å„ç§å¤æ‚çš„æ–‡æ¡£æ ¼å¼ï¼ˆå¦‚PDFã€DOCXã€XLSXã€HTMLã€å›¾ç‰‡ç­‰ï¼‰ä¸€é”®è§£æï¼Œè‡ªåŠ¨è½¬åŒ–ä¸ºç»“æ„åŒ–çš„JSONã€Markdownæˆ–HTMLæ ¼å¼ã€‚è¿™äº›æ ¼å¼å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç”Ÿæˆå¼AIæ¥è¯´ï¼Œç®€ç›´æ˜¯â€œç¾å‘³ä½³è‚´â€--å®ƒä»¬èƒ½ç›´æ¥ç”¨æ¥è®­ç»ƒã€å¾®è°ƒAIï¼Œæˆ–ä½œä¸ºçŸ¥è¯†åº“æ”¯æ’‘æ™ºèƒ½é—®ç­”ã€å†…å®¹ç”Ÿæˆç­‰å‰æ²¿åº”ç”¨ã€‚

**ä¸ºä»€ä¹ˆDoclingå¦‚æ­¤ç‰¹åˆ«ï¼Ÿ**

- **æè‡´çš„æ ¼å¼å…¼å®¹åŠ›**ï¼šæ— è®ºæ˜¯å¤šæ æ’ç‰ˆçš„å¹´åº¦æŠ¥å‘Šã€å¸¦æœ‰åµŒå…¥å›¾ç‰‡å’Œè¡¨æ ¼çš„æŠ€æœ¯æ‰‹å†Œï¼Œè¿˜æ˜¯æ‰«æç‰ˆçš„å‘ç¥¨å’ŒåˆåŒï¼ŒDoclingéƒ½èƒ½ç²¾å‡†è¯†åˆ«æ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼ã€ä»£ç å—ã€æ•°å­¦å…¬å¼ç­‰å…ƒç´ ï¼Œç”šè‡³è¿˜èƒ½ç†è§£é¡µé¢å¸ƒå±€å’Œé˜…è¯»é¡ºåºã€‚
- **è¶…å¼ºçš„PDFè§£æèƒ½åŠ›**ï¼šPDFä¸€ç›´æ˜¯AIç•Œçš„â€œç¡¬éª¨å¤´â€ï¼Œå› ä¸ºå…¶å†…å®¹ç±»å‹æ··æ‚ä¸”ç»“æ„å¤æ‚ã€‚Doclingä¸ä»…èƒ½æŠŠå¤šé¡µè¡¨æ ¼è¿˜åŸæˆä¸€ä¸ªæ•´ä½“ï¼Œè¿˜èƒ½è¯†åˆ«å…¬å¼ã€ä»£ç å’Œå›¾ç‰‡ï¼Œæœ€å¤§ç¨‹åº¦ä¿ç•™åŸå§‹è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ã€‚
- **ç»Ÿä¸€çš„æ–‡æ¡£è¡¨è¾¾æ ¼å¼**ï¼šDoclingåˆ›æ–°æ€§åœ°æå‡ºäº†DoclingDocumentæ ¼å¼ï¼Œæ— è®ºåŸå§‹æ–‡æ¡£æ¥è‡ªä½•ç§æ ¼å¼ï¼Œæœ€ç»ˆéƒ½èƒ½è½¬æ¢æˆæ ‡å‡†åŒ–çš„ç»“æ„å¯¹è±¡ï¼Œæå¤§ç®€åŒ–äº†åç»­AIå¤„ç†æµç¨‹ã€‚
- **çµæ´»çš„å¯¼å‡ºä¸æœ¬åœ°æ‰§è¡Œ**ï¼šç”¨æˆ·å¯æ ¹æ®éœ€æ±‚é€‰æ‹©å¯¼å‡ºä¸ºMarkdownã€HTMLæˆ–æ— æŸJSONæ ¼å¼ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒDoclingæ”¯æŒæœ¬åœ°ç¦»çº¿è¿è¡Œï¼Œæ•°æ®éšç§å’Œå®‰å…¨æ€§æ— å¿§ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†æ•æ„Ÿæˆ–å†…ç½‘ç¯å¢ƒä¸‹çš„ä¼ä¸šæ•°æ®ã€‚
- **ä¸ä¸»æµAIæ¡†æ¶æ— ç¼é›†æˆ**ï¼šDoclingå·²æ·±åº¦é›†æˆLangChainã€LlamaIndexã€Crew AIã€Haystackç­‰çƒ­é—¨ç”Ÿæˆå¼AIç”Ÿæ€ï¼Œå¼€å‘è€…åªéœ€å‡ è¡Œä»£ç ï¼Œå³å¯å°†æ–‡æ¡£çŸ¥è¯†æ³¨å…¥AIæ™ºèƒ½ä½“ï¼Œå®ç°è‡ªåŠ¨é—®ç­”ã€å†…å®¹ç”Ÿæˆã€çŸ¥è¯†æ£€ç´¢ç­‰åˆ›æ–°åœºæ™¯ã€‚

**Doclingæ­£åœ¨å¦‚ä½•æ”¹å˜è¡Œä¸šï¼Ÿ**

åœ¨çŸ¥è¯†ç®¡ç†ã€ä¼ä¸šæ™ºèƒ½ã€æ³•å¾‹åˆè§„ã€æŠ€æœ¯æ–‡æ¡£åˆ†æç­‰é¢†åŸŸï¼ŒDoclingå·²æˆä¸ºä¸å¯æˆ–ç¼ºçš„â€œçŸ¥è¯†ç®¡é“â€ã€‚ä¾‹å¦‚ï¼ŒRed Hatå’ŒIBMå°†Doclingé›†æˆåˆ°RHEL AIå’ŒInstructLabå¹³å°ï¼Œå¸®åŠ©ä¼ä¸šç”¨è‡ªæœ‰æ–‡æ¡£å¾®è°ƒå¤§æ¨¡å‹ï¼Œè®©AIçœŸæ­£â€œæ‡‚ä¸šåŠ¡ã€æ‡‚è¡Œä¸šâ€ã€‚å¼€æºç¤¾åŒºæ›´æ˜¯çƒ­æƒ…é«˜æ¶¨ï¼ŒDoclingåœ¨GitHubä¸Šæ˜Ÿæ ‡æ•°é£™å‡ï¼Œæˆä¸ºAIå¼€å‘è€…çš„æ–°å® ã€‚

**æœªæ¥å±•æœ›ï¼šAIä¸æ–‡æ¡£çš„æ— é™å¯èƒ½**

Doclingçš„ç ”å‘å›¢é˜Ÿæ­£æŒç»­æ‰©å±•å…¶èƒ½åŠ›ï¼Œæœªæ¥å°†æ”¯æŒæ›´å¤æ‚çš„æ•°æ®ç±»å‹ï¼Œå¦‚å›¾è¡¨ã€åŒ–å­¦ç»“æ„ã€ä¸šåŠ¡è¡¨å•ç­‰ã€‚æƒ³è±¡ä¸€ä¸‹ï¼ŒAIä¸ä»…èƒ½è¯»æ‡‚æŠ€æœ¯æ‰‹å†Œï¼Œè¿˜èƒ½è‡ªåŠ¨è§£æè´¢æŠ¥å›¾è¡¨ã€è¯†åˆ«ä¸“åˆ©åˆ†å­ç»“æ„ï¼Œä¼ä¸šçš„çŸ¥è¯†èµ„äº§å°†è¢«å½»åº•æ¿€æ´»ï¼Œæˆä¸ºAIé©±åŠ¨åˆ›æ–°çš„ç‡ƒæ–™ã€‚

Doclingä¸æ˜¯ä¸‹ä¸€ä¸ªâ€œæ–‡æ¡£OCRå·¥å…·â€ï¼Œè€Œæ˜¯AIæ—¶ä»£çš„â€œçŸ¥è¯†å‘åŠ¨æœºâ€ã€‚å®ƒè®©æ²‰ç¡åœ¨æµ·é‡æ–‡æ¡£ä¸­çš„æ•°æ®ç„•å‘æ–°ç”Ÿï¼ŒåŠ©åŠ›æ¯ä¸€ä¸ªç»„ç»‡å’Œå¼€å‘è€…ï¼Œè½»æ¾è·¨è¶Šâ€œæ–‡æ¡£åˆ°AIâ€çš„é¸¿æ²Ÿã€‚æœªæ¥å·²æ¥ï¼ŒDoclingæ­£å¸¦ä½ é¢†è·‘AIçŸ¥è¯†é©å‘½ï¼

### ğŸš€å®‰è£…

```bash
pip install litellm google-generativeai docling
```

### ğŸš€å‘½ä»¤è¡Œ

```bash
docling https://arxiv.org/pdf/2206.01062

```

### ğŸš€åŸºç¡€ç”¨æ³•

```bash
from docling.document_converter import DocumentConverter

source = "./test/docling.pdf"  # document per local path or URL
output_path = "./output/docling.md"  # ä¿®æ”¹ä¸ºä½ å¸Œæœ›ä¿å­˜çš„è·¯å¾„

converter = DocumentConverter()
result = converter.convert(source)

markdown_text = result.document.export_to_markdown()

# ä¿å­˜åˆ°æœ¬åœ° Markdown æ–‡ä»¶
with open(output_path, "w", encoding="utf-8") as f:
    f.write(markdown_text)

print(f"Markdown å·²ä¿å­˜åˆ°ï¼š{output_path}")
```

### ğŸš€docling+LM Studio

```bash
import logging
import os
from pathlib import Path
import requests
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    ApiVlmOptions,
    ResponseFormat,
    VlmPipelineOptions,
)
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.pipeline.vlm_pipeline import VlmPipeline

def check_lm_studio_connection(url="http://127.0.0.1:1234", timeout=5):
    """æ£€æŸ¥LM Studioæ˜¯å¦æ­£å¸¸è¿è¡Œ"""
    try:
        response = requests.get(f"{url}/v1/models", timeout=timeout)
        if response.status_code == 200:
            models = response.json()
            logging.info("LM Studioè¿æ¥æˆåŠŸ")
            return True, models
        else:
            logging.error(f"LM Studioå“åº”å¼‚å¸¸ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return False, None
    except Exception as e:
        logging.error(f"æ— æ³•è¿æ¥åˆ°LM Studio: {e}")
        return False, None

def lm_studio_vlm_options(model: str, prompt: str, timeout: int = 300):
    """é…ç½®LM Studioçš„VLMé€‰é¡¹"""
    options = ApiVlmOptions(
        url="http://localhost:1234/v1/chat/completions",
        params=dict(
            model=model,
            max_tokens=8192,
            temperature=0.1,
        ),
        prompt=prompt,
        timeout=timeout,
        scale=0.5,  # å¯ä»¥è°ƒæ•´å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹
        response_format=ResponseFormat.MARKDOWN,
    )
    return options

def process_single_pdf(pdf_path: Path, output_dir: Path, model_name: str = "internvl3-9b"):
    """å¤„ç†å•ä¸ªPDFæ–‡ä»¶"""
    logging.info(f"æ­£åœ¨å¤„ç†: {pdf_path.name}")

    # é…ç½®VLMæµæ°´çº¿
    pipeline_options = VlmPipelineOptions(
        enable_remote_services=True
    )

    pipeline_options.vlm_options = lm_studio_vlm_options(
        model=model_name,
        prompt="OCR the full page to markdown.",

        #         prompt="""Please accurately extract all text content from this page, including:
# 1. Text content
# 2. Mathematical formulas (in LaTeX format if possible)
# 3. Figure captions and references
# 4. Table content
# 5. Any other relevant information
#
# Format the output in markdown.""",
        timeout=300
    )

    # åˆ›å»ºæ–‡æ¡£è½¬æ¢å™¨
    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options,
                pipeline_cls=VlmPipeline,
            )
        }
    )

    try:
        # æ‰§è¡Œè½¬æ¢
        result = doc_converter.convert(pdf_path)

        # ä¿å­˜ç»“æœ
        markdown_content = result.document.export_to_markdown()
        output_file = output_dir / f"{pdf_path.stem}_content.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        logging.info(f"è½¬æ¢å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return True, output_file

    except Exception as e:
        logging.error(f"å¤„ç† {pdf_path.name} æ—¶å‡ºé”™: {e}")
        return False, None

def process_pdf_folder(input_folder: str, output_folder: str = "./output", model_name: str = "internvl3-2b"):
    """å¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶"""

    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶å¤¹
    input_path = Path(input_folder)
    if not input_path.exists():
        logging.error(f"è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {input_folder}")
        return

    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_path = Path(output_folder)
    output_path.mkdir(parents=True, exist_ok=True)

    # æ£€æŸ¥LM Studioè¿æ¥
    logging.info("=== æ£€æŸ¥LM Studioè¿æ¥ ===")
    success, models = check_lm_studio_connection()
    if not success:
        logging.error("æ— æ³•è¿æ¥åˆ°LM Studioï¼Œè¯·ç¡®ä¿ï¼š")
        logging.error("1. LM Studioå·²å¯åŠ¨")
        logging.error("2. APIæœåŠ¡å™¨å·²å¯ç”¨")
        logging.error("3. internvl3-2bæ¨¡å‹å·²åŠ è½½")
        return

    # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
    pdf_files = list(input_path.glob("*.pdf"))
    if not pdf_files:
        logging.warning(f"åœ¨ {input_folder} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
        return

    logging.info(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")

    # å¤„ç†æ¯ä¸ªPDFæ–‡ä»¶
    success_count = 0
    failed_files = []

    for i, pdf_file in enumerate(pdf_files, 1):
        logging.info(f"\n=== å¤„ç†ç¬¬ {i}/{len(pdf_files)} ä¸ªæ–‡ä»¶ ===")
        success, output_file = process_single_pdf(pdf_file, output_path, model_name)

        if success:
            success_count += 1
            # æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹é¢„è§ˆ
            with open(output_file, 'r', encoding='utf-8') as f:
                content = f.read()
                print(f"\n--- {pdf_file.name} è½¬æ¢ç»“æœé¢„è§ˆ ---")
                print(content[:200] + "..." if len(content) > 200 else content)
        else:
            failed_files.append(pdf_file.name)

    # è¾“å‡ºå¤„ç†ç»“æœç»Ÿè®¡
    logging.info(f"\n=== å¤„ç†å®Œæˆ ===")
    logging.info(f"æˆåŠŸå¤„ç†: {success_count}/{len(pdf_files)} ä¸ªæ–‡ä»¶")
    if failed_files:
        logging.warning(f"å¤±è´¥æ–‡ä»¶: {', '.join(failed_files)}")

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è¾“å…¥æ–‡ä»¶å¤¹ - ä¿®æ”¹è¿™é‡ŒæŒ‡å®šä½ çš„PDFæ–‡ä»¶æ‰€åœ¨ä½ç½®
    input_folder = "./pdf_files"  # ä¿®æ”¹ä¸ºä½ çš„PDFæ–‡ä»¶å¤¹è·¯å¾„

    # è®¾ç½®è¾“å‡ºæ–‡ä»¶å¤¹
    output_folder = "./output"  # ç»“æœä¿å­˜ä½ç½®

    # è®¾ç½®æ¨¡å‹åç§°
    model_name = "internvl3-9b"  # æˆ–è€…å…¶ä»–ä½ åœ¨LM Studioä¸­ä½¿ç”¨çš„æ¨¡å‹åç§°

    # å¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDF
    process_pdf_folder(input_folder, output_folder, model_name)

if __name__ == "__main__":
    main()
```

### ğŸš€docling+Gemini2.5 Pro

```bash
import logging
import os
from pathlib import Path
import litellm
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    ApiVlmOptions,
    ResponseFormat,
    VlmPipelineOptions,
)
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.pipeline.vlm_pipeline import VlmPipeline
import threading
import queue
import time

# åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„ API æœåŠ¡å™¨æ¨¡æ‹Ÿå™¨
class GeminiAPIServer:
    def __init__(self):
        self.is_running = False
        self.server_thread = None
        self.model_cache = {}

    def start(self):
        """å¯åŠ¨ API æœåŠ¡å™¨"""
        import http.server
        import socketserver
        import json
        from urllib.parse import parse_qs, urlparse

        class CustomHandler(http.server.SimpleHTTPRequestHandler):
            server_obj = self

            def do_POST(self):
                if self.path == '/v1/chat/completions':
                    content_length = int(self.headers['Content-Length'])
                    post_data = self.rfile.read(content_length)

                    try:
                        request_data = json.loads(post_data.decode('utf-8'))
                        response = self.handle_completion(request_data)

                        self.send_response(200)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        self.wfile.write(json.dumps(response).encode())
                    except Exception as e:
                        self.send_response(500)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        error_response = {"error": str(e)}
                        self.wfile.write(json.dumps(error_response).encode())
                else:
                    self.send_response(404)
                    self.end_headers()

            def handle_completion(self, request_data):
                model = request_data.get("model", "gemini-2.5-pro-preview-05-06")
                messages = request_data.get("messages", [])

                # ä½¿ç”¨ liteLLM è¿›è¡Œå®é™…è°ƒç”¨
                try:
                    response = litellm.completion(
                        model=f"gemini/{model}",
                        messages=messages,
                        temperature=request_data.get("temperature", 0.1),
                        max_tokens=request_data.get("max_tokens", 65536)
                    )

                    # ç¡®ä¿å“åº”æ ¼å¼ç¬¦åˆ OpenAI æ ‡å‡†
                    return {
                        "id": response.id,
                        "object": "chat.completion",
                        "created": int(time.time()),
                        "model": model,
                        "choices": [
                            {
                                "index": 0,
                                "message": {
                                    "role": "assistant",
                                    "content": response.choices[0].message.content
                                },
                                "finish_reason": "stop"
                            }
                        ],
                        "usage": response.usage.dict() if response.usage else {}
                    }
                except Exception as e:
                    raise e

            def log_message(self, format, *args):
                pass  # å‡å°‘æ—¥å¿—è¾“å‡º

        def run_server():
            with socketserver.TCPServer(("", 4000), CustomHandler) as httpd:
                httpd.timeout = 1  # è®¾ç½®è¶…æ—¶ï¼Œä¾¿äºä¼˜é›…å…³é—­
                self.httpd = httpd
                while self.is_running:
                    httpd.handle_request()

        self.is_running = True
        self.server_thread = threading.Thread(target=run_server)
        self.server_thread.start()
        time.sleep(2)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨

    def stop(self):
        """åœæ­¢ API æœåŠ¡å™¨"""
        self.is_running = False
        if self.server_thread:
            self.server_thread.join(timeout=5)
        logging.info("API æœåŠ¡å™¨å·²åœæ­¢")

# å…¨å±€ API æœåŠ¡å™¨å®ä¾‹
api_server = GeminiAPIServer()

def gemini_vlm_options(model: str, prompt: str, timeout: int = 300):
    """é…ç½® Gemini çš„ VLM é€‰é¡¹"""
    options = ApiVlmOptions(
        url="http://localhost:4000/v1/chat/completions",
        params=dict(
            model=model,
            max_tokens=65536,
            temperature=1,
        ),
        prompt=prompt,
        timeout=timeout,
        scale=1.0, # å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹
        response_format=ResponseFormat.MARKDOWN,
    )
    return options

def process_single_pdf(pdf_path: Path, output_dir: Path, model_name: str = "gemini-2.5-pro-preview-05-06"):
    """å¤„ç†å•ä¸ªPDFæ–‡ä»¶"""
    logging.info(f"æ­£åœ¨å¤„ç†: {pdf_path.name}")

    # é…ç½®VLMæµæ°´çº¿
    pipeline_options = VlmPipelineOptions(
        enable_remote_services=True
    )

    pipeline_options.vlm_options = gemini_vlm_options(
        model=model_name,
        prompt="OCR the full page to markdown.",
        timeout=300
    )

    # åˆ›å»ºæ–‡æ¡£è½¬æ¢å™¨
    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options,
                pipeline_cls=VlmPipeline,
            )
        }
    )

    try:
        # æ‰§è¡Œè½¬æ¢
        result = doc_converter.convert(pdf_path)

        # ä¿å­˜ç»“æœ
        markdown_content = result.document.export_to_markdown()
        output_file = output_dir / f"{pdf_path.stem}_content.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        logging.info(f"è½¬æ¢å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return True, output_file

    except Exception as e:
        logging.error(f"å¤„ç† {pdf_path.name} æ—¶å‡ºé”™: {e}")
        return False, None

def process_pdf_folder(input_folder: str, output_folder: str = "./output", model_name: str = "gemini-2.5-pro-preview-05-06"):
    """å¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶"""

    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("GEMINI_API_KEY"):
        logging.error("æœªè®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡")
        logging.error("è¯·è®¾ç½®ä½ çš„ Gemini API å¯†é’¥: export GEMINI_API_KEY='your-api-key'")
        return

    # åˆå§‹åŒ– liteLLM
    os.environ["LITELLM_LOG"] = "ERROR"  # å‡å°‘æ—¥å¿—è¾“å‡º

    # å¯åŠ¨æœ¬åœ° API æœåŠ¡å™¨
    logging.info("=== å¯åŠ¨æœ¬åœ° API æœåŠ¡å™¨ ===")
    try:
        api_server.start()
        logging.info("API æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
    except Exception as e:
        logging.error(f"æ— æ³•å¯åŠ¨ API æœåŠ¡å™¨: {e}")
        return

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶å¤¹
    input_path = Path(input_folder)
    if not input_path.exists():
        logging.error(f"è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {input_folder}")
        api_server.stop()
        return

    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_path = Path(output_folder)
    output_path.mkdir(parents=True, exist_ok=True)

    try:
        # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
        pdf_files = list(input_path.glob("*.pdf"))
        if not pdf_files:
            logging.warning(f"åœ¨ {input_folder} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return

        logging.info(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")

        # å¤„ç†æ¯ä¸ªPDFæ–‡ä»¶
        success_count = 0
        failed_files = []

        for i, pdf_file in enumerate(pdf_files, 1):
            logging.info(f"\n=== å¤„ç†ç¬¬ {i}/{len(pdf_files)} ä¸ªæ–‡ä»¶ ===")
            success, output_file = process_single_pdf(pdf_file, output_path, model_name)

            if success:
                success_count += 1
                # æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹é¢„è§ˆ
                with open(output_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    print(f"\n--- {pdf_file.name} è½¬æ¢ç»“æœé¢„è§ˆ ---")
                    print(content[:200] + "..." if len(content) > 200 else content)
            else:
                failed_files.append(pdf_file.name)

        # è¾“å‡ºå¤„ç†ç»“æœç»Ÿè®¡
        logging.info(f"\n=== å¤„ç†å®Œæˆ ===")
        logging.info(f"æˆåŠŸå¤„ç†: {success_count}/{len(pdf_files)} ä¸ªæ–‡ä»¶")
        if failed_files:
            logging.warning(f"å¤±è´¥æ–‡ä»¶: {', '.join(failed_files)}")

    finally:
        # åœæ­¢æœåŠ¡å™¨
        api_server.stop()

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è¾“å…¥æ–‡ä»¶å¤¹
    input_folder = "./pdf_files"  # ä¿®æ”¹ä¸ºä½ çš„PDFæ–‡ä»¶å¤¹è·¯å¾„

    # è®¾ç½®è¾“å‡ºæ–‡ä»¶å¤¹
    output_folder = "./output"  # ç»“æœä¿å­˜ä½ç½®

    # è®¾ç½®æ¨¡å‹åç§°
    model_name = "gemini-2.5-pro-preview-05-06"  # ä½¿ç”¨ Gemini 2.5 Pro Preview

    # å¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDF
    process_pdf_folder(input_folder, output_folder, model_name)

if __name__ == "__main__":
    main()
```