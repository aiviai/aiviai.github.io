---
layout: single
title: "Reflection 70B-AIç•Œçš„æ–°æ˜Ÿæ¥è¢­ï¼šæ·±åº¦è§£æReflection-Llama3.1-70bæ¨¡å‹ï¼Œæ­ç§˜å…¶è¶…å¼ºæ¨ç†èƒ½åŠ›ä¸è‡ªæˆ‘çº é”™æŠ€æœ¯ï¼Œå¸¦ä½ ä½“éªŒAIæ€ç»´çš„é©å‘½æ€§çªç ´"
sidebar:
  nav: "docs"
date: 2024-09-08 00:00:00 +0800
categories: [AI-Agents, Fine-Tuning, LLM, RAG, Tutorial]
tags: [AI, AI-Agents, AutoGen, DeepSeek, HuggingFace, LLM, Llama-3, LlamaIndex, Ollama, RAG]
classes: wide
author_profile: true
---


#  **Reflection 70B-AIç•Œçš„æ–°æ˜Ÿæ¥è¢­ï¼šæ·±åº¦è§£æReflection-Llama3.1-70bæ¨¡å‹ï¼Œæ­ç§˜å…¶è¶…å¼ºæ¨ç†èƒ½åŠ›ä¸è‡ªæˆ‘çº é”™æŠ€æœ¯ï¼Œå¸¦ä½ ä½“éªŒAIæ€ç»´çš„é©å‘½æ€§çªç ´**

###  hugging face 

[ https://huggingface.co/mattshumer/Reflection-Llama-3.1-70B ](<https://huggingface.co/mattshumer/Reflection-Llama-3.1-70B>)

###  ç®—æ³•æµ‹è¯• 

[ https://leetcode.com/problems/text-justification/ ](<https://leetcode.com/problems/text-justification/>)

###  æ¨ç†æµ‹è¯• 
    
    
    Which is bigger -- 9.11 or 9.9?
    
    
    How many Rs are in strawberry?
    
    
```
    å°Aå»å•†åº—ä¹°äº†90å…ƒçš„ä¸œè¥¿,ä½†å‘ç°è‡ªå·±åªå¸¦äº†20å…ƒã€‚
    å•†åº—è€æ¿å€Ÿç»™ä»–80å…ƒ,å°Aç”¨è¿™100å…ƒä»˜æ¬¾å,å•†åº—è€æ¿æ‰¾å›10å…ƒç»™ä»–ã€‚
    å›å®¶åæ‹¿åˆ°é’±åï¼Œå°Aæ¥åˆ°å•†åº—æŠŠ80å…ƒè¿˜ç»™è¶…å¸‚è€æ¿ã€‚
    ä½†è¶…å¸‚è€æ¿æ€»æ„Ÿè§‰ä¸‹Aç»™çš„å°‘äº†ï¼Œè¯·é—®å°Aåº”è¯¥ç»™å¤šå°‘é’±ï¼Ÿ
```
    

###  ollama 
    
    
    curl -fsSL https://ollama.com/install.sh | sh
    
    
    ollama run reflection:70b

##  **ğŸ‘‰ğŸ‘‰ğŸ‘‰å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng**

##  **ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡**

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰æˆ‘çš„å¼€æºé¡¹ç›®** **[ https://github.com/win4r/AISuperDomain ](<https://github.com/win4r/AISuperDomain>) **

###  api 

[ https://openrouter.ai/chat?models=mattshumer/reflection-70b:free ](<https://openrouter.ai/chat?models=mattshumer/reflection-70b:free>)
    
    
```
    curl https://openrouter.ai/api/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $OPENROUTER_API_KEY" \
      -d '{
      "model": "mattshumer/reflection-70b:free",
      "messages": [
        {"role": "user", "content": "What is the meaning of life?"}
      ]
```
      
    }'

###  LM Studio 

[ https://huggingface.co/lmstudio-community/Reflection-Llama-3.1-70B-GGUF ](<https://huggingface.co/lmstudio-community/Reflection-Llama-3.1-70B-GGUF>)
    
    
```
    curl https://api.deepseek.com/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer sk-9d626022379a49a29e4147f593853732" \
      -d '{
            "model": "deepseek-chat",
            "messages": [
              {"role": "system", "content": "You are a helpful assistant."},
              {"role": "user", "content": "Hello!"}
            ],
            "stream": false
          }'
```

###  AutoGen+LlamaIndex 
    
    
    !pip install pyautogen llama-index-vector-stores-chroma llama-index llama-index-embeddings-huggingface llama-index-llms-together
    
```
    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
    from llama_index.vector_stores.chroma import ChromaVectorStore
    from llama_index.core import StorageContext
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.core import Settings
    from llama_index.llms.together import TogetherLLM
```
    
```
    import chromadb
    import autogen
    from autogen import ConversableAgent
```
    
    
    
    # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    !mkdir -p ./documents
    
    # ä¸‹è½½æ–‡ä»¶åˆ° ./documents ç›®å½•
    !wget -P ./documents https://raw.githubusercontent.com/win4r/mytest/main/book.txt
    
    
    
    
```
    def initialize_index():
        # åˆå§‹åŒ– Chroma æ•°æ®åº“å®¢æˆ·ç«¯
        db = chromadb.PersistentClient(path="./chroma_db")
        # è·å–æˆ–åˆ›å»ºä¸€ä¸ªåä¸º "my-docs" çš„é›†åˆ
        chroma_collection = db.get_or_create_collection("my-docs")
        # åˆ›å»º ChromaVectorStore å®ä¾‹
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
```
    
        # ä½¿ç”¨ BAAI/bge-large-en-v1.5 åµŒå…¥æ¨¡å‹
        embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-large-en-v1.5")
    
```
        # è®¾ç½®å…¨å±€åµŒå…¥æ¨¡å‹
        Settings.embed_model = embed_model
        Settings.llm = TogetherLLM( model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo", api_key="sk-" )
```
    
```
        # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨æ•°æ®
        if chroma_collection.count() > 0:
            print("Loading existing index...")
            # å¦‚æœå­˜åœ¨ï¼Œä»å‘é‡å­˜å‚¨åŠ è½½ç´¢å¼•
            return VectorStoreIndex.from_vector_store(
                vector_store, storage_context=storage_context
            )
        else:
            print("Creating new index...")
            # å¦‚æœä¸å­˜åœ¨ï¼Œä»æ–‡æ¡£ç›®å½•åŠ è½½æ•°æ®å¹¶åˆ›å»ºæ–°ç´¢å¼•
            documents = SimpleDirectoryReader("./documents").load_data()
            return VectorStoreIndex.from_documents(
                documents, storage_context=storage_context
            )
```
    
```
    # åˆå§‹åŒ–ç´¢å¼•
    index = initialize_index()
    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine()
```
    
    
    
    def create_prompt(user_input):
        result = query_engine.query(user_input)
    
        prompt = f"""
        Your Task: Provide a concise and informative response to the user's query, drawing on the provided context.
    
        Context: {result}
    
        User Query: {user_input}
    
```
        Guidelines:
        1. Relevance: Focus directly on the user's question.
        2. Conciseness: Avoid unnecessary details.
        3. Accuracy: Ensure factual correctness.
        4. Clarity: Use clear language.
        5. Contextual Awareness: Use general knowledge if context is insufficient.
        6. Honesty: State if you lack information.
```
    
```
        Response Format:
        - Direct answer
        - Brief explanation (if necessary)
        - Citation (if relevant)
        - Conclusion
        """
```
    
        return prompt
    
    
    
    
```
    # é…ç½®LLM(è¯­è¨€æ¨¡å‹)
    llm_config = {
        "config_list": [
            # {
            #     "model": "llama-3.1-8b-instant",
            #     "api_key": os.getenv("GROQ_API_KEY"),
            #     "api_type": "groq",
            # }
            {
              "model": "mattshumer/reflection-70b:free",
              "base_url": "https://openrouter.ai/api/v1",
              "api_key": "sk-or-v1-",
              "cache_seed": 42
            },
        ]
    }
```
    
```
    # åˆ›å»ºRAGæœºå™¨äººä»£ç†
    rag_agent = ConversableAgent(
        name="RAGbot",
        system_message="You are a RAG chatbot",
        llm_config=llm_config,
        code_execution_config=False,
        human_input_mode="NEVER",
    )
```
    
    
    
    
    
    prompt = create_prompt("Show me some samples of Knowledge Integration prompts")
    
    reply = rag_agent.generate_reply(messages=[{"content": prompt, "role": "user"}])
    
    print("\nType of reply:", type(reply))
    print("\nContent of reply:", reply)
    
```
    if isinstance(reply, dict) and 'content' in reply:
        print(f"\nRAGbot: {reply['content']}")
    elif isinstance(reply, str):
        print(f"\nRAGbot: {reply}")
    else:
        print("\nUnexpected reply format")
```
    
    
    
    
    
