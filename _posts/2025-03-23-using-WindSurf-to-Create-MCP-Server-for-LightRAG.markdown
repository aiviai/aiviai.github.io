---
layout: single
title: "üöÄCursorÈôç‰ΩéÊô∫ÂïÜÔºÅWindSurfÈõ∂‰ª£Á†ÅÂºÄÂèëMCP ServerÔºÅ‰∫îÂàÜÈíüËΩªÊùæÂÆûÁé∞LightRAG+MCP‰∏∫ClaudeÂíåAutoGenÊåÇËΩΩÁü•ËØÜÂ∫ìÔºÅÂ¢ûÂº∫ClaudeÂíåAutoGenÁöÑÁü•ËØÜÂ∫ìÊ£ÄÁ¥¢ËÉΩÂäõ"
sidebar:
  nav: "docs"
date: 2025-03-23 00:00:00 +0800
categories: AIAgents
tags: [MCP, MCP Server, Cursor, WindSurf, AIÁºñÁ®ã]
classes: wide
author_profile: true
---

ÈöèÁùÄ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁöÑ‰∏çÊñ≠Á™ÅÁ†¥ÔºåÁºñÁ®ãÂ∑•ÂÖ∑Ê≠£ËøéÊù•ÂâçÊâÄÊú™ÊúâÁöÑÂèòÈù©„ÄÇÁî± Codeium Âõ¢ÈòüÊé®Âá∫ÁöÑ WindSurfÔºå‰ª•ÂÖ®Êñ∞ÁöÑ AI Flow ËåÉÂºèÂíåÂ§öÂ∑•ÂÖ∑ÂçèÂêåËÉΩÂäõÔºåÊ≠£ÈÄêÊ≠•Ë∂ÖË∂äÂ§áÂèóÂÖ≥Ê≥®ÁöÑ CursorÔºåÊàê‰∏∫ÂºÄÂèëËÄÖÊèêÈ´òÂ∑•‰ΩúÊïàÁéáÁöÑÂà©Âô®„ÄÇ

WindSurf ÁöÑÊúÄÂ§ß‰∫ÆÁÇπÂú®‰∫éÂÖ∂Ê∑±Â∫¶‰∏ä‰∏ãÊñáÁêÜËß£ËÉΩÂäõ„ÄÇ‰º†ÁªüÁöÑÁºñÁ®ãÂä©ÊâãÂæÄÂæÄÂè™ËÉΩÂØπÁÆÄÂçï‰ª£Á†ÅÁâáÊÆµËøõË°åË°•ÂÖ®ÔºåËÄå WindSurf ÂàôËÉΩÊô∫ËÉΩÊçïÊçâÈ°πÁõÆÊï¥‰ΩìÁªìÊûÑ„ÄÅÂèòÈáèÂÖ≥Á≥ª‰ª•ÂèäÂáΩÊï∞Ë∞ÉÁî®ÈìæÔºåÊó†ÈúÄÂºÄÂèëËÄÖÂèçÂ§çËæìÂÖ•ÊèêÁ§∫„ÄÇÂÆÉËÉΩÂ§ü‰∏ªÂä®È¢ÑÊµãÈúÄÊ±ÇÔºåÂú®‰ª£Á†ÅÁºñÂÜô„ÄÅÈáçÊûÑ„ÄÅË∞ÉËØïÁ≠âËøáÁ®ã‰∏≠Êèê‰æõÁ≤æÂáÜÂª∫ËÆÆÔºå‰ªéËÄåÂ§ßÂπÖÂáèÂ∞ëÊâãÂä®Ë∞ÉËØïÂíåÂèçÂ§çÁ°ÆËÆ§ÁöÑÊó∂Èó¥„ÄÇ

### üöÄÊú¨ÁØáÁ¨îËÆ∞ÊâÄÂØπÂ∫îÁöÑËßÜÈ¢ëÔºö

- [üëâüëâüëâ ÈÄöËøáÂìîÂì©ÂìîÂì©ËßÇÁúã](https://www.bilibili.com/video/BV1P6XYYgEjY/)
- [üëâüëâüëâ ÈÄöËøáYouTubeËßÇÁúã](https://youtu.be/KGZ_zM6Xi-U)
- [üëâüëâüëâ ÊàëÁöÑÂºÄÊ∫êÈ°πÁõÆ](https://github.com/win4r/AISuperDomain)
- [üëâüëâüëâ ËØ∑ÊàëÂñùÂíñÂï°](https://ko-fi.com/aila)
- üëâüëâüëâ ÊàëÁöÑÂæÆ‰ø°Ôºöstoeng
- üëâüëâüëâ ÊâøÊé•Â§ßÊ®°ÂûãÂæÆË∞É„ÄÅRAG„ÄÅAIÊô∫ËÉΩ‰Ωì„ÄÅAIÁõ∏ÂÖ≥Â∫îÁî®ÂºÄÂèëÁ≠âÈ°πÁõÆ„ÄÇ

### üî•AIÊô∫ËÉΩ‰ΩìÁõ∏ÂÖ≥ËßÜÈ¢ë

1. [AIÊô∫ËÉΩ‰ΩìËßÜÈ¢ë 1](https://youtu.be/vYm0brFoMwA)  
2. [AIÊô∫ËÉΩ‰ΩìËßÜÈ¢ë 2](https://youtu.be/szTXELuaJos)  
3. [AIÊô∫ËÉΩ‰ΩìËßÜÈ¢ë 3](https://youtu.be/szTXELuaJos)  
4. [AIÊô∫ËÉΩ‰ΩìËßÜÈ¢ë 4](https://youtu.be/RxR3x_Uyq4c)  
5. [AIÊô∫ËÉΩ‰ΩìËßÜÈ¢ë 5](https://youtu.be/IrTEDPnEVvU)  


WindSurf ÂÜÖÁΩÆ‰∫Ü‰∏ÄÊï¥Â•óÂ∑•ÂÖ∑ÈõÜÊàêÁ≥ªÁªü„ÄÇÊó†ËÆ∫ÊòØÊñá‰ª∂ÊêúÁ¥¢„ÄÅÁõÆÂΩïÁÆ°ÁêÜÔºåËøòÊòØÂëΩ‰ª§Ë°åÊâßË°åÔºåÊâÄÊúâÂäüËÉΩÂùáÂÆûÁé∞Êó†ÁºùË°îÊé•„ÄÇÂÄüÂä©Ëøô‰∏ÄÁ≥ªÁªüÔºåÂºÄÂèëËÄÖÂè™ÈúÄÂú®‰∏Ä‰∏™Âπ≥Âè∞ÂÜÖÂ∞±ËÉΩÂÆåÊàêÈ°πÁõÆÁÆ°ÁêÜ„ÄÅ‰ª£Á†ÅÁºñËæë„ÄÅ‰æùËµñÂÆâË£ÖÁ≠âÂ§öÈ°π‰ªªÂä°„ÄÇ‰æãÂ¶ÇÔºåÂú®È°πÁõÆÈáçÊûÑËøáÁ®ã‰∏≠ÔºåWindSurf ÂèØ‰ª•Ëá™Âä®Ê£ÄÊµãÊ∫êÁõÆÂΩï‰∏éÁõÆÊ†áÁõÆÂΩïÁöÑÁä∂ÊÄÅÔºåÊô∫ËÉΩÂà§Êñ≠Êñá‰ª∂ÂÜ≤Á™ÅÔºåÂπ∂Ê†πÊçÆÂéÜÂè≤Êìç‰ΩúËá™Âä®ÈÄâÊã©ÊúÄÂêàÈÄÇÁöÑÊâßË°åÁ≠ñÁï•ÔºåËÆ©Êï¥‰∏™ËøáÁ®ãÂèòÂæóÊµÅÁïÖËÄåÈ´òÊïà„ÄÇ

‰∏é Cursor Áõ∏ÊØîÔºåWindSurf Âú®Â§öÊ≠•È™§‰ªªÂä°ËßÑÂàíÂíåÂçèÂêåÂ∑•‰ΩúÊñπÈù¢‰ºòÂäøÊõ¥‰∏∫ÊòéÊòæ„ÄÇÂÖ∂ÊîØÊåÅÂ§öÂ∑•ÂÖ∑ËÅîÂä®ÔºåÈÄöËøáËá™Âä®Áª¥Êä§‰∏ä‰∏ãÊñáÁä∂ÊÄÅÔºåËÉΩÂ∞ÜÂ§çÊùÇ‰ªªÂä°ÂàÜËß£‰∏∫Â§ö‰∏™Â≠ê‰ªªÂä°ÔºåÈÄêÊ≠•ÂÆåÊàêÊìç‰Ωú„ÄÇËøôÁßçËÉΩÂäõ‰∏ç‰ªÖÈÄÇÁî®‰∫éÂ§ßÂûãÈ°πÁõÆÁöÑ‰ª£Á†ÅÈáçÊûÑÔºå‰πüËÉΩÂ∏ÆÂä©ÂàùÂ≠¶ËÄÖÂú®ÁÜüÊÇâÈ°πÁõÆÁªìÊûÑÁöÑÂêåÊó∂ÔºåÂø´ÈÄüÊéåÊè°ÂºÄÂèëÊäÄÂ∑ß„ÄÇ

WindSurf ÁöÑÊô∫ËÉΩÁºñÁ®ãÂä©ÊâãÂäüËÉΩÊõ¥ÊòØ‰∏∫ÂºÄÂèëËÄÖÊèê‰æõ‰∫ÜÂÖ®Êñ∞ÁöÑ‰∫§‰∫í‰ΩìÈ™å„ÄÇÂà©Áî®ÂÖàËøõÁöÑ GPT-4o Âíå Claude 3.5 Ê®°ÂûãÔºåWindSurf ÂèØÂ§ÑÁêÜÊñáÊú¨„ÄÅÂõæÂÉèÁ≠âÂ§öÊ®°ÊÄÅËæìÂÖ•ÔºåÁîüÊàêÈ´òË¥®Èáè‰ª£Á†ÅÂíåËß£ÂÜ≥ÊñπÊ°à„ÄÇ‰∏ç‰ªÖÂ¶ÇÊ≠§ÔºåÂÆÉËøòËÉΩÊ†πÊçÆÂºÄÂèëËÄÖÁöÑÊìç‰Ωú‰π†ÊÉØËøõË°åËá™Âä®Â≠¶‰π†ÔºåÂ∞ÜÂ∏∏Áî®Êìç‰ΩúËÆ∞ÂΩï‰∏∫ÈöêÂºèËÆ∞ÂøÜÔºå‰ªéËÄåÂú®ÂêéÁª≠ÂØπËØù‰∏≠Êèê‰æõÊõ¥Âä†‰∏™ÊÄßÂåñÁöÑÂª∫ËÆÆ„ÄÇ

Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÊØîÂ¶Ç‰∏Ä‰∏™Âü∫‰∫é Nuxt 3 ÁöÑ AI Â∑•ÂÖ∑ÈõÜÈ°πÁõÆÔºåÂºÄÂèëËÄÖÈúÄË¶ÅÂ∞ÜÊóßÊúâÁõÆÂΩïËøÅÁßªËá≥Êñ∞Êû∂ÊûÑ„ÄÇ‰º†ÁªüÊñπÊ≥ïÂæÄÂæÄÈúÄË¶ÅÂ§öÊ¨°ÊâãÂä®Êìç‰ΩúÔºåËÄå WindSurf ÂàôËÉΩËá™Âä®Êâ´ÊèèÈ°πÁõÆÁªìÊûÑ„ÄÅËØÜÂà´ÈÖçÁΩÆÊñá‰ª∂„ÄÅÂ§ÑÁêÜ‰æùËµñÈóÆÈ¢òÔºåÂπ∂Âà©Áî®ÂÜÖÁΩÆÂëΩ‰ª§È°∫Âà©ÂÆåÊàêÊñá‰ª∂Â§çÂà∂‰∏éÂà†Èô§„ÄÅÈÖçÁΩÆÊõ¥Êñ∞Á≠â‰ªªÂä°„ÄÇÊï¥‰∏™ÊµÅÁ®ãÂá†‰πéÊó†ÈúÄ‰∫∫Â∑•Âπ≤È¢ÑÔºåÂ§ßÂ§ßÊèêÂçá‰∫ÜÈ°πÁõÆÈáçÊûÑÊïàÁéá„ÄÇ

Êú™Êù•ÔºåÈöèÁùÄ AI ÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ïÔºåWindSurf Â∞ÜÊåÅÁª≠‰ºòÂåñÂÖ∂Êô∫ËÉΩ‰∫§‰∫íÂíåËá™Âä®ÂåñËÉΩÂäõÔºåÊãìÂ±ïÊõ¥Â§ö‰ºÅ‰∏öÁ∫ßÂäüËÉΩÔºåÂ¶Ç SSO ËÆ§ËØÅ„ÄÅÊ∑±Â∫¶Êó•ÂøóÂÆ°ËÆ°Á≠â„ÄÇÂá≠ÂÄüÂº∫Â§ßÁöÑ‰∏ä‰∏ãÊñáÊÑüÁü•„ÄÅÂ§öÊ≠•È™§‰ªªÂä°ÂçèÂêåÂíåÁÅµÊ¥ªÁöÑÂ∑•ÂÖ∑ÈõÜÊàêÔºåWindSurf ‰∏ç‰ªÖÈáçÊñ∞ÂÆö‰πâ‰∫Ü AI ËæÖÂä©ÁºñÁ®ãÁöÑÊ†áÂáÜÔºåÊõ¥‰∏∫ÂºÄÂèëËÄÖÂºÄËæü‰∫Ü‰∏ÄÊù°È´òÊïà„ÄÅÊô∫ËÉΩÁöÑÂ∑•‰Ωú‰πãË∑Ø„ÄÇ

WindSurf ‰Ωú‰∏∫‰∏ÄÊ¨æÈõÜÊô∫ËÉΩ„ÄÅÈ´òÊïà„ÄÅ‰æøÊç∑‰∫é‰∏Ä‰ΩìÁöÑÂÖ®Êñ∞ AI ÁºñÁ®ãÂ∑•ÂÖ∑ÔºåÊ≠£Âú®ÂºïÈ¢ÜÁºñÁ®ãÂ∑•ÂÖ∑ÁöÑÊú™Êù•„ÄÇÊó†ËÆ∫ÊòØÂØπ‰∏™‰∫∫ÂºÄÂèëËÄÖËøòÊòØÂõ¢ÈòüÂçè‰ΩúËÄåË®ÄÔºåWindSurf ÈÉΩÂ±ïÁ§∫‰∫ÜÂÆÉÂú®ÊèêÂçá‰ª£Á†ÅË¥®Èáè„ÄÅ‰ºòÂåñÂºÄÂèëÊµÅÁ®ã‰ª•ÂèäÂáèÂ∞ëÈáçÂ§çÂä≥Âä®ÊñπÈù¢ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇÊú™Êù•ÁöÑÁºñÁ®ã‰∏ñÁïåÔºåÂ∞ÜÁî±Ëøô‰∫õÊô∫ËÉΩÂ∑•ÂÖ∑È©±Âä®ÔºåËÆ©ÂºÄÂèëËÄÖÁúüÊ≠£ÂÆûÁé∞‚ÄúÂÜô‰ª£Á†ÅÔºå‰∫´ÂèóÁîüÊ¥ª‚Äù„ÄÇ


### WindSurf Prompt

```bash

execute @task.md 
```

### ËôöÊãüÁéØÂ¢ÉÂàõÂª∫

```markdown

python3 -m venv .venv
source .venv/bin/activate
```

### Claude desktop app prompt

```bash
 Using lightrag-server naive_search Tool to Search for "Êï∞ÊçÆË¥®ÈáèÊéßÂà∂ÊµÅÁ®ã"ÔºåÂπ∂Áî®‰∏≠ÊñáÂõûÁ≠î
```

### AutoGen

```bash

pip install -U "autogen-ext[mcp]"
pip install -U "autogen-agentchat"

import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools

async def main() -> None:
    # Get the fetch tool from mcp-server-fetch.
    rag_mcp_server = StdioServerParams(
        command="/Users/charlesqin/PycharmProjects/ligtrag-test/.venv/bin/python",
        args=["/Users/charlesqin/PycharmProjects/ligtrag-test/mcp_server.py"],
        env={"OPENAI_API_KEY": "sk-proj-xxxxxxxx"}
    )
    tools = await mcp_server_tools(rag_mcp_server)

    # Create an agent that can use the fetch tool.
    model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")
    agent = AssistantAgent(name="fetcher", model_client=model_client, tools=tools, reflect_on_tool_use=True)  # type: ignore

    # Let the agent fetch the content of a URL and summarize it.
    result = await agent.run(task="Using lightrag-server naive_search Tool to Search for 'Êï∞ÊçÆË¥®ÈáèÊéßÂà∂ÊµÅÁ®ã'ÔºåÂπ∂Áî®‰∏≠ÊñáÂõûÁ≠î")
    print(result.messages[-1].content)

asyncio.run(main())

```

### MCPÈÖçÁΩÆÊñá‰ª∂

```bash
{
  "mcpServers": {
    "lightrag": {
      "command": "/Users/charlesqin/PycharmProjects/ligtrag-test/.venv/bin/python",
      "args": ["/Users/charlesqin/PycharmProjects/ligtrag-test/mcp_server.py"],
      "env": {
        "OPENAI_API_KEY": "sk-proj---"
      }
    }
  }
}
```

```bash
#--------------------------ÂÆâË£ÖLightRAG--------------------------#

git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
pip install -e .
cd ..
export OPENAI_API_KEY=sk-proj-xxxxxxx

curl https://raw.githubusercontent.com/win4r/mytest/refs/heads/main/book.txt > ./book.txt

#--------------------------OpenAI--------------------------#

import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status

WORKING_DIR = "./mybook"

if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
        # llm_model_func=gpt_4o_complete
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())

    with open("./book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="naive")
        )
    )

    # Perform local search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="local")
        )
    )

    # Perform global search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="global")
        )
    )

    # Perform hybrid search
    print(
        rag.query(
            "What are the top themes in this story?", param=QueryParam(mode="hybrid")
        )
    )

if __name__ == "__main__":
    main()
    

#--------------------------ollama--------------------------#

  
import asyncio
import nest_asyncio

nest_asyncio.apply()
import os
import inspect
import logging
from lightrag import LightRAG, QueryParam
from lightrag.llm.ollama import ollama_model_complete, ollama_embed
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status

WORKING_DIR = "./dickens"

logging.basicConfig(format="%(levelname)s:%(message)s", level=logging.INFO)

if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=ollama_model_complete,
        llm_model_name="gemma2:2b",
        llm_model_max_async=4,
        llm_model_max_token_size=32768,
        llm_model_kwargs={
            "host": "http://localhost:11434",
            "options": {"num_ctx": 32768},
        },
        embedding_func=EmbeddingFunc(
            embedding_dim=768,
            max_token_size=8192,
            func=lambda texts: ollama_embed(
                texts, embed_model="nomic-embed-text", host="http://localhost:11434"
            ),
        ),
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag

async def print_stream(stream):
    async for chunk in stream:
        print(chunk, end="", flush=True)

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())

    # Insert example text
    with open("./book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Test different query modes
    print("\nNaive Search:")
    print(
        rag.query(
            "What are the top themes in this story?", param=QueryParam(mode="naive")
        )
    )

    print("\nLocal Search:")
    print(
        rag.query(
            "What are the top themes in this story?", param=QueryParam(mode="local")
        )
    )

    print("\nGlobal Search:")
    print(
        rag.query(
            "What are the top themes in this story?", param=QueryParam(mode="global")
        )
    )

    print("\nHybrid Search:")
    print(
        rag.query(
            "What are the top themes in this story?", param=QueryParam(mode="hybrid")
        )
    )

    # stream response
    resp = rag.query(
        "What are the top themes in this story?",
        param=QueryParam(mode="hybrid", stream=True),
    )

    if inspect.isasyncgen(resp):
        asyncio.run(print_stream(resp))
    else:
        print(resp)

if __name__ == "__main__":
    main()
    
    
 

```

### windsurf Prompt

```bash
### task:

Build an MCP server for the LightRAG.
This server should allow me to input any query text and perform search operations using four distinct MCP Tools: naive search, local search, global search, and hybrid search.

Support Multiple Query Modes:

Naive Search: Perform a straightforward search over the inserted content.

Local Search: Focus on retrieving results within a limited, local context of the inserted documents.

Global Search: Conduct a comprehensive search across the entire dataset.

Hybrid Search: Combine aspects of local and global searches to enhance result relevance and consistency.

The WORKING_DIR path is /Users/charlesqin/PycharmProjects/ligtrag-test

Please activate the venv:
cd /Users/charlesqin/PycharmProjects/ligtrag-test
source .venv/bin/activate

Here's the Code and Response for the LightRAG:

### Code (app.py):

import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status

WORKING_DIR = "./mybook"

if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
        # llm_model_func=gpt_4o_complete
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())

    with open("./book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="naive")
        )
    )

    # Perform local search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="local")
        )
    )

    # Perform global search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="global")
        )
    )

    # Perform hybrid search
    print(
        rag.query(
            "What are the top themes in this story?", param=QueryParam(mode="hybrid")
        )
    )

if __name__ == "__main__":
    main()
    
    

### Response:

[Full response logs showing the initialization and search results]

(.venv) charlesqin@charless-MacBook-Pro ligtrag-test % python app.py  
INFO: Process 31212 Shared-Data created for Single Process
INFO:nano-vectordb:Load (89, 1536) data
INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './mybook/vdb_entities.json'} 89 data
INFO:nano-vectordb:Load (85, 1536) data
INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './mybook/vdb_relationships.json'} 85 data
INFO:nano-vectordb:Load (9, 1536) data
INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './mybook/vdb_chunks.json'} 9 data
INFO: Process 31212 initialized updated flags for namespace: [full_docs]
INFO: Process 31212 ready to initialize storage namespace: [full_docs]
INFO: Process 31212 initialized updated flags for namespace: [text_chunks]
INFO: Process 31212 ready to initialize storage namespace: [text_chunks]
INFO: Process 31212 initialized updated flags for namespace: [entities]
INFO: Process 31212 initialized updated flags for namespace: [relationships]
INFO: Process 31212 initialized updated flags for namespace: [chunks]
INFO: Process 31212 initialized updated flags for namespace: [chunk_entity_relation]
INFO: Process 31212 initialized updated flags for namespace: [llm_response_cache]
INFO: Process 31212 ready to initialize storage namespace: [llm_response_cache]
INFO: Process 31212 initialized updated flags for namespace: [doc_status]
INFO: Process 31212 ready to initialize storage namespace: [doc_status]
INFO: Process 31212 storage namespace already initialized: [full_docs]
INFO: Process 31212 storage namespace already initialized: [text_chunks]
INFO: Process 31212 storage namespace already initialized: [llm_response_cache]
INFO: Process 31212 storage namespace already initialized: [doc_status]
INFO: Process 31212 Pipeline namespace initialized
## Self-Consistency Prompt

The Self-Consistency prompt is a technique utilized to ensure that the output generated by ChatGPT maintains consistency with the provided input. This approach is particularly useful for tasks such as fact-checking, data validation, or ensuring consistency in text generation.

### Prompt Formula
The prompt formula for the Self-Consistency prompt involves embedding the input text alongside an instruction. The instruction typically states, "Please ensure the following text is self-consistent."

### Examples of Application

1. **Text Generation**
   - **Task**: Generate a product review.
   - **Instructions**: The review should be consistent with the product information provided in the input.
   - **Prompt formula**: "Generate a product review that is consistent with the following product information [insert product information]."

2. **Text Summarization**
   - **Task**: Summarize a news article.
   - **Instructions**: The summary should be consistent with the information provided in the article.
   - **Prompt formula**: "Summarize the following news article in a way that is consistent with the information provided [insert news article]."

3. **Fact-checking**
   - **Task**: Check for consistency in a given news article.
   - **Input text**: "The article states that the population of the city is 5 million, but later on, it says that the population is 7 million."
   - **Prompt formula**: "Please ensure the following text is self-consistent: The article states that the population of the city is 5 million, but later on, it says that the population is 7 million."

4. **Data Validation**
   - **Task**: Check for consistency in a given data set.
   - **Input text**: "The data shows that the average temperature in July is 30 degrees, but the minimum temperature is recorded as 20 degrees."
   - **Prompt formula**: "Please ensure the following text is self-consistent: The data shows that the average temperature in July is 30 degrees, but the minimum temperature is recorded as 20 degrees."

By employing the Self-Consistency prompt, users can enhance the reliability of the text generated by ensuring that it aligns with the provided inputs.

### References
- [KG] Self-Consistency Prompt Description (File: Document Chunks)
- [KG] Application Examples for Self-Consistency Prompts (File: Document Chunks)
The Knowledge Base does not provide specific information on the "Self-Consistency Prompt." It typically discusses various prompt engineering techniques and their applications with language models like ChatGPT, but mentions self-consistency without elaborating on its definition or use cases.

If you have more detailed questions about other prompt techniques or topics related to prompt engineering, please feel free to ask!

### References
- [KG] Prompt Engineering Techniques encompass a variety of methods and practices for effectively engaging with language models like ChatGPT. (File: unknown_source)
- [KG] The Art of Asking ChatGPT for High-Quality Answers: A Complete Guide to Prompt Engineering Techniques covers various techniques related to prompt engineering and the effective use of ChatGPT. (File: unknown_source)
- [KG] ChatGPT is a state-of-the-art language model capable of generating human-like text based on the prompts given. (File: unknown_source)
The Knowledge Base provided does not contain specific information about the "Self-Consistency Prompt." This term may refer to a prompting technique meant to encourage models, like ChatGPT, to generate consistent outputs across different prompts or contexts, but further details are not available in the provided content. Therefore, I cannot provide a detailed explanation or context for the Self-Consistency Prompt.

If you have any other questions or need information on a different topic, feel free to ask! 

### References
- [KG] Ibrahim John discusses various prompting techniques in his book (File: unknown_source)
- [KG] Knowledge generation techniques are outlined in the book's content (File: unknown_source)
I don‚Äôt have specific details about the story you‚Äôre referring to, as the Knowledge Base does not provide information on any particular narrative beyond discussing techniques for text generation and prompt engineering. 

However, common themes in literature often include:

1. **Love** - exploring emotional connections and relationships.
2. **Personal Growth** - focusing on the journey of self-discovery and development.
3. **Conflict** - presenting struggles between characters or within oneself.
4. **Change** - highlighting transformations in characters or settings (e.g., changing seasons).
5. **Morality** - examining the concepts of right and wrong.

If you could provide more details or specify the story, I could help further! 

### References
- [KG] Love is a complex emotion that forms the basis of many poems and literary works. (File: unknown_source)
- [KG] Personal growth refers to the process of self-improvement and development, focusing on various aspects of life such as emotional, intellectual and relational growth. (File: unknown_source)
- [KG] The changing seasons refer to the transition periods throughout the year characterized by different weather patterns, environmental changes, and their impact on nature and human life. (File: unknown_source)
(.venv) charlesqin@charless-MacBook-Pro ligtrag-test % 

### The MCP document is:
‚ö†Ô∏èËØ∑Â∞Ü‰∏ãÈù¢ÈìæÊé•ÈáåÁöÑMCPÊñáÊ°£ÂÆåÊï¥ÁöÑÂ§çÂà∂Á≤òË¥¥Âà∞ËøôÈáåÔºåÁî±‰∫éÂçöÂÆ¢Â≠óÊï∞ÊúâÈôêÔºåÊâÄ‰ª•ËØ∑Ëá™Ë°åÂ§çÂà∂ÊñáÊ°£ÂÜÖÂÆπ‚¨áÔ∏è
https://modelcontextprotocol.io/llms-full.txt

```