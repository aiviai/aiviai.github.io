---
layout: single
title: "ğŸš€Cursoré™ä½æ™ºå•†ï¼WindSurfé›¶ä»£ç å¼€å‘MCP Serverï¼äº”åˆ†é’Ÿè½»æ¾å®ç°LightRAG+MCPä¸ºClaudeå’ŒAutoGenæŒ‚è½½çŸ¥è¯†åº“ï¼å¢å¼ºClaudeå’ŒAutoGençš„çŸ¥è¯†åº“æ£€ç´¢èƒ½åŠ›"
sidebar:
  nav: "docs"
date: 2025-03-23 00:00:00 +0800
categories: AIAgents
tags: [MCP, MCP Server, Cursor, WindSurf, AIç¼–ç¨‹]
classes: wide
author_profile: true
---


ä½¿ç”¨WindSurfå¼€å‘LightRAG MCPæœåŠ¡å™¨ï¼Œå¢å¼ºClaudeå’ŒAutoGençš„çŸ¥è¯†åº“èƒ½åŠ›ã€‚æœ¬è§†é¢‘å±•ç¤ºå¦‚ä½•ä½¿ç”¨WindSurfï¼ˆæ— éœ€ç¼–å†™ä»£ç ï¼‰å¼€å‘ä¸€ä¸ªLightRAG MCPæœåŠ¡å™¨ï¼Œå¹¶å°†å…¶é›†æˆåˆ°Claudeæ¡Œé¢ç‰ˆå’ŒAutoGenæ™ºèƒ½ä½“æ¡†æ¶ä¸­ï¼Œæä¾›å¼ºå¤§çš„çŸ¥è¯†åº“æ£€ç´¢åŠŸèƒ½ã€‚

éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­çªç ´ï¼Œç¼–ç¨‹å·¥å…·æ­£è¿æ¥å‰æ‰€æœªæœ‰çš„å˜é©ã€‚ç”± Codeium å›¢é˜Ÿæ¨å‡ºçš„ WindSurfï¼Œä»¥å…¨æ–°çš„ AI Flow èŒƒå¼å’Œå¤šå·¥å…·ååŒèƒ½åŠ›ï¼Œæ­£é€æ­¥è¶…è¶Šå¤‡å—å…³æ³¨çš„ Cursorï¼Œæˆä¸ºå¼€å‘è€…æé«˜å·¥ä½œæ•ˆç‡çš„åˆ©å™¨ã€‚

WindSurf çš„æœ€å¤§äº®ç‚¹åœ¨äºå…¶æ·±åº¦ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„ç¼–ç¨‹åŠ©æ‰‹å¾€å¾€åªèƒ½å¯¹ç®€å•ä»£ç ç‰‡æ®µè¿›è¡Œè¡¥å…¨ï¼Œè€Œ WindSurf åˆ™èƒ½æ™ºèƒ½æ•æ‰é¡¹ç›®æ•´ä½“ç»“æ„ã€å˜é‡å…³ç³»ä»¥åŠå‡½æ•°è°ƒç”¨é“¾ï¼Œæ— éœ€å¼€å‘è€…åå¤è¾“å…¥æç¤ºã€‚å®ƒèƒ½å¤Ÿä¸»åŠ¨é¢„æµ‹éœ€æ±‚ï¼Œåœ¨ä»£ç ç¼–å†™ã€é‡æ„ã€è°ƒè¯•ç­‰è¿‡ç¨‹ä¸­æä¾›ç²¾å‡†å»ºè®®ï¼Œä»è€Œå¤§å¹…å‡å°‘æ‰‹åŠ¨è°ƒè¯•å’Œåå¤ç¡®è®¤çš„æ—¶é—´ã€‚

### ğŸš€æœ¬ç¯‡ç¬”è®°æ‰€å¯¹åº”çš„è§†é¢‘ï¼š

- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡å“”å“©å“”å“©è§‚çœ‹](https://www.bilibili.com/video/BV1P6XYYgEjY/)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡YouTubeè§‚çœ‹](https://youtu.be/KGZ_zM6Xi-U)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¼€æºé¡¹ç›®](https://github.com/win4r/AISuperDomain)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ è¯·æˆ‘å–å’–å•¡](https://ko-fi.com/aila)
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¾®ä¿¡ï¼šstoeng
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æ‰¿æ¥å¤§æ¨¡å‹å¾®è°ƒã€RAGã€AIæ™ºèƒ½ä½“ã€AIç›¸å…³åº”ç”¨å¼€å‘ç­‰é¡¹ç›®ã€‚

### ğŸ”¥AIæ™ºèƒ½ä½“ç›¸å…³è§†é¢‘

1. [AIæ™ºèƒ½ä½“è§†é¢‘ 1](https://youtu.be/vYm0brFoMwA)  
2. [AIæ™ºèƒ½ä½“è§†é¢‘ 2](https://youtu.be/szTXELuaJos)  
3. [AIæ™ºèƒ½ä½“è§†é¢‘ 3](https://youtu.be/szTXELuaJos)  
4. [AIæ™ºèƒ½ä½“è§†é¢‘ 4](https://youtu.be/RxR3x_Uyq4c)  
5. [AIæ™ºèƒ½ä½“è§†é¢‘ 5](https://youtu.be/IrTEDPnEVvU)  


WindSurf å†…ç½®äº†ä¸€æ•´å¥—å·¥å…·é›†æˆç³»ç»Ÿã€‚æ— è®ºæ˜¯æ–‡ä»¶æœç´¢ã€ç›®å½•ç®¡ç†ï¼Œè¿˜æ˜¯å‘½ä»¤è¡Œæ‰§è¡Œï¼Œæ‰€æœ‰åŠŸèƒ½å‡å®ç°æ— ç¼è¡”æ¥ã€‚å€ŸåŠ©è¿™ä¸€ç³»ç»Ÿï¼Œå¼€å‘è€…åªéœ€åœ¨ä¸€ä¸ªå¹³å°å†…å°±èƒ½å®Œæˆé¡¹ç›®ç®¡ç†ã€ä»£ç ç¼–è¾‘ã€ä¾èµ–å®‰è£…ç­‰å¤šé¡¹ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨é¡¹ç›®é‡æ„è¿‡ç¨‹ä¸­ï¼ŒWindSurf å¯ä»¥è‡ªåŠ¨æ£€æµ‹æºç›®å½•ä¸ç›®æ ‡ç›®å½•çš„çŠ¶æ€ï¼Œæ™ºèƒ½åˆ¤æ–­æ–‡ä»¶å†²çªï¼Œå¹¶æ ¹æ®å†å²æ“ä½œè‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„æ‰§è¡Œç­–ç•¥ï¼Œè®©æ•´ä¸ªè¿‡ç¨‹å˜å¾—æµç•…è€Œé«˜æ•ˆã€‚

ä¸ Cursor ç›¸æ¯”ï¼ŒWindSurf åœ¨å¤šæ­¥éª¤ä»»åŠ¡è§„åˆ’å’ŒååŒå·¥ä½œæ–¹é¢ä¼˜åŠ¿æ›´ä¸ºæ˜æ˜¾ã€‚å…¶æ”¯æŒå¤šå·¥å…·è”åŠ¨ï¼Œé€šè¿‡è‡ªåŠ¨ç»´æŠ¤ä¸Šä¸‹æ–‡çŠ¶æ€ï¼Œèƒ½å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œé€æ­¥å®Œæˆæ“ä½œã€‚è¿™ç§èƒ½åŠ›ä¸ä»…é€‚ç”¨äºå¤§å‹é¡¹ç›®çš„ä»£ç é‡æ„ï¼Œä¹Ÿèƒ½å¸®åŠ©åˆå­¦è€…åœ¨ç†Ÿæ‚‰é¡¹ç›®ç»“æ„çš„åŒæ—¶ï¼Œå¿«é€ŸæŒæ¡å¼€å‘æŠ€å·§ã€‚

WindSurf çš„æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹åŠŸèƒ½æ›´æ˜¯ä¸ºå¼€å‘è€…æä¾›äº†å…¨æ–°çš„äº¤äº’ä½“éªŒã€‚åˆ©ç”¨å…ˆè¿›çš„ GPT-4o å’Œ Claude 3.5 æ¨¡å‹ï¼ŒWindSurf å¯å¤„ç†æ–‡æœ¬ã€å›¾åƒç­‰å¤šæ¨¡æ€è¾“å…¥ï¼Œç”Ÿæˆé«˜è´¨é‡ä»£ç å’Œè§£å†³æ–¹æ¡ˆã€‚ä¸ä»…å¦‚æ­¤ï¼Œå®ƒè¿˜èƒ½æ ¹æ®å¼€å‘è€…çš„æ“ä½œä¹ æƒ¯è¿›è¡Œè‡ªåŠ¨å­¦ä¹ ï¼Œå°†å¸¸ç”¨æ“ä½œè®°å½•ä¸ºéšå¼è®°å¿†ï¼Œä»è€Œåœ¨åç»­å¯¹è¯ä¸­æä¾›æ›´åŠ ä¸ªæ€§åŒ–çš„å»ºè®®ã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ¯”å¦‚ä¸€ä¸ªåŸºäº Nuxt 3 çš„ AI å·¥å…·é›†é¡¹ç›®ï¼Œå¼€å‘è€…éœ€è¦å°†æ—§æœ‰ç›®å½•è¿ç§»è‡³æ–°æ¶æ„ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€éœ€è¦å¤šæ¬¡æ‰‹åŠ¨æ“ä½œï¼Œè€Œ WindSurf åˆ™èƒ½è‡ªåŠ¨æ‰«æé¡¹ç›®ç»“æ„ã€è¯†åˆ«é…ç½®æ–‡ä»¶ã€å¤„ç†ä¾èµ–é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å†…ç½®å‘½ä»¤é¡ºåˆ©å®Œæˆæ–‡ä»¶å¤åˆ¶ä¸åˆ é™¤ã€é…ç½®æ›´æ–°ç­‰ä»»åŠ¡ã€‚æ•´ä¸ªæµç¨‹å‡ ä¹æ— éœ€äººå·¥å¹²é¢„ï¼Œå¤§å¤§æå‡äº†é¡¹ç›®é‡æ„æ•ˆç‡ã€‚

æœªæ¥ï¼Œéšç€ AI æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼ŒWindSurf å°†æŒç»­ä¼˜åŒ–å…¶æ™ºèƒ½äº¤äº’å’Œè‡ªåŠ¨åŒ–èƒ½åŠ›ï¼Œæ‹“å±•æ›´å¤šä¼ä¸šçº§åŠŸèƒ½ï¼Œå¦‚ SSO è®¤è¯ã€æ·±åº¦æ—¥å¿—å®¡è®¡ç­‰ã€‚å‡­å€Ÿå¼ºå¤§çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€å¤šæ­¥éª¤ä»»åŠ¡ååŒå’Œçµæ´»çš„å·¥å…·é›†æˆï¼ŒWindSurf ä¸ä»…é‡æ–°å®šä¹‰äº† AI è¾…åŠ©ç¼–ç¨‹çš„æ ‡å‡†ï¼Œæ›´ä¸ºå¼€å‘è€…å¼€è¾Ÿäº†ä¸€æ¡é«˜æ•ˆã€æ™ºèƒ½çš„å·¥ä½œä¹‹è·¯ã€‚

WindSurf ä½œä¸ºä¸€æ¬¾é›†æ™ºèƒ½ã€é«˜æ•ˆã€ä¾¿æ·äºä¸€ä½“çš„å…¨æ–° AI ç¼–ç¨‹å·¥å…·ï¼Œæ­£åœ¨å¼•é¢†ç¼–ç¨‹å·¥å…·çš„æœªæ¥ã€‚æ— è®ºæ˜¯å¯¹ä¸ªäººå¼€å‘è€…è¿˜æ˜¯å›¢é˜Ÿåä½œè€Œè¨€ï¼ŒWindSurf éƒ½å±•ç¤ºäº†å®ƒåœ¨æå‡ä»£ç è´¨é‡ã€ä¼˜åŒ–å¼€å‘æµç¨‹ä»¥åŠå‡å°‘é‡å¤åŠ³åŠ¨æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚æœªæ¥çš„ç¼–ç¨‹ä¸–ç•Œï¼Œå°†ç”±è¿™äº›æ™ºèƒ½å·¥å…·é©±åŠ¨ï¼Œè®©å¼€å‘è€…çœŸæ­£å®ç°â€œå†™ä»£ç ï¼Œäº«å—ç”Ÿæ´»â€ã€‚

### WindSurf Prompt

```bash

source .venv/bin/activate
execute @task.md 
```

### Claude desktop app prompt

```bash
 Using lightrag-server naive_search Tool to Search for "æ•°æ®è´¨é‡æ§åˆ¶æµç¨‹"ï¼Œå¹¶ç”¨ä¸­æ–‡å›ç­”
```

### AutoGen

```bash

pip install -U "autogen-ext[mcp]"

import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools

async def main() -> None:
    # Get the fetch tool from mcp-server-fetch.
    rag_mcp_server = StdioServerParams(
        command="/Users/charlesqin/PycharmProjects/ligtrag-test/.venv/bin/python",
        args=["/Users/charlesqin/PycharmProjects/ligtrag-test/mcp_server.py"],
        env={"OPENAI_API_KEY": "sk-proj-xxxxxxxx"}
    )
    tools = await mcp_server_tools(rag_mcp_server)

    # Create an agent that can use the fetch tool.
    model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")
    agent = AssistantAgent(name="fetcher", model_client=model_client, tools=tools, reflect_on_tool_use=True)  # type: ignore

    # Let the agent fetch the content of a URL and summarize it.
    result = await agent.run(task="Using lightrag-server naive_search Tool to Search for 'æ•°æ®è´¨é‡æ§åˆ¶æµç¨‹'ï¼Œå¹¶ç”¨ä¸­æ–‡å›ç­”")
    print(result.messages[-1].content)

asyncio.run(main())

```

### MCPé…ç½®æ–‡ä»¶

```bash
{
  "mcpServers": {
    "lightrag": {
      "command": "/Users/charlesqin/PycharmProjects/ligtrag-test/.venv/bin/python",
      "args": ["/Users/charlesqin/PycharmProjects/ligtrag-test/mcp_server.py"],
      "env": {
        "OPENAI_API_KEY": "sk-proj---"
      }
    }
  }
}
```

```bash
#--------------------------å®‰è£…LightRAG--------------------------#

git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
pip install -e .
cd ..
export OPENAI_API_KEY=sk-proj-xxxxxxx

curl https://raw.githubusercontent.com/win4r/mytest/refs/heads/main/book.txt > ./book.txt

#--------------------------OpenAI--------------------------#

import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status

WORKING_DIR = "./mybook"

if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
        # llm_model_func=gpt_4o_complete
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())

    with open("./book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="naive")
        )
    )

    # Perform local search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="local")
        )
    )

    # Perform global search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="global")
        )
    )

    # Perform hybrid search
    print(
        rag.query(
            "What are the top themes in this story?", param=QueryParam(mode="hybrid")
        )
    )

if __name__ == "__main__":
    main()
    
    
 

```

### windsurf Prompt

```bash
### task:

ä¸º LightRAG æ„å»ºä¸€ä¸ª MCP æœåŠ¡å™¨(MCP Server)ã€‚  

è¯¥æœåŠ¡å™¨åº”å…è®¸æˆ‘è¾“å…¥ä»»æ„æŸ¥è¯¢æ–‡æœ¬ï¼Œå¹¶ä½¿ç”¨å››ç§ä¸åŒçš„ MCP å·¥å…·æ‰§è¡Œæœç´¢æ“ä½œï¼š**æœ´ç´ æœç´¢ã€æœ¬åœ°æœç´¢ã€å…¨å±€æœç´¢å’Œæ··åˆæœç´¢**ã€‚  

### æ”¯æŒå¤šç§æŸ¥è¯¢æ¨¡å¼ï¼š  

- **æœ´ç´ æœç´¢ï¼ˆNaive Searchï¼‰**ï¼šå¯¹æ’å…¥çš„å†…å®¹æ‰§è¡Œç›´æ¥æœç´¢ã€‚  
- **æœ¬åœ°æœç´¢ï¼ˆLocal Searchï¼‰**ï¼šåœ¨æ’å…¥æ–‡æ¡£çš„å±€éƒ¨ä¸Šä¸‹æ–‡ä¸­æ£€ç´¢ç»“æœã€‚  
- **å…¨å±€æœç´¢ï¼ˆGlobal Searchï¼‰**ï¼šå¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œå…¨é¢æœç´¢ã€‚  
- **æ··åˆæœç´¢ï¼ˆHybrid Searchï¼‰**ï¼šç»“åˆæœ¬åœ°æœç´¢å’Œå…¨å±€æœç´¢çš„ç‰¹ç‚¹ï¼Œä»¥æé«˜ç»“æœçš„ç›¸å…³æ€§å’Œä¸€è‡´æ€§ã€‚  

**å·¥ä½œç›®å½•ï¼ˆWORKING_DIRï¼‰**è·¯å¾„ä¸ºï¼š  
`/Users/charlesqin/PycharmProjects/ligtrag-test`  

è¯·æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆvenvï¼‰ï¼š
cd /Users/charlesqin/Documents/test-mcp
source .venv/bin/activate

export OPENAI_API_KEY=sk-proj-aZpGaivSXgNJ--

Here's the Code and Response for the LightRAG:

### Code (app.py):

import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status

WORKING_DIR = "./mybook"

if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
        # llm_model_func=gpt_4o_complete
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())

    with open("./book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="naive")
        )
    )

    # Perform local search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="local")
        )
    )

    # Perform global search
    print(
        rag.query(
            "What is the Self-Consistency Prompt?", param=QueryParam(mode="global")
        )
    )

    # Perform hybrid search
    print(
        rag.query(
            "What are the top themes in this story?", param=QueryParam(mode="hybrid")
        )
    )

if __name__ == "__main__":
    main()
    
    

### Response:

[Full response logs showing the initialization and search results]

(.venv) charlesqin@charless-MacBook-Pro ligtrag-test % python app.py  
INFO: Process 31212 Shared-Data created for Single Process
INFO:nano-vectordb:Load (89, 1536) data
INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './mybook/vdb_entities.json'} 89 data
INFO:nano-vectordb:Load (85, 1536) data
INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './mybook/vdb_relationships.json'} 85 data
INFO:nano-vectordb:Load (9, 1536) data
INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './mybook/vdb_chunks.json'} 9 data
INFO: Process 31212 initialized updated flags for namespace: [full_docs]
INFO: Process 31212 ready to initialize storage namespace: [full_docs]
INFO: Process 31212 initialized updated flags for namespace: [text_chunks]
INFO: Process 31212 ready to initialize storage namespace: [text_chunks]
INFO: Process 31212 initialized updated flags for namespace: [entities]
INFO: Process 31212 initialized updated flags for namespace: [relationships]
INFO: Process 31212 initialized updated flags for namespace: [chunks]
INFO: Process 31212 initialized updated flags for namespace: [chunk_entity_relation]
INFO: Process 31212 initialized updated flags for namespace: [llm_response_cache]
INFO: Process 31212 ready to initialize storage namespace: [llm_response_cache]
INFO: Process 31212 initialized updated flags for namespace: [doc_status]
INFO: Process 31212 ready to initialize storage namespace: [doc_status]
INFO: Process 31212 storage namespace already initialized: [full_docs]
INFO: Process 31212 storage namespace already initialized: [text_chunks]
INFO: Process 31212 storage namespace already initialized: [llm_response_cache]
INFO: Process 31212 storage namespace already initialized: [doc_status]
INFO: Process 31212 Pipeline namespace initialized
## Self-Consistency Prompt

The Self-Consistency prompt is a technique utilized to ensure that the output generated by ChatGPT maintains consistency with the provided input. This approach is particularly useful for tasks such as fact-checking, data validation, or ensuring consistency in text generation.

### Prompt Formula
The prompt formula for the Self-Consistency prompt involves embedding the input text alongside an instruction. The instruction typically states, "Please ensure the following text is self-consistent."

### Examples of Application

1. **Text Generation**
   - **Task**: Generate a product review.
   - **Instructions**: The review should be consistent with the product information provided in the input.
   - **Prompt formula**: "Generate a product review that is consistent with the following product information [insert product information]."

2. **Text Summarization**
   - **Task**: Summarize a news article.
   - **Instructions**: The summary should be consistent with the information provided in the article.
   - **Prompt formula**: "Summarize the following news article in a way that is consistent with the information provided [insert news article]."

3. **Fact-checking**
   - **Task**: Check for consistency in a given news article.
   - **Input text**: "The article states that the population of the city is 5 million, but later on, it says that the population is 7 million."
   - **Prompt formula**: "Please ensure the following text is self-consistent: The article states that the population of the city is 5 million, but later on, it says that the population is 7 million."

4. **Data Validation**
   - **Task**: Check for consistency in a given data set.
   - **Input text**: "The data shows that the average temperature in July is 30 degrees, but the minimum temperature is recorded as 20 degrees."
   - **Prompt formula**: "Please ensure the following text is self-consistent: The data shows that the average temperature in July is 30 degrees, but the minimum temperature is recorded as 20 degrees."

By employing the Self-Consistency prompt, users can enhance the reliability of the text generated by ensuring that it aligns with the provided inputs.

### References
- [KG] Self-Consistency Prompt Description (File: Document Chunks)
- [KG] Application Examples for Self-Consistency Prompts (File: Document Chunks)
The Knowledge Base does not provide specific information on the "Self-Consistency Prompt." It typically discusses various prompt engineering techniques and their applications with language models like ChatGPT, but mentions self-consistency without elaborating on its definition or use cases.

If you have more detailed questions about other prompt techniques or topics related to prompt engineering, please feel free to ask!

### References
- [KG] Prompt Engineering Techniques encompass a variety of methods and practices for effectively engaging with language models like ChatGPT. (File: unknown_source)
- [KG] The Art of Asking ChatGPT for High-Quality Answers: A Complete Guide to Prompt Engineering Techniques covers various techniques related to prompt engineering and the effective use of ChatGPT. (File: unknown_source)
- [KG] ChatGPT is a state-of-the-art language model capable of generating human-like text based on the prompts given. (File: unknown_source)
The Knowledge Base provided does not contain specific information about the "Self-Consistency Prompt." This term may refer to a prompting technique meant to encourage models, like ChatGPT, to generate consistent outputs across different prompts or contexts, but further details are not available in the provided content. Therefore, I cannot provide a detailed explanation or context for the Self-Consistency Prompt.

If you have any other questions or need information on a different topic, feel free to ask! 

### References
- [KG] Ibrahim John discusses various prompting techniques in his book (File: unknown_source)
- [KG] Knowledge generation techniques are outlined in the book's content (File: unknown_source)
I donâ€™t have specific details about the story youâ€™re referring to, as the Knowledge Base does not provide information on any particular narrative beyond discussing techniques for text generation and prompt engineering. 

However, common themes in literature often include:

1. **Love** - exploring emotional connections and relationships.
2. **Personal Growth** - focusing on the journey of self-discovery and development.
3. **Conflict** - presenting struggles between characters or within oneself.
4. **Change** - highlighting transformations in characters or settings (e.g., changing seasons).
5. **Morality** - examining the concepts of right and wrong.

If you could provide more details or specify the story, I could help further! 

### References
- [KG] Love is a complex emotion that forms the basis of many poems and literary works. (File: unknown_source)
- [KG] Personal growth refers to the process of self-improvement and development, focusing on various aspects of life such as emotional, intellectual and relational growth. (File: unknown_source)
- [KG] The changing seasons refer to the transition periods throughout the year characterized by different weather patterns, environmental changes, and their impact on nature and human life. (File: unknown_source)
(.venv) charlesqin@charless-MacBook-Pro ligtrag-test % 

### The MCP document is:
https://modelcontextprotocol.io/llms-full.txt

```