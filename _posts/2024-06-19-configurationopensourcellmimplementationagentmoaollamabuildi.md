---
layout: single
title: "æœ¬åœ°é…ç½®å¼€æºå¤§æ¨¡å‹å®ç°æ··åˆæ™ºèƒ½ä½“ï¼ŒMoA+ollamaæ‰“é€ çœŸæ­£è¶…è¶Šgpt4oçš„AI agent"
sidebar:
  nav: "docs"
date: 2024-06-19 00:00:00 +0800
categories: [AI-Agents, Fine-Tuning, LLM, RAG]
tags: [AI, AI-Agents, GPT, GPT-4, LLM, Llama-3, Mistral, Ollama, RAG]
classes: wide
author_profile: true
---


#  æœ¬åœ°é…ç½®å¼€æºå¤§æ¨¡å‹å®ç°æ··åˆæ™ºèƒ½ä½“ï¼ŒMoA+ollamaæ‰“é€ çœŸæ­£è¶…è¶Šgpt4oçš„AI agent 

###  è§†é¢‘ä¸­çš„æ¨¡å‹ç»„åˆ 

> phi3 3.8b   
>  llama3 8b   
>  mistral 7b   
> 

###  è®ºæ–‡: [ https://arxiv.org/abs/2406.04692 ](<https://arxiv.org/abs/2406.04692>)

##  é¡¹ç›®åœ°å€ 

###  1.æ”¯æŒollamaç­‰å„ç§APIæ¥å£çš„MoA [ https://github.com/win4r/MoA ](<https://github.com/win4r/MoA>)

###  2.å®˜æ–¹ç‰ˆMoA(åªæ”¯æŒtogetherAPI) [ https://github.com/togethercomputer/MoA ](<https://github.com/togethercomputer/MoA>)

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng 

##  **ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡**

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>) **

###  è®ºæ–‡ç®€ä»‹: 

> è®ºæ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºæ··åˆæ™ºèƒ½ä½“(Mixture-of-Agents,MoA)çš„æ–¹æ³•,åˆ©ç”¨å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹(LLM)çš„é›†ä½“æ™ºæ…§æ¥æé«˜è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚ 
> 
>   1. MoAé‡‡ç”¨äº†åˆ†å±‚ç»“æ„,æ¯ä¸€å±‚åŒ…å«å¤šä¸ªLLMæ™ºèƒ½ä½“ã€‚æ¯ä¸ªæ™ºèƒ½ä½“éƒ½å°†å‰ä¸€å±‚æ‰€æœ‰æ™ºèƒ½ä½“çš„è¾“å‡ºä½œä¸ºè¾…åŠ©ä¿¡æ¯æ¥ç”Ÿæˆè‡ªå·±çš„å›ç­”ã€‚é€šè¿‡è¿­ä»£åœ°ç»¼åˆå’Œä¼˜åŒ–å›ç­”,MoAå¯ä»¥å……åˆ†åˆ©ç”¨ä¸åŒLLMçš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚ 
> 

>   2. å®éªŒå‘ç°,å³ä½¿å…¶ä»–æ¨¡å‹æä¾›çš„è¾…åŠ©å›ç­”è´¨é‡è¾ƒä½,LLMä¹Ÿå€¾å‘äºç”Ÿæˆæ›´å¥½çš„å›ç­”,ä½“ç°å‡ºLLMå…·æœ‰å†…åœ¨çš„åä½œæ€§ã€‚MoAæ­£æ˜¯åˆ©ç”¨äº†è¿™ç§åä½œæ€§ã€‚ 
> 

>   3. åœ¨AlpacaEval 2.0ã€MT-Benchå’ŒFLASKç­‰åŸºå‡†æµ‹è¯•ä¸­,MoAå–å¾—äº†ç›®å‰æœ€ä½³çš„æ€§èƒ½,ä»…ä½¿ç”¨å¼€æºLLMå°±è¶…è¿‡äº†GPT-4ã€‚ä¾‹å¦‚åœ¨AlpacaEval 2.0ä¸Š,MoAè¾¾åˆ°äº†65.1%çš„å¾—åˆ†,è€ŒGPT-4 Omniä¸º57.5%ã€‚ 
> 

>   4. è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜,MoAå¹¶éç®€å•åœ°ä»è¾…åŠ©å›ç­”ä¸­é€‰æ‹©æœ€ä½³ç­”æ¡ˆ,è€Œæ˜¯å¯¹å®ƒä»¬è¿›è¡Œäº†å¤æ‚çš„ç»¼åˆ;ä½¿ç”¨æ›´å¤šä¸åŒçš„LLMä½œä¸ºæè®®è€…å¯ä»¥æé«˜MoAçš„æ€§èƒ½ã€‚ 
> 

>   5. é€šè¿‡é¢„ç®—åˆ†æ,MoAçš„å‡ ç§å®ç°å¯ä»¥è¾¾åˆ°ä¸GPT-4 Turboç›¸å½“çš„æ€§èƒ½,åŒæ—¶æˆæœ¬å´é™ä½äº†ä¸€åŠã€‚ 
> 

> 
> å±•ç¤ºäº†å¦‚ä½•é€šè¿‡æ··åˆæ™ºèƒ½ä½“çš„æ¡†æ¶æ¥å‘æŒ¥å¤šä¸ªLLMçš„ååŒæ•ˆåº”,åœ¨æé«˜æ€§èƒ½çš„åŒæ—¶å…¼é¡¾äº†è®¡ç®—æˆæœ¬,ä¸ºåç»­ç ”ç©¶æŒ‡æ˜äº†ä¸€ä¸ªå¾ˆæœ‰å‰æ™¯çš„æ–¹å‘ã€‚ 

###  MoA çš„æ ¸å¿ƒæ€æƒ³ 

> MoA çš„æ ¸å¿ƒæ€æƒ³æ˜¯â€œæ··åˆæ™ºèƒ½ä½“â€ï¼Œå³é€šè¿‡ç»“åˆå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„ä¼˜åŠ¿æ¥å®ç°æ›´å¼ºå¤§çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„ LLM é€šå¸¸åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œåœ¨å…¶ä»–ä»»åŠ¡ä¸Šå¯èƒ½å­˜åœ¨ä¸è¶³ã€‚MoA åˆ™é€šè¿‡å°†ä¸åŒ LLM çš„èƒ½åŠ›è¿›è¡Œæ•´åˆï¼Œä»è€Œåœ¨å„ç§ä»»åŠ¡ä¸Šéƒ½è¾¾åˆ°æ›´é«˜çš„æ°´å¹³ã€‚ 

###  å·¥ä½œåŸç† 

> **MoA çš„å·¥ä½œåŸç†**
> 
> MoA é‡‡ç”¨äº†ä¸€ç§åˆ†å±‚çš„æ¶æ„ï¼Œæ¯ä¸€å±‚éƒ½åŒ…å«å¤šä¸ª LLM ä»£ç†ã€‚è¿™äº›ä»£ç†ååŒå·¥ä½œï¼Œå…±åŒå¤„ç†è¾“å…¥å¹¶ç”Ÿæˆå“åº”ã€‚MoA çš„å·¥ä½œæµç¨‹é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š 
> 
>   1. **è¾“å…¥å¤„ç†ï¼š** å°†ç”¨æˆ·çš„è¾“å…¥å‘é€ç»™ MoA çš„ç¬¬ä¸€å±‚ã€‚ 
> 

>   2. **åˆ†å±‚å¤„ç†ï¼š** æ¯ä¸€å±‚çš„ LLM ä»£ç†éƒ½ä¼šå¯¹è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œå¹¶ç”Ÿæˆä¸­é—´ç»“æœã€‚ 
> 

>   3. **ç»“æœèšåˆï¼š** å°†æ¯ä¸€å±‚çš„ä¸­é—´ç»“æœè¿›è¡Œèšåˆï¼Œç”Ÿæˆæœ€ç»ˆçš„å“åº”ã€‚ 
> 


###  æµç¨‹ 

[ ![Image](%E6%9C%AC%E5%9C%B0%E9%85%8D%E7%BD%AE%E5%BC%80%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E4%BD%93%EF%BC%8CMoA+ollama%E6%89%93%E9%80%A0%E7%9C%9F%E6%AD%A3%E8%B6%85%E8%B6%8Agpt4o%E7%9A%84AI%20agent%209bbdb6b5be3d488487c183c4ffe44f60/Untitled.png) ](<%E6%9C%AC%E5%9C%B0%E9%85%8D%E7%BD%AE%E5%BC%80%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%E6%B7%B7%E5%90%88%E6%99%BA%E8%83%BD%E4%BD%93%EF%BC%8CMoA+ollama%E6%89%93%E9%80%A0%E7%9C%9F%E6%AD%A3%E8%B6%85%E8%B6%8Agpt4o%E7%9A%84AI%20agent%209bbdb6b5be3d488487c183c4ffe44f60/Untitled.png>)

###  MoA çš„ä¼˜åŠ¿ 

>   * **æ€§èƒ½æå‡ï¼š** é€šè¿‡ç»“åˆå¤šä¸ª LLM çš„ä¼˜åŠ¿ï¼ŒMoA åœ¨å„ç§ä»»åŠ¡ä¸Šéƒ½å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç”šè‡³åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº† GPT-4oã€‚ 
> 

>   * **çµæ´»æ€§ï¼š** MoA å¯ä»¥çµæ´»åœ°é€‰æ‹©å’Œç»„åˆä¸åŒçš„ LLMï¼Œä»è€Œé€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚ 
> 

>   * **å¯æ‰©å±•æ€§ï¼š** MoA å¯ä»¥æ–¹ä¾¿åœ°æ·»åŠ æ–°çš„ LLMï¼Œä»è€Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚ 
> 


###  **MoA çš„åº”ç”¨**

>   * **æ–‡æœ¬ç”Ÿæˆï¼š** MoA å¯ä»¥ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´æµç•…ã€æ›´å¯Œæœ‰åˆ›é€ æ€§çš„æ–‡æœ¬ã€‚ 
> 

>   * **æœºå™¨ç¿»è¯‘ï¼š** MoA å¯ä»¥å®ç°æ›´é«˜è´¨é‡çš„æœºå™¨ç¿»è¯‘ã€‚ 
> 

>   * **å¯¹è¯ç³»ç»Ÿï¼š** MoA å¯ä»¥æ„å»ºæ›´æ™ºèƒ½ã€æ›´äººæ€§åŒ–çš„å¯¹è¯ç³»ç»Ÿã€‚ 
> 


###  APIæ³¨å†Œ 

together API: [ https://api.together.ai/settings/billing ](<https://api.together.ai/settings/billing>)

groq API: [ https://console.groq.com/keys ](<https://console.groq.com/keys>)

###  ollama API 
    
    
```
        "model": "codestral:22b",
        "base_url": "http://localhost:11434/v1",
        "api_key": "ollama",
```

###  ollamaæ‹‰å–å¹¶è¿è¡Œæ¨¡å‹ 
    
    
```
    ollama run llama3:instruct
    ollama run mistral:instruct
    ollama run phi3:instruct
```

###  é…ç½®å‘½ä»¤ 
    
    
    ##ä¸‹è½½å®˜æ–¹çš„ï¼Œä¸æ”¯æŒollama
    git clone https://github.com/togethercomputer/MoA.git   
    
    cd MoA
    
    pip install -r requirements.txt
    
    cd alpaca_eval
    
    pip install -e .
    
    cd FastChat
    
    pip install -e ".[model_worker,llm_judge]"
    
    cd ..
    
    export TOGETHER_API_KEY=<API KEY>
    
    export OPENAI_API_KEY=<API KEY>
    
    ##è‡ªå®šä¹‰éƒ¨åˆ†
    export GROQ_API_KEY=<API KEY>
    
    
    python bot.py
    

###  æµ‹è¯•é—®é¢˜ 
    
    
    How can I dive deeper into machine learning?
    
    Implement a Snake game using Python.

bot.py 
    
    
```
    import datasets
    from functools import partial
    from loguru import logger
    from utils import (
        generate_together_stream,
        generate_with_references,
        DEBUG,
    )
    import typer
    from rich import print
    from rich.console import Console
    from rich.markdown import Markdown
    from rich.prompt import Prompt
    from datasets.utils.logging import disable_progress_bar
    from time import sleep
```
    
    disable_progress_bar()
    
    console = Console()
    
    welcome_message = """
    # Welcome to the Together AI MoA (Mixture-of-Agents) interactive demo!
    
    Mixture of Agents (MoA) is a novel approach that leverages the collective strengths of multiple LLMs to enhance performance, achieving state-of-the-art results. By employing a layered architecture where each layer comprises several LLM agents, MoA significantly outperforms GPT-4 Omniâ€™s 57.5% on AlpacaEval 2.0 with a score of 65.1%, using only open-source models!
    
```
    This demo uses the following LLMs as reference models, then passes the results to the aggregate model for the final response:
    - llama3-8b-8192
    - llama3-70b-8192
    - mixtral-8x7b-32768
    - gemma-7b-it
```
    
    """
    
```
    default_reference_models = [
        "llama3-8b-8192",###[modify]
        "llama3-70b-8192",###[modify]
        "mixtral-8x7b-32768",###[modify]
        "gemma-7b-it",###[modify]
    ]
```
    
    
```
    def process_fn(
        item,
        temperature=0.7,
        max_tokens=2048,
    ):
        """
        Processes a single item (e.g., a conversational turn) using specified model parameters to generate a response.
```
    
```
        Args:
            item (dict): A dictionary containing details about the conversational turn. It should include:
                         - 'references': a list of reference responses that the model may use for context.
                         - 'model': the identifier of the model to use for generating the response.
                         - 'instruction': the user's input or prompt for which the response is to be generated.
            temperature (float): Controls the randomness and creativity of the generated response. A higher temperature
                                 results in more varied outputs. Default is 0.7.
            max_tokens (int): The maximum number of tokens to generate. This restricts the length of the model's response.
                              Default is 2048.
```
    
```
        Returns:
            dict: A dictionary containing the 'output' key with the generated response as its value.
        """
```
    
```
        references = item.get("references", [])
        model = item["model"]
        messages = item["instruction"]
```
    
```
        output = generate_with_references(
            model=model,
            messages=messages,
            references=references,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        if DEBUG:
            logger.info(
                f"model: {model}, instruction: {item['instruction']}, output: {output[:20]}"
            )
```
    
        print(f"\nFinished querying [bold]{model}.[/bold]")
    
        return {"output": output}
    
    
```
    def main(
        model: str = "llama3/llama3-8b-8192",
        reference_models: list[str] = default_reference_models,
        temperature: float = 0.7,
        max_tokens: int = 512,
        rounds: int = 1,
        multi_turn=True,
    ):
        """
        Runs a continuous conversation between user and MoA.
```
    
```
        Args:
        - model (str): The primary model identifier used for generating the final response. This model aggregates the outputs from the reference models to produce the final response.
        - reference_models (List[str]): A list of model identifiers that are used as references in the initial rounds of generation. These models provide diverse perspectives and are aggregated by the primary model.
        - temperature (float): A parameter controlling the randomness of the response generation. Higher values result in more varied outputs. The default value is 0.7.
        - max_tokens (int): The maximum number of tokens that can be generated in the response. This limits the length of the output from each model per turn. Default is 2048.
        - rounds (int): The number of processing rounds to refine the responses. In each round, the input is processed through the reference models, and their outputs are aggregated. Default is 1.
        - multi_turn (bool): Enables multi-turn interaction, allowing the conversation to build context over multiple exchanges. When True, the system maintains context and builds upon previous interactions. Default is True. When False, the system generates responses independently for each input.
        """
        md = Markdown(welcome_message)
        console.print(md)
        sleep(0.75)
        console.print(
            "\n[bold]To use this demo, answer the questions below to get started [cyan](press enter to use the defaults)[/cyan][/bold]:"
        )
```
    
```
        data = {
            "instruction": [[] for _ in range(len(reference_models))],
            "references": [""] * len(reference_models),
            "model": [m for m in reference_models],
        }
```
    
        num_proc = len(reference_models)
    
```
        model = Prompt.ask(
            "\n1. What main model do you want to use?",
            default="llama3-70b-8192",###[modify]
        )
        console.print(f"Selected {model}.", style="yellow italic")
        temperature = float(
            Prompt.ask(
                "2. What temperature do you want to use? [cyan bold](0.7) [/cyan bold]",
                default=0.7,
                show_default=True,
            )
        )
        console.print(f"Selected {temperature}.", style="yellow italic")
        max_tokens = int(
            Prompt.ask(
                "3. What max tokens do you want to use? [cyan bold](512) [/cyan bold]",
                default=512,
                show_default=True,
            )
        )
        console.print(f"Selected {max_tokens}.", style="yellow italic")
```
    
        while True:
    
```
            try:
                instruction = Prompt.ask(
                    "\n[cyan bold]Prompt >>[/cyan bold] ",
                    default="Top things to do in NYC",
                    show_default=True,
                )
            except EOFError:
                break
```
    
```
            if instruction == "exit" or instruction == "quit":
                print("Goodbye!")
                break
            if multi_turn:
                for i in range(len(reference_models)):
                    data["instruction"][i].append({"role": "user", "content": instruction})
                    data["references"] = [""] * len(reference_models)
            else:
                data = {
                    "instruction": [[{"role": "user", "content": instruction}]]
                    * len(reference_models),
                    "references": [""] * len(reference_models),
                    "model": [m for m in reference_models],
                }
```
    
            eval_set = datasets.Dataset.from_dict(data)
    
```
            with console.status("[bold green]Querying all the models...") as status:
                for i_round in range(rounds):
                    eval_set = eval_set.map(
                        partial(
                            process_fn,
                            temperature=temperature,
                            max_tokens=max_tokens,
                        ),
                        batched=False,
                        num_proc=num_proc,
                    )
                    references = [item["output"] for item in eval_set]
                    data["references"] = references
                    eval_set = datasets.Dataset.from_dict(data)
```
    
```
            console.print(
                "[cyan bold]Aggregating results & querying the aggregate model...[/cyan bold]"
            )
            output = generate_with_references(
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                messages=data["instruction"][0],
                references=references,
                generate_fn=generate_together_stream,
            )
```
    
```
            all_output = ""
            print("\n")
            console.log(Markdown(f"## Final answer from {model}"))
```
    
```
            for chunk in output:
                out = chunk.choices[0].delta.content
                if out is not None:
                    console.print(out, end="")
                    all_output += out
            print()
```
    
```
            if DEBUG:
                logger.info(
                    f"model: {model}, instruction: {data['instruction'][0]}, output: {all_output[:20]}"
                )
            if multi_turn:
                for i in range(len(reference_models)):
                    data["instruction"][i].append(
                        {"role": "assistant", "content": all_output}
                    )
```
    
    
    if __name__ == "__main__":
        typer.run(main)

utils.py 
    
    
```
    import os
    import json
    import time
    import requests
    import openai
    import copy
```
    
    from loguru import logger
    
    
    DEBUG = int(os.environ.get("DEBUG", "0"))
    
    
```
    def generate_together(
        model,
        messages,
        max_tokens=2048,
        temperature=0.7,
        streaming=False,
    ):
```
    
        output = None
    
        for sleep_time in [1, 2, 4, 8, 16, 32]:
    
            try:
    
                endpoint = "https://api.groq.com/openai/v1/chat/completions"###[modify]
    
```
                if DEBUG:
                    logger.debug(
                        f"Sending messages ({len(messages)}) (last message: `{messages[-1]['content'][:20]}...`) to `{model}`."
                    )
```
    
```
                res = requests.post(
                    endpoint,
                    json={
                        "model": model,
                        "max_tokens": max_tokens,
                        "temperature": (temperature if temperature > 1e-4 else 0),
                        "messages": messages,
                    },
                    headers={
                        "Authorization": f"Bearer {os.environ.get('TOGETHER_API_KEY')}",
                    },
                )
                if "error" in res.json():
                    logger.error(res.json())
                    if res.json()["error"]["type"] == "invalid_request_error":
                        logger.info("Input + output is longer than max_position_id.")
                        return None
```
    
                output = res.json()["choices"][0]["message"]["content"]
    
                break
    
```
            except Exception as e:
                logger.error(e)
                if DEBUG:
                    logger.debug(f"Msgs: `{messages}`")
```
    
                logger.info(f"Retry in {sleep_time}s..")
                time.sleep(sleep_time)
    
        if output is None:
    
            return output
    
        output = output.strip()
    
        if DEBUG:
            logger.debug(f"Output: `{output[:20]}...`.")
    
        return output
    
    
```
    def generate_together_stream(
        model,
        messages,
        max_tokens=2048,
        temperature=0.7,
    ):
        endpoint = "https://api.groq.com/openai/v1"###[modify]
        client = openai.OpenAI(
            api_key=os.environ.get("GROQ_API_KEY"), base_url=endpoint ###[modify]
        )
        endpoint = "https://api.groq.com/openai/chat/completions" ###[modify]
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature if temperature > 1e-4 else 0,
            max_tokens=max_tokens,
            stream=True,  # this time, we set stream=True
        )
```
    
        return response
    
    
```
    def generate_openai(
        model,
        messages,
        max_tokens=2048,
        temperature=0.7,
    ):
```
    
```
        client = openai.OpenAI(
            api_key=os.environ.get("OPENAI_API_KEY"),
        )
```
    
        for sleep_time in [1, 2, 4, 8, 16, 32]:
            try:
    
```
                if DEBUG:
                    logger.debug(
                        f"Sending messages ({len(messages)}) (last message: `{messages[-1]['content'][:20]}`) to `{model}`."
                    )
```
    
```
                completion = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                output = completion.choices[0].message.content
                break
```
    
```
            except Exception as e:
                logger.error(e)
                logger.info(f"Retry in {sleep_time}s..")
                time.sleep(sleep_time)
```
    
        output = output.strip()
    
        return output
    
    
```
    def inject_references_to_messages(
        messages,
        references,
    ):
```
    
        messages = copy.deepcopy(messages)
    
        system = f"""You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.
    
    Responses from models:"""
    
        for i, reference in enumerate(references):
    
            system += f"\n{i+1}. {reference}"
    
        if messages[0]["role"] == "system":
    
            messages[0]["content"] += "\n\n" + system
    
        else:
    
            messages = [{"role": "system", "content": system}] + messages
    
        return messages
    
    
```
    def generate_with_references(
        model,
        messages,
        references=[],
        max_tokens=2048,
        temperature=0.7,
        generate_fn=generate_together,
    ):
```
    
        if len(references) > 0:
    
            messages = inject_references_to_messages(messages, references)
    
```
        return generate_fn(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
```
    
