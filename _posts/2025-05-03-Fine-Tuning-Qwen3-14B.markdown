---
layout: single
title: "ğŸš€unslothå¾®è°ƒQwen3å¤§æ¨¡å‹ä¿å§†çº§è§†é¢‘æ•™ç¨‹ï¼ä»æ•°æ®å¤„ç†åˆ°LoRAå¾®è°ƒQwen3-14Båˆ°4æ¯”ç‰¹é‡åŒ–å¹¶ä¸”ç”¨LM Studioè¿è¡Œï¼é›¶ä»£ç åŸºç¡€ä¹Ÿèƒ½å®Œæˆçš„LoRAé«˜æ•ˆå¾®è°ƒå…¨è¿‡ç¨‹è¯¦è§£ï¼å°ç™½ä¹Ÿèƒ½è½»æ¾å…¥é—¨"
sidebar:
  nav: "docs"
date: 2025-05-03 00:00:00 +0800
categories: LLMs
tags: [Qwen3, Fine-Tuning, å¾®è°ƒ, å¾®è°ƒQwen3, Qwen3-14B, AI, AIGC]
classes: wide
author_profile: true
---

unslothå¾®è°ƒQwen3æ¨¡å‹æä¾›æ˜¾è‘—ä¼˜åŠ¿ï¼šè®­ç»ƒé€Ÿåº¦æé«˜2å€ï¼ŒVRAMä½¿ç”¨å‡å°‘70%ï¼Œæ”¯æŒ8å€é•¿çš„ä¸Šä¸‹æ–‡ã€‚Qwen3-30B-A3Bä»…éœ€17.5GB VRAMå³å¯è¿è¡Œã€‚unslothçš„Dynamic 2.0é‡åŒ–æŠ€æœ¯ä¿è¯äº†é«˜ç²¾åº¦ï¼ŒåŒæ—¶æ”¯æŒåŸç”Ÿ128Kä¸Šä¸‹æ–‡é•¿åº¦ã€‚Qwen3æ¨¡å‹å…·æœ‰æ€è€ƒæ¨¡å¼å’Œéæ€è€ƒæ¨¡å¼ï¼Œé€‚ç”¨äºä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡ã€‚å¾®è°ƒåçš„æ¨¡å‹å¯ç”¨äºæ³•å¾‹æ–‡æ¡£åˆ†æã€å®šåˆ¶çŸ¥è¯†åº“æ„å»ºç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿå¤„ç†ç‰¹å®šé¢†åŸŸæŸ¥è¯¢å¹¶ä¿æŒä¸Šä¸‹æ–‡ï¼Œä¼˜äºçº¯æ£€ç´¢ç³»ç»Ÿã€‚unslothæ”¯æŒ4bit/16bitçš„QLoRA/LoRAå¾®è°ƒï¼Œé€‚ç”¨äº2018å¹´åçš„NVIDIA GPUï¼Œä¸ºèµ„æºæœ‰é™ç¯å¢ƒä¸‹çš„æ¨¡å‹å®šåˆ¶æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚

### ğŸš€æœ¬ç¯‡ç¬”è®°æ‰€å¯¹åº”çš„è§†é¢‘ï¼š

- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡å“”å“©å“”å“©è§‚çœ‹](https://www.bilibili.com/video/BV17jGdzAETg/)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡YouTubeè§‚çœ‹](https://youtu.be/opk1f-XtIsw)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¼€æºé¡¹ç›®](https://github.com/win4r/AISuperDomain)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ è¯·æˆ‘å–å’–å•¡](https://ko-fi.com/aila)
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¾®ä¿¡ï¼šstoeng
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æ‰¿æ¥å¤§æ¨¡å‹å¾®è°ƒã€RAGã€AIæ™ºèƒ½ä½“ã€AIç›¸å…³åº”ç”¨å¼€å‘ç­‰é¡¹ç›®ã€‚

### ğŸ”¥AIæ™ºèƒ½ä½“ç›¸å…³è§†é¢‘

1. [AIæ™ºèƒ½ä½“è§†é¢‘ 1](https://youtu.be/vYm0brFoMwA) 
2. [AIæ™ºèƒ½ä½“è§†é¢‘ 2](https://youtu.be/szTXELuaJos)  
3. [AIæ™ºèƒ½ä½“è§†é¢‘ 3](https://youtu.be/szTXELuaJos)  
4. [AIæ™ºèƒ½ä½“è§†é¢‘ 4](https://youtu.be/RxR3x_Uyq4c)  
5. [AIæ™ºèƒ½ä½“è§†é¢‘ 5](https://youtu.be/IrTEDPnEVvU)  

## Qwen3æ¨¡å‹å¾®è°ƒçš„ä¸»è¦åœºæ™¯

unslothæ”¯æŒå¯¹Qwen3æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥åº”ç”¨äºä»¥ä¸‹åœºæ™¯ï¼š

1. æ³•å¾‹æ–‡æ¡£è¾…åŠ© - åœ¨æ³•å¾‹æ–‡æœ¬ï¼ˆåˆåŒã€æ¡ˆä¾‹æ³•ã€æ³•è§„ï¼‰ä¸Šè¿›è¡Œå¾®è°ƒï¼Œç”¨äºåˆåŒåˆ†æã€æ¡ˆä¾‹æ³•ç ”ç©¶æˆ–åˆè§„æ”¯æŒ
2. å®šåˆ¶çŸ¥è¯†åº“ - å°†ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†ç›´æ¥åµŒå…¥åˆ°æ¨¡å‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†ç‰¹å®šé¢†åŸŸçš„æŸ¥è¯¢å’Œæ–‡æ¡£æ€»ç»“

Qwen3æ¨¡å‹æœ¬èº«å…·æœ‰ä¸¤ç§å·¥ä½œæ¨¡å¼ï¼Œä½¿å¾®è°ƒåçš„æ¨¡å‹æ›´åŠ çµæ´»ï¼š

1. æ€è€ƒæ¨¡å¼(Thinking Mode)ï¼šæ¨¡å‹ä¼šåœ¨ç»™å‡ºæœ€ç»ˆç­”æ¡ˆå‰è¿›è¡Œé€æ­¥æ¨ç†ï¼Œé€‚åˆéœ€è¦æ·±åº¦æ€è€ƒçš„å¤æ‚é—®é¢˜
2. éæ€è€ƒæ¨¡å¼(Non-Thinking Mode)ï¼šæ¨¡å‹æä¾›å¿«é€Ÿã€è¿‘ä¹å³æ—¶çš„å›ç­”ï¼Œé€‚åˆç®€å•é—®é¢˜

## ä½¿ç”¨unslothå¾®è°ƒQwen3çš„ä¸»è¦ä¼˜åŠ¿

unslothä½¿Qwen3(8B)å¾®è°ƒé€Ÿåº¦æé«˜2å€ï¼ŒVRAMä½¿ç”¨å‡å°‘70%ï¼Œå¹¶ä¸”æ¯”æ‰€æœ‰ä½¿ç”¨Flash Attention 2çš„ç¯å¢ƒæ”¯æŒé•¿8å€çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚ä½¿ç”¨unslothï¼ŒQwen3-30B-A3Bæ¨¡å‹å¯ä»¥èˆ’é€‚åœ°åœ¨ä»…17.5GB VRAMçš„ç¯å¢ƒä¸­è¿è¡Œã€‚

unslothä¸ºQwen3æä¾›äº†Dynamic 2.0é‡åŒ–æ–¹æ³•ï¼Œåœ¨5-shot MMLUå’ŒKLæ•£åº¦åŸºå‡†æµ‹è¯•ä¸­æä¾›æœ€ä½³æ€§èƒ½ã€‚è¿™æ„å‘³ç€å¯ä»¥è¿è¡Œå’Œå¾®è°ƒé‡åŒ–åçš„Qwen3 LLMï¼ŒåŒæ—¶ä¿æŒæœ€å°çš„ç²¾åº¦æŸå¤±ã€‚unslothè¿˜ä¸Šä¼ äº†æ”¯æŒåŸç”Ÿ128Kä¸Šä¸‹æ–‡é•¿åº¦çš„Qwen3ç‰ˆæœ¬ã€‚

unslothæ”¯æŒå¤šç§å¾®è°ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬4bitå’Œ16bitçš„QLoRA/LoRAå¾®è°ƒã€‚å®ƒé€šè¿‡æ‰‹åŠ¨æ¨å¯¼æ‰€æœ‰è®¡ç®—å¯†é›†å‹æ•°å­¦æ­¥éª¤å¹¶æ‰‹å†™GPUæ ¸å¿ƒï¼Œåœ¨ä¸æ›´æ”¹ç¡¬ä»¶çš„æƒ…å†µä¸‹ä½¿è®­ç»ƒé€Ÿåº¦æ›´å¿«ã€‚

## æŠ€æœ¯ç‰¹ç‚¹ä¸æ”¯æŒ

unslothæä¾›äº†å¤šç§è®¾ç½®é€‰é¡¹æ¥ä¼˜åŒ–å¾®è°ƒè¿‡ç¨‹ï¼š

- max_seq_length = 2048ï¼šæ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ã€‚è™½ç„¶Qwen3æ”¯æŒ40960ï¼Œä½†å»ºè®®æµ‹è¯•æ—¶ä½¿ç”¨2048ã€‚unslothèƒ½å¤Ÿå®ç°8å€é•¿çš„ä¸Šä¸‹æ–‡å¾®è°ƒ
- load_in_4bit = Trueï¼šå¯ç”¨4ä½é‡åŒ–ï¼Œå‡å°‘å¾®è°ƒæ—¶å†…å­˜ä½¿ç”¨é‡è‡³åŸæ¥çš„1/4ï¼Œé€‚ç”¨äº16GB GPU

unslothä¸Šä¼ äº†æ‰€æœ‰ç‰ˆæœ¬çš„Qwen3ï¼ŒåŒ…æ‹¬Dynamic 2.0 GGUFã€åŠ¨æ€4ä½ç­‰æ ¼å¼åˆ°Hugging Faceã€‚æ­¤å¤–ï¼Œunslothè¿˜æ”¯æŒåŒ…æ‹¬30B-A3Bå’Œ235B-A22Båœ¨å†…çš„Qwen3 MOEæ¨¡å‹ã€‚

unslothçš„æŠ€æœ¯æ”¯æŒåŒ…æ‹¬ï¼š

- æ”¯æŒ2018å¹´ä»¥åçš„NVIDIA GPUï¼Œæœ€ä½CUDAèƒ½åŠ›è¦æ±‚7.0
- æ”¯æŒå„ç§Transformeré£æ ¼çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬Phi-4æ¨ç†ã€Mixtralã€MOEã€Cohereç­‰
- æ”¯æŒä»»ä½•è®­ç»ƒç®—æ³•ï¼Œæ¯”å¦‚å¸¦VLMçš„GRPO

## å®é™…åº”ç”¨ä¼˜åŠ¿

ä¸çº¯æ£€ç´¢ç³»ç»Ÿç›¸æ¯”ï¼Œå¾®è°ƒæä¾›äº†å‡ ä¸ªæ˜¾è‘—ä¼˜åŠ¿ï¼š

1. å¾®è°ƒå‡ ä¹å¯ä»¥åšåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)èƒ½åšçš„ä¸€åˆ‡ï¼Œä½†åä¹‹åˆ™ä¸ç„¶
2. åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå¤–éƒ¨çŸ¥è¯†ç›´æ¥åµŒå…¥åˆ°æ¨¡å‹ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†ç‰¹å®šé¢†åŸŸæŸ¥è¯¢å¹¶åœ¨ä¸ä¾èµ–å¤–éƒ¨æ£€ç´¢ç³»ç»Ÿçš„æƒ…å†µä¸‹ä¿æŒä¸Šä¸‹æ–‡
3. å³ä½¿åœ¨åŒæ—¶ä½¿ç”¨å¾®è°ƒå’ŒRAGçš„æ··åˆè®¾ç½®ä¸­ï¼Œå¾®è°ƒåçš„æ¨¡å‹ä¹Ÿæä¾›äº†å¯é çš„åå¤‡æ–¹æ¡ˆ

åœ¨ç‰¹å®šé¢†åŸŸï¼Œå¦‚åŒ»ç–—ä¿å¥é¢†åŸŸçš„è§†è§‰é—®ç­”(VQA)ä»»åŠ¡ä¸­ï¼Œå¾®è°ƒä½¿æ¨¡å‹æ›´å¥½åœ°ç†è§£é¢†åŸŸç‰¹å®šçš„ç»†å¾®å·®åˆ«ï¼Œæé«˜å…¶æä¾›å‡†ç¡®å’Œä¸Šä¸‹æ–‡ç›¸å…³å“åº”çš„èƒ½åŠ›ã€‚å¾®è°ƒåçš„æ¨¡å‹åœ¨ç²¾ç¡®åº¦å’Œå¬å›ç‡ä¸Šè¡¨ç°æ˜æ˜¾ä¼˜äºé›¶æ ·æœ¬é¢„æµ‹ã€‚

ä¸ºè·å¾—æœ€ä½³ç»“æœï¼Œå»ºè®®ç­–åˆ’ç»“æ„è‰¯å¥½çš„æ•°æ®é›†ï¼Œç†æƒ³æƒ…å†µä¸‹æ˜¯é—®ç­”å¯¹å½¢å¼ã€‚è¿™å¯ä»¥å¢å¼ºå­¦ä¹ ã€ç†è§£å’Œå“åº”å‡†ç¡®æ€§ã€‚

ä½¿ç”¨unslothå¾®è°ƒQwen3æ¨¡å‹å¯ä»¥å®ç°æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€æ›´ä½çš„å†…å­˜éœ€æ±‚å’Œæ›´é•¿çš„ä¸Šä¸‹æ–‡æ”¯æŒï¼ŒåŒæ—¶ä¿æŒé«˜ç²¾åº¦ã€‚è¿™ä½¿å¾—å³ä½¿åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ï¼Œä¹Ÿèƒ½å¤Ÿå°†å¼ºå¤§çš„Qwen3æ¨¡å‹é€‚é…åˆ°ç‰¹å®šé¢†åŸŸçš„åº”ç”¨åœºæ™¯ä¸­ã€‚

### ğŸ”¥ğŸ”¥ğŸ”¥å®Œæ•´å¾®è°ƒä»£ç 

```python

**å¾®è°ƒåçš„æ¨¡å‹è·å¾—çš„èƒ½åŠ›:**

1. åŒæ¨¡å¼æ“ä½œèƒ½åŠ›:

 - æ™®é€šå¯¹è¯æ¨¡å¼: é€‚ç”¨äºæ—¥å¸¸èŠå¤©åœºæ™¯
 - æ€è€ƒæ¨¡å¼(Thinking Mode): ç”¨äºè§£å†³éœ€è¦æ¨ç†çš„é—®é¢˜

2. æ•°å­¦æ¨ç†èƒ½åŠ›: èƒ½å¤Ÿè§£å†³æ•°å­¦é—®é¢˜å¹¶å±•ç¤ºè¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ï¼Œå¦‚ç¤ºä¾‹ä¸­çš„"è§£æ–¹ç¨‹(x + 2)^2 = 0"
3. å¯¹è¯èƒ½åŠ›ä¿æŒ: åŒæ—¶ä¿æŒäº†è‡ªç„¶å¯¹è¯çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¿›è¡Œæµç•…çš„å¤šè½®å¯¹è¯

å¾®è°ƒä½¿æ¨¡å‹æˆä¸ºä¸€ä¸ª"åŒé‡äººæ ¼"çš„åŠ©æ‰‹ï¼Œæ—¢èƒ½è¿›è¡Œæ™®é€šé—²èŠï¼Œåˆèƒ½åœ¨éœ€è¦æ—¶åˆ‡æ¢åˆ°æ›´ä¸¥è°¨çš„æ€è€ƒæ¨¡å¼æ¥è§£å†³å¤æ‚é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ•°å­¦é—®é¢˜ã€‚

### å®‰è£…
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     # å¦‚æœä¸æ˜¯åœ¨Google Colabç¯å¢ƒä¸­è¿è¡Œï¼Œåˆ™ç®€å•å®‰è£…unslothåº“
#     !pip install unsloth
# else:
#     # åœ¨Google Colabç¯å¢ƒä¸­è¿è¡Œæ—¶çš„ç‰¹æ®Šå®‰è£…æµç¨‹
#     # é¦–å…ˆå®‰è£…æ‰€æœ‰ä¾èµ–åº“ï¼Œä½†ä¸å¤„ç†å®ƒä»¬çš„ä¾èµ–å…³ç³»(--no-depså‚æ•°)
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo
#     # å®‰è£…å¸¸ç”¨çš„è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ¨¡å‹æ‰˜ç®¡å·¥å…·
#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
#     # æœ€åå®‰è£…unslothåº“æœ¬èº«ï¼Œä¸å¤„ç†ä¾èµ–(é¿å…ç‰ˆæœ¬å†²çª)
#     !pip install --no-deps unsloth
#

"""### Unsloth"""

from unsloth import FastLanguageModel
import torch

fourbit_models = [
    "unsloth/Qwen3-1.7B-unsloth-bnb-4bit", # Qwen 14B 2x faster
    "unsloth/Qwen3-4B-unsloth-bnb-4bit",
    "unsloth/Qwen3-8B-unsloth-bnb-4bit",
    "unsloth/Qwen3-14B-unsloth-bnb-4bit",
    "unsloth/Qwen3-32B-unsloth-bnb-4bit",

    # 4bit dynamic quants for superior accuracy and low memory use
    "unsloth/gemma-3-12b-it-unsloth-bnb-4bit",
    "unsloth/Phi-4",
    "unsloth/Llama-3.1-8B",
    "unsloth/Llama-3.2-3B",
    "unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit" # [NEW] We support TTS models!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen3-14B",
    max_seq_length = 2048,   # Context length - can be longer, but uses more memory
    load_in_4bit = True,     # 4bit uses much less memory
    load_in_8bit = False,    # A bit more accurate, uses 2x memory
    full_finetuning = False, # We have full finetuning now!
    token = "",      # use one if using gated models
)

"""We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"""

# æ·»åŠ LoRAé€‚é…å™¨
# é€šè¿‡LoRAæŠ€æœ¯ï¼Œåªéœ€è¦æ›´æ–°1-10%çš„å‚æ•°å³å¯å®ç°æœ‰æ•ˆå¾®è°ƒ
model = FastLanguageModel.get_peft_model(
    model,
    r = 32,           # # LoRAç§©ï¼Œå»ºè®®å€¼ä¸º8,16,32,64,128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 32,  # LoRA alphaå€¼ï¼Œå»ºè®®è®¾ä¸ºrankæˆ–rank*2
    lora_dropout = 0, # LoRA dropoutï¼Œ0å€¼ç»è¿‡ä¼˜åŒ–
    bias = "none",    # åç½®è®¾ç½®ï¼Œ"none"å·²ä¼˜åŒ–
    # [æ–°ç‰¹æ€§] "unsloth"æ¨¡å¼å‡å°‘30%æ˜¾å­˜ï¼Œå¯é€‚åº”2å€å¤§çš„æ‰¹æ¬¡å¤§å°
    use_gradient_checkpointing = "unsloth", #æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œç”¨äºé•¿ä¸Šä¸‹æ–‡
    random_state = 3407,  # éšæœºç§å­
    use_rslora = False,   # æ˜¯å¦ä½¿ç”¨rank stabilized LoRA
    loftq_config = None,  # LoftQé…ç½®
)

"""<a name="Data"></a>
### Data Prep
Qwen3 has both reasoning and a non reasoning mode. So, we should use 2 datasets:

1. We use the [Open Math Reasoning]() dataset which was used to win the [AIMO](https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/leaderboard) (AI Mathematical Olympiad - Progress Prize 2) challenge! We sample 10% of verifiable reasoning traces that used DeepSeek R1, and whicht got > 95% accuracy.

2. We also leverage [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we need to convert it to HuggingFace's normal multiturn format as well.
"""

# æ•°æ®å‡†å¤‡
# Qwen3åŒæ—¶å…·æœ‰æ¨ç†å’Œéæ¨ç†æ¨¡å¼ï¼Œå› æ­¤ä½¿ç”¨ä¸¤ç§æ•°æ®é›†ï¼š
# 1. OpenMathReasoningæ•°æ®é›† - ç”¨äºæ•°å­¦æ¨ç†èƒ½åŠ›
# 2. FineTome-100kæ•°æ®é›† - ç”¨äºä¸€èˆ¬å¯¹è¯èƒ½åŠ›
from datasets import load_dataset
# åŠ è½½æ•°å­¦æ¨ç†æ•°æ®é›†
reasoning_dataset = load_dataset("unsloth/OpenMathReasoning-mini", split = "cot",token="")
# åŠ è½½å¯¹è¯æ•°æ®é›†
non_reasoning_dataset = load_dataset("mlabonne/FineTome-100k", split = "train",token="")

"""Let's see the structure of both datasets:"""

# æŸ¥çœ‹æ¨ç†æ•°æ®é›†ç»“æ„
reasoning_dataset

# æŸ¥çœ‹éæ¨ç†æ•°æ®é›†ç»“æ„
non_reasoning_dataset

"""We now convert the reasoning dataset into conversational format:"""

# å°†æ¨ç†æ•°æ®é›†è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
# å°†æ•°å­¦é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆè½¬æ¢ä¸ºç”¨æˆ·-åŠ©æ‰‹å¯¹è¯æ ¼å¼
# å‚æ•°:
#     examples: æ‰¹é‡æ ·æœ¬ï¼ŒåŒ…å«é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
# è¿”å›:
#     åŒ…å«å¯¹è¯æ ¼å¼çš„å­—å…¸

def generate_conversation(examples):
    problems  = examples["problem"]
    solutions = examples["generated_solution"]
    conversations = []
    for problem, solution in zip(problems, solutions):
        conversations.append([
            {"role" : "user",      "content" : problem},
            {"role" : "assistant", "content" : solution},
        ])
    return { "conversations": conversations, }

# å°†è½¬æ¢åçš„æ¨ç†æ•°æ®é›†åº”ç”¨å¯¹è¯æ¨¡æ¿
reasoning_conversations = tokenizer.apply_chat_template(
    reasoning_dataset.map(generate_conversation, batched = True)["conversations"],
    tokenize = False, # ä¸è¿›è¡Œåˆ†è¯ï¼Œä»…åº”ç”¨æ¨¡æ¿
)

"""Let's see the first transformed row:"""

# æŸ¥çœ‹è½¬æ¢åçš„ç¬¬ä¸€ä¸ªæ ·æœ¬
reasoning_conversations[0]

"""Next we take the non reasoning dataset and convert it to conversational format as well.

We have to use Unsloth's `standardize_sharegpt` function to fix up the format of the dataset first.
"""

# å¤„ç†éæ¨ç†æ•°æ®é›†ï¼Œè½¬æ¢ä¸ºæ ‡å‡†å¯¹è¯æ ¼å¼
from unsloth.chat_templates import standardize_sharegpt
dataset = standardize_sharegpt(non_reasoning_dataset)

# å°†æ ‡å‡†åŒ–åçš„éæ¨ç†æ•°æ®é›†åº”ç”¨å¯¹è¯æ¨¡æ¿
non_reasoning_conversations = tokenizer.apply_chat_template(
    dataset["conversations"],
    tokenize = False,
)

"""Let's see the first row"""

# æŸ¥çœ‹è½¬æ¢åçš„ç¬¬ä¸€ä¸ªéæ¨ç†æ ·æœ¬
non_reasoning_conversations[0]

"""Now let's see how long both datasets are:"""

# æŸ¥çœ‹ä¸¤ä¸ªæ•°æ®é›†çš„å¤§å°
print(len(reasoning_conversations))
print(len(non_reasoning_conversations))

"""The non reasoning dataset is much longer. Let's assume we want the model to retain some reasoning capabilities, but we specifically want a chat model.

Let's define a ratio of chat only data. The goal is to define some mixture of both sets of data.

Let's select 25% reasoning and 75% chat based:
"""

# è®¾ç½®èŠå¤©æ•°æ®æ¯”ä¾‹
# è®©æ¨¡å‹ä¿æŒ25%çš„æ¨ç†èƒ½åŠ›å’Œ75%çš„èŠå¤©èƒ½åŠ›
chat_percentage = 0.75

"""Let's sample the reasoning dataset by 25% (or whatever is 100% - chat_percentage)"""

# ä»éæ¨ç†æ•°æ®é›†ä¸­æŠ½æ ·ï¼ŒæŠ½å–æ•°é‡ä¸ºæ¨ç†æ•°æ®é›†çš„25%
import pandas as pd
non_reasoning_subset = pd.Series(non_reasoning_conversations)
non_reasoning_subset = non_reasoning_subset.sample(
    int(len(reasoning_conversations) * (1.0 - chat_percentage)),# é‡‡æ ·å¤§å°ï¼šæ¨ç†æ•°æ®é›†å¤§å°çš„25%
    random_state = 2407,
)

"""Finally combine both datasets:"""

# åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†
data = pd.concat([
    pd.Series(reasoning_conversations),    # æ¨ç†å¯¹è¯æ•°æ®
    pd.Series(non_reasoning_subset)        # é‡‡æ ·åçš„éæ¨ç†å¯¹è¯æ•°æ®
])
data.name = "text"  # è®¾ç½®æ•°æ®åˆ—åä¸º"text"

# å°†åˆå¹¶çš„æ•°æ®è½¬æ¢ä¸ºHuggingFace Datasetæ ¼å¼
from datasets import Dataset
combined_dataset = Dataset.from_pandas(pd.DataFrame(data))
# éšæœºæ‰“ä¹±æ•°æ®é›†
combined_dataset = combined_dataset.shuffle(seed = 3407)

# æŸ¥çœ‹æ•°æ®é›†çš„åŸºæœ¬ä¿¡æ¯
print(combined_dataset)

# ä½¿ç”¨DataFrameå±•ç¤ºå‰10æ¡è®°å½•
import pandas as pd

# è½¬æ¢ä¸ºpandas DataFrameä»¥ä¾¿æ›´å¥½åœ°æ˜¾ç¤º
df = pd.DataFrame(combined_dataset[:10])
display(df)

"""<a name="Train"></a>
### Train the model
Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.
"""

# ä½¿ç”¨HuggingFace TRLçš„SFTTrainerè¿›è¡Œè®­ç»ƒ
from trl import SFTTrainer, SFTConfig
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = combined_dataset,
    eval_dataset = None,  # å¯ä»¥è®¾ç½®è¯„ä¼°æ•°æ®é›†
    args = SFTConfig(
        dataset_text_field = "text",  # æŒ‡å®šæ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µ
        per_device_train_batch_size = 2,  # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°
        gradient_accumulation_steps = 4,  # ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡å¤§å°
        warmup_steps = 5,  # é¢„çƒ­æ­¥æ•°
        # num_train_epochs = 1,  # è®¾ç½®ä¸º1ä»¥è¿›è¡Œå®Œæ•´è®­ç»ƒ
        max_steps = 30,
        learning_rate = 2e-4,   # å­¦ä¹ ç‡ï¼ˆé•¿æœŸè®­ç»ƒå¯é™è‡³2e-5ï¼‰
        logging_steps = 1,  # æ—¥å¿—è®°å½•é—´éš”
        optim = "adamw_8bit",  # ä¼˜åŒ–å™¨
        weight_decay = 0.01,  # æƒé‡è¡°å‡
        lr_scheduler_type = "linear",  # å­¦ä¹ ç‡è°ƒåº¦ç±»å‹
        seed = 3407,  # éšæœºç§å­
        report_to = "none",   # å¯è®¾ç½®ä¸º"wandb"ç­‰è¿›è¡Œå®éªŒè¿½è¸ª
    ),
)

# æ˜¾ç¤ºå½“å‰å†…å­˜ç»Ÿè®¡
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

"""Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"""

# å¼€å§‹è®­ç»ƒæ¨¡å‹
# è¦æ¢å¤è®­ç»ƒï¼Œå¯è®¾ç½® resume_from_checkpoint = True
trainer_stats = trainer.train()

# æ˜¾ç¤ºæœ€ç»ˆå†…å­˜å’Œæ—¶é—´ç»Ÿè®¡
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)
lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(
    f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training."
)
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

"""<a name="Inference"></a>
### Inference
Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`

For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`
"""

# æ¨¡å‹æ¨ç†
# ä½¿ç”¨UnslothåŸç”Ÿæ¨ç†åŠŸèƒ½æµ‹è¯•æ¨¡å‹
# æ ¹æ®Qwen-3å›¢é˜Ÿå»ºè®®ï¼š
# - æ¨ç†æ¨¡å¼ï¼štemperature=0.6, top_p=0.95, top_k=20
# - æ™®é€šèŠå¤©æ¨¡å¼ï¼štemperature=0.7, top_p=0.8, top_k=20

# æµ‹è¯•æ²¡æœ‰å¯ç”¨thinkingæ¨¡å¼çš„æ™®é€šå¯¹è¯
messages = [
    {"role" : "user", "content" : "Solve (x + 2)^2 = 0."}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize = False,
    add_generation_prompt = True, # å¿…é¡»æ·»åŠ ç”Ÿæˆæç¤º
    enable_thinking = False,  # ç¦ç”¨thinkingæ¨¡å¼
)

# ä½¿ç”¨æ™®é€šå¯¹è¯å‚æ•°è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
from transformers import TextStreamer
_ = model.generate(
    **tokenizer(text, return_tensors = "pt").to("cuda"),
    max_new_tokens = 256, # å¢åŠ ä»¥è·å¾—æ›´é•¿è¾“å‡º
    temperature = 0.7, top_p = 0.8, top_k = 20, # æ™®é€šå¯¹è¯æ¨¡å¼å‚æ•°
    streamer = TextStreamer(tokenizer, skip_prompt = True),
)

# æµ‹è¯•å¯ç”¨thinkingæ¨¡å¼çš„æ¨ç†å¯¹è¯
messages = [
    {"role" : "user", "content" : "Solve (x + 2)^2 = 0."}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize = False,
    add_generation_prompt = True,  # å¿…é¡»æ·»åŠ ç”Ÿæˆæç¤º
    enable_thinking = True, # å¯ç”¨thinkingæ¨¡å¼
)

# ä½¿ç”¨æ¨ç†æ¨¡å¼å‚æ•°è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
from transformers import TextStreamer
_ = model.generate(
    **tokenizer(text, return_tensors = "pt").to("cuda"),
    max_new_tokens = 1024,  # å¢åŠ ä»¥è·å¾—æ›´é•¿è¾“å‡º
    temperature = 0.6, top_p = 0.95, top_k = 20, # æ¨ç†æ¨¡å¼å‚æ•°
    streamer = TextStreamer(tokenizer, skip_prompt = True),
)

"""<a name="Save"></a>
### Saving, loading finetuned models
To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.

**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!
"""

# æ¨¡å‹ä¿å­˜
# ä»¥ä¸‹æ˜¯å¤šç§ä¿å­˜æ¨¡å‹çš„æ–¹å¼

# ä¿å­˜LoRAé€‚é…å™¨ï¼ˆä¸åŒ…å«å®Œæ•´æ¨¡å‹ï¼Œä½“ç§¯å°ï¼‰
model.save_pretrained("lora_model")  # Local saving
tokenizer.save_pretrained("lora_model")
# model.push_to_hub("leo009/Qwen3-lora_model", token = "") # ä¸Šä¼ åˆ°HuggingFace Hub
# tokenizer.push_to_hub("leo009/Qwen3-lora_model", token = "") # ä¸Šä¼ åˆ°HuggingFace Hub

"""Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"""

# åŠ è½½åˆšåˆšä¿å­˜çš„LoRAé€‚é…å™¨ï¼ˆç”¨äºæ¨ç†ï¼‰
if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model",  # è®­ç»ƒæ—¶ä½¿ç”¨çš„æ¨¡å‹
        max_seq_length = 2048,
        load_in_4bit = True,
    )

"""### Saving to float16 for VLLM

We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.
"""

# ä¿å­˜ä¸ºfloat16æ ¼å¼ï¼ˆç”¨äºVLLMï¼‰
# æ”¯æŒå¤šç§ä¿å­˜æ–¹å¼ï¼šmerged_16bitï¼ˆfloat16ï¼‰ã€merged_4bitï¼ˆint4ï¼‰æˆ–loraï¼ˆé€‚é…å™¨ï¼‰

# Merge to 16bit
if False:
    model.save_pretrained_merged("model", tokenizer, save_method = "merged_16bit",)
if False: # ä¸Šä¼ åˆ°HuggingFace Hub
    model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_16bit", token = "")

# ä¿å­˜ä¸º4ä½ç²¾åº¦
if True:
    model.save_pretrained_merged("model", tokenizer, save_method = "merged_4bit_forced",) # æ”¹ä¸º_forcedç‰ˆæœ¬
if True: # ä¸Šä¼ åˆ°HuggingFace Hub
    model.push_to_hub_merged("leo009/Qwen3-vLLM", tokenizer, save_method = "merged_4bit_forced", token = "") # åŒæ ·æ”¹ä¸º_forcedç‰ˆæœ¬

# ä»…ä¿å­˜LoRAé€‚é…å™¨
if False:
    model.save_pretrained_merged("model", tokenizer, save_method = "lora",)
if False: # ä¸Šä¼ åˆ°HuggingFace Hub
    model.push_to_hub_merged("hf/model", tokenizer, save_method = "lora", token = "")

"""### GGUF / llama.cpp Conversion
To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.

Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):
* `q8_0` - Fast conversion. High resource use, but generally acceptable.
* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.
* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.

[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)
"""

# GGUF / llama.cpp æ ¼å¼è½¬æ¢
# æ”¯æŒå¤šç§é‡åŒ–æ–¹æ³•ï¼Œå¦‚q8_0ã€q4_k_mã€q5_k_mç­‰

# F16ï¼ˆFloat16ï¼‰æ ¼å¼

# ç²¾åº¦ç±»å‹ï¼šåŠç²¾åº¦æµ®ç‚¹æ•°ï¼ˆ16ä½æµ®ç‚¹æ•°ï¼‰
# å†…å­˜å ç”¨ï¼šæ¯”åŸå§‹FP32ï¼ˆ32ä½æµ®ç‚¹æ•°ï¼‰å‡å°‘çº¦50%çš„å­˜å‚¨ç©ºé—´
# ç²¾åº¦ä¿ç•™ï¼šä¿ç•™äº†ç›¸å¯¹è¾ƒé«˜çš„æ•°å€¼ç²¾åº¦ï¼ŒæŸå¤±è¾ƒå°
# æ¨ç†æ€§èƒ½ï¼šæ¯”FP32å¿«ï¼Œä½†æ¯”æ›´ä½ä½é‡åŒ–æ ¼å¼æ…¢
# é€‚ç”¨åœºæ™¯ï¼šå½“éœ€è¦åœ¨å†…å­˜ä½¿ç”¨å’Œæ¨¡å‹ç²¾åº¦ä¹‹é—´å–å¾—å¹³è¡¡æ—¶ä½¿ç”¨

# Q4_K_Mæ ¼å¼

# ç²¾åº¦ç±»å‹ï¼šæ··åˆ4ä½é‡åŒ–æ ¼å¼ï¼ˆæ˜¯GGUFé‡åŒ–æ–¹æ¡ˆçš„ä¸€ç§ï¼‰
# å†…å­˜å ç”¨ï¼šæ¯”F16å‡å°‘çº¦75%çš„å­˜å‚¨ç©ºé—´ï¼Œæ¯”åŸå§‹FP32å‡å°‘çº¦87.5%
# é‡åŒ–ç­–ç•¥ï¼šé’ˆå¯¹ä¸åŒæƒé‡é‡‡ç”¨ä¸åŒçš„é‡åŒ–ç­–ç•¥

# å¯¹æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„WVçŸ©é˜µå’Œå‰é¦ˆç½‘ç»œä¸­çš„W2çŸ©é˜µçš„ä¸€åŠä½¿ç”¨Q6_Ké‡åŒ–
# å¯¹å…¶ä½™æƒé‡ä½¿ç”¨Q4_Ké‡åŒ–

# ç²¾åº¦ä¸é€Ÿåº¦ï¼šç‰ºç‰²ä¸€å®šç²¾åº¦ä»¥è·å¾—æ›´å°çš„æ–‡ä»¶å¤§å°å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦
# é€‚ç”¨åœºæ™¯ï¼šé€‚åˆåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šè¿è¡Œæ¨¡å‹ï¼Œå¦‚ä¸ªäººç”µè„‘æˆ–ç§»åŠ¨è®¾å¤‡

# # Save to 8bit Q8_0
# if False:
#     model.save_pretrained_gguf("model", tokenizer,)
# # Remember to go to https://huggingface.co/settings/tokens for a token!
# # And change hf to your username!
# if False:
#     model.push_to_hub_gguf("hf/model", tokenizer, token = "")

# # ä¿å­˜ä¸º16ä½GGUF
# if False:
#     model.save_pretrained_gguf("model", tokenizer, quantization_method = "f16")
# if False: # ä¸Šä¼ åˆ°HuggingFace Hub
#     model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "f16", token = "")

# # ä¿å­˜ä¸ºq4_k_mæ ¼å¼GGUF
if True:
    model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
if True:# ä¸Šä¼ åˆ°HuggingFace Hub
    model.push_to_hub_gguf("leo009/Qwen3-GGUF", tokenizer, quantization_method = "q4_k_m", token = "")

# # ä¿å­˜å¤šç§GGUFæ ¼å¼ï¼ˆæ‰¹é‡å¯¼å‡ºæ›´é«˜æ•ˆï¼‰
# if False:
#     model.push_to_hub_gguf(
#         "hf/model", # Change hf to your username!
#         tokenizer,
#         quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
#         token = "", # Get a token at https://huggingface.co/settings/tokens
#     )

from google.colab import drive
drive.mount('/content/gdrive')

# Save to Google Drive with q4_k_m quantization
if True:
    model.save_pretrained_gguf("/content/gdrive/MyDrive/MyModel/model",
                              tokenizer,
                              quantization_method = "q4_k_m")

"""Now, use the `model.gguf` file or `model-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)

And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!

Some other links:
1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)
2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)
3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)
6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!

<div class="align-center">
  <a href="https://unsloth.ai"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord.png" width="145"></a>
  <a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a>

  Join Discord if you need help + â­ï¸ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> â­ï¸
</div>

"""
```



