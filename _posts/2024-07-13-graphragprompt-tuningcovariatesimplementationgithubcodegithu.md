---
layout: single
title: "GraphRAGæœ¬åœ°æ£€ç´¢+ Prompt Tuning + covariates(åå˜é‡)+çˆ¬è™«å®ç°GitHubé¡¹ç›®ä»£ç æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œå¿«é€ŸæŒæ¡æœ€æ–°GitHubå¼€æºé¡¹ç›®"
sidebar:
  nav: "docs"
date: 2024-07-13 00:00:00 +0800
categories: [AI-Agents, Fine-Tuning, LLM, RAG, Tutorial, Vision]
tags: [AI, AI-Agents, Chainlit, GPT, GraphRAG, RAG]
classes: wide
author_profile: true
---


#  GraphRAGæœ¬åœ°æ£€ç´¢+ **Prompt Tuning** \+ **covariates(åå˜é‡)+çˆ¬è™«å®ç°GitHubé¡¹ç›®ä»£ç æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œå¿«é€ŸæŒæ¡æœ€æ–°GitHubå¼€æºé¡¹ç›®**

###  **ğŸ”¥Prompt Tuning**

> GraphRAGæ˜¯å¾®è½¯ç ”ç©¶é™¢å¼€å‘çš„ä¸€ä¸ªçŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKnowledge Graph Retrieval-Augmented Generationï¼‰ç³»ç»Ÿã€‚   
>  åœ¨GraphRAGä¸­ï¼ŒPrompt Tuningï¼ˆæç¤ºè°ƒä¼˜ï¼‰æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œä¸ºç”¨æˆ·æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”çµæ´»çš„æ–¹å¼æ¥ä¼˜åŒ–å’Œå®šåˆ¶æŸ¥è¯¢ä½“éªŒã€‚ä»¥ä¸‹æ˜¯GraphRAGä¸­Prompt Tuningçš„ä¸»è¦ä¼˜åŠ¿å’Œä½œç”¨ï¼š   
> 
> 
> ##  ç®€åŒ–é…ç½®è¿‡ç¨‹ 
> 
>   1. **é»˜è®¤æç¤ºæ¨¡æ¿** : GraphRAGæä¾›äº†ä¸€å¥—é»˜è®¤çš„æç¤ºæ¨¡æ¿ï¼Œæ¶µç›–äº†å®ä½“/å…³ç³»æå–ã€æè¿°æ€»ç»“ã€å£°æ˜æå–ç­‰å…³é”®ä»»åŠ¡ã€‚è¿™äº›é¢„è®¾æ¨¡æ¿ä½¿ç”¨æˆ·èƒ½å¤Ÿå¿«é€Ÿå¯åŠ¨å’Œä½¿ç”¨ç³»ç»Ÿï¼Œæ— éœ€å¤æ‚çš„åˆå§‹é…ç½®ã€‚ 
> 

>   2. **è‡ªåŠ¨æ¨¡æ¿åŒ–** : GraphRAGå¼•å…¥äº†è‡ªåŠ¨æ¨¡æ¿åŒ–åŠŸèƒ½ï¼Œè¯¥åŠŸèƒ½å¯ä»¥æ ¹æ®ç”¨æˆ·çš„è¾“å…¥æ•°æ®å’Œä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äº¤äº’è‡ªåŠ¨ç”Ÿæˆé€‚åº”ç‰¹å®šé¢†åŸŸçš„æ¨¡æ¿ã€‚è¿™å¤§å¤§é™ä½äº†ç”¨æˆ·çš„å·¥ä½œé‡ï¼ŒåŒæ—¶æé«˜äº†ç³»ç»Ÿå¯¹ä¸åŒé¢†åŸŸæ•°æ®çš„é€‚åº”æ€§ã€‚ 
> 

> 
> ##  æé«˜æ€§èƒ½å’Œæ•ˆç‡ 
> 
>   1. **å‚æ•°é«˜æ•ˆæ€§** : Prompt Tuningåªéœ€æ›´æ–°å°‘é‡çš„æç¤ºå‚æ•°ï¼Œè€Œæ— éœ€å¯¹æ•´ä¸ªé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¿™ç§æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹é€šç”¨èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚ 
> 

>   2. **æ€§èƒ½æ¥è¿‘å®Œå…¨å¾®è°ƒ** : éšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§ï¼ŒPrompt Tuningçš„æ€§èƒ½è¶Šæ¥è¶Šæ¥è¿‘å®Œå…¨å¾®è°ƒï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šç”šè‡³èƒ½è¶…è¿‡å®Œå…¨å¾®è°ƒã€‚è¿™æ„å‘³ç€ç”¨æˆ·å¯ä»¥è·å¾—æ¥è¿‘å…¨é¢å¾®è°ƒçš„æ€§èƒ½ï¼Œä½†ä»˜å‡ºçš„è®¡ç®—æˆæœ¬è¦ä½å¾—å¤šã€‚ 
> 

>   3. **é«˜æ•ˆæ¨ç†** : ç”±äºåªæ›´æ–°å°‘é‡å‚æ•°ï¼ŒPrompt Tuningä¿ç•™äº†é¢„è®­ç»ƒæ¨¡å‹çš„é«˜æ•ˆæ¨ç†èƒ½åŠ›ã€‚è¿™å¯¹äºéœ€è¦å¿«é€Ÿå“åº”çš„å®æ—¶åº”ç”¨å°¤ä¸ºé‡è¦ã€‚ 
> 

> 
> ##  å¢å¼ºé€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ› 
> 
>   1. **é¢†åŸŸé€‚åº”æ€§** : é€šè¿‡å°†ä»»åŠ¡ç‰¹å®šçš„ä¿¡æ¯ç¼–ç åœ¨æç¤ºä¸­ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹å‚æ•°ä¸å˜ï¼ŒPrompt Tuningåœ¨é¢†åŸŸè¿ç§»é—®é¢˜ä¸Šè¡¨ç°æ›´å¥½ã€‚è¿™ä½¿å¾—GraphRAGèƒ½å¤Ÿæ›´å®¹æ˜“åœ°é€‚åº”ä¸åŒçš„çŸ¥è¯†é¢†åŸŸå’Œåº”ç”¨åœºæ™¯ã€‚ 
> 

>   2. **æç¤ºé›†æˆ** : GraphRAGå…è®¸å­¦ä¹ å¤šä¸ªæç¤ºå¹¶è¿›è¡Œé›†æˆï¼Œè¿™ç§æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æé«˜ç³»ç»Ÿæ€§èƒ½ï¼Œå¹¶æä¾›æ›´åŠ çµæ´»çš„æŸ¥è¯¢ç­–ç•¥ã€‚ 
> 

> 
> ##  æé«˜å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ 
> 
>   1. **å¯è§£é‡Šæ€§** : å­¦ä¹ åˆ°çš„æç¤ºå‚æ•°å…·æœ‰ä¸€å®šçš„å¯è§£é‡Šæ€§ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·ç†è§£æ¨¡å‹çš„è¡Œä¸ºã€‚è¿™å¯¹äºéœ€è¦é€æ˜åº¦å’Œå¯è§£é‡Šæ€§çš„åº”ç”¨åœºæ™¯éå¸¸æœ‰ä»·å€¼ã€‚ 
> 

>   2. **ç²¾ç»†æ§åˆ¶** : é€šè¿‡æ‰‹åŠ¨é…ç½®é€‰é¡¹ï¼Œé«˜çº§ç”¨æˆ·å¯ä»¥å¯¹æç¤ºè¿›è¡Œæ›´ç²¾ç»†çš„æ§åˆ¶å’Œè°ƒæ•´ã€‚è¿™ä¸ºä¸“ä¸šç”¨æˆ·æä¾›äº†æ›´å¤§çš„çµæ´»æ€§å’Œå®šåˆ¶åŒ–èƒ½åŠ›ã€‚ 
> 

> 
> ##  ä¼˜åŒ–çŸ¥è¯†å›¾è°±æ„å»º 
> 
>   1. **å®ä½“å’Œå…³ç³»æå–** : Prompt Tuningä¼˜åŒ–äº†å®ä½“è¯†åˆ«å’Œå…³ç³»æå–çš„è¿‡ç¨‹ï¼Œæé«˜äº†çŸ¥è¯†å›¾è°±æ„å»ºçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ 
> 

>   2. **æè¿°æ€»ç»“** : é€šè¿‡ä¼˜åŒ–çš„æç¤ºï¼Œç³»ç»Ÿèƒ½å¤Ÿæ›´å¥½åœ°ç”Ÿæˆå®ä½“å’Œå…³ç³»çš„æè¿°æ‘˜è¦ï¼Œä¸°å¯Œäº†çŸ¥è¯†å›¾è°±çš„å†…å®¹ã€‚ 
> 

>   3. **ç¤¾åŒºæŠ¥å‘Šç”Ÿæˆ** : Prompt Tuningè¿˜æ”¹è¿›äº†ç¤¾åŒºæŠ¥å‘Šçš„ç”Ÿæˆè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ‘˜è¦ã€å½±å“è¯„ä¼°å’Œå…³é”®å‘ç°çš„æå–ï¼Œä½¿å¾—ç”Ÿæˆçš„æŠ¥å‘Šæ›´åŠ å…¨é¢å’Œæœ‰æ´å¯ŸåŠ›ã€‚ 
> 

> 
> GraphRAGä¸­çš„Prompt TuningæŠ€æœ¯é€šè¿‡æä¾›çµæ´»ã€é«˜æ•ˆä¸”æ˜“äºä½¿ç”¨çš„æç¤ºè°ƒä¼˜æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„æ€§èƒ½ã€é€‚åº”æ€§å’Œå¯ç”¨æ€§ã€‚å®ƒä¸ä»…ç®€åŒ–äº†ç”¨æˆ·çš„æ“ä½œæµç¨‹ï¼Œè¿˜åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä¸ºçŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆä»»åŠ¡æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚ 

###  **ğŸ”¥** covariates(åå˜é‡) 

> GraphRAGä¸­çš„covariates(åå˜é‡)æ˜¯ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†,å…·æœ‰ä»¥ä¸‹å‡ ä¸ªå…³é”®ä½œç”¨å’Œä¼˜åŠ¿: 
> 
>   1. **æä¾›ç»“æ„åŒ–å£°æ˜ä¿¡æ¯**   
>  Covariatesä»£è¡¨ä»æ–‡æœ¬ä¸­æå–çš„å…³äºå®ä½“çš„ç»“æ„åŒ–å£°æ˜æˆ–é™ˆè¿°ä¿¡æ¯,è¿™äº›ä¿¡æ¯å¯èƒ½æ˜¯æ—¶é—´ç›¸å…³çš„ã€‚å®ƒä»¬ä½œä¸ºGraphRAGçŸ¥è¯†æ¨¡å‹çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†,ä¸å®ä½“ã€å…³ç³»ç­‰ä¸€èµ·æ„æˆäº†å®Œæ•´çš„çŸ¥è¯†å›¾è°±ç»“æ„ã€‚   
> 
> 

>   2. **å¢å¼ºä¸Šä¸‹æ–‡ç†è§£**   
>  Covariatesä¸ºçŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å’Œå…³ç³»æä¾›äº†é¢å¤–çš„ä¸Šä¸‹æ–‡å’Œç»†èŠ‚ä¿¡æ¯ã€‚è¿™äº›ä¸Šä¸‹æ–‡ä¿¡æ¯å¯ä»¥è¢«åˆ©ç”¨æ¥ç”Ÿæˆæ›´ç›¸å…³å’Œè¿è´¯çš„æŸ¥è¯¢å“åº”ã€‚   
> 
> 

>   3. **æ•è·ç»†ç²’åº¦çŸ¥è¯†**   
>  é€šè¿‡åŒ…å«covariates,GraphRAGèƒ½å¤Ÿä»æºæ–‡æ¡£ä¸­æ•è·æ›´ç»†è‡´å’Œå…¨é¢çš„çŸ¥è¯†,è€Œä¸ä»…ä»…æ˜¯ç®€å•çš„å®ä½“-å…³ç³»ä¿¡æ¯ã€‚è¿™ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„æŸ¥è¯¢å’Œæ¨ç†ä»»åŠ¡ã€‚   
> 
> 

>   4. **æ”¯æŒé«˜çº§æ¨ç†**   
>  Covariatesä½¿GraphRAGèƒ½å¤Ÿåœ¨çŸ¥è¯†å›¾è°±ä¸Šæ‰§è¡Œæ›´é«˜çº§çš„æ¨ç†å’Œæ¨æ–­,è¶…è¶Šäº†ç®€å•çš„äº‹å®æ£€ç´¢ã€‚å®ƒä»¬æ•è·çš„å…³ç³»å’Œå±æ€§ä¿¡æ¯å…è®¸ç³»ç»Ÿå‘ç°éšå«çš„è¿æ¥å¹¶å¾—å‡ºæ–°çš„è§è§£ã€‚   
> 
> 

>   5. **æ”¹è¿›ä¿¡æ¯å¯è§£é‡Šæ€§**   
>  ä½œä¸ºæ•´ä½“çŸ¥è¯†å›¾è°±çš„ä¸€éƒ¨åˆ†,covariatesçš„ç»“æ„åŒ–è¡¨ç¤ºä½¿ä¿¡æ¯æ›´æ˜“äºè¯­è¨€æ¨¡å‹ç»„ä»¶è®¿é—®å’Œè§£é‡Šã€‚è¿™ç§ç»“æ„åŒ–æ•°æ®å¯ä»¥æ›´æœ‰æ•ˆåœ°é›†æˆåˆ°å“åº”ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚   
> 
> 

>   6. **ä¼˜åŒ–ç¤¾åŒºæŠ¥å‘Šç”Ÿæˆ**   
>  åœ¨GraphRAGçš„ç¤¾åŒºæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­,covariatesä¿¡æ¯è¢«ç”¨äºç”Ÿæˆæ›´å…¨é¢å’Œæœ‰æ´å¯ŸåŠ›çš„æ‘˜è¦ã€å½±å“è¯„ä¼°å’Œå…³é”®å‘ç°ã€‚   
> 
> 

>   7. **æ”¯æŒæ—¶åºåˆ†æ**   
>  ç”±äºcovariateså¯ä»¥åŒ…å«æ—¶é—´ç›¸å…³çš„ä¿¡æ¯,å®ƒä»¬ä½¿GraphRAGèƒ½å¤Ÿæ‰§è¡Œæ—¶åºåˆ†æå’Œè·Ÿè¸ªå®ä½“éšæ—¶é—´çš„å˜åŒ–ã€‚   
> 
> 

> 
> covariatesä½œä¸ºGraphRAGæ–¹æ³•çš„å…³é”®ç»„æˆéƒ¨åˆ†,æä¾›äº†å®è´µçš„ä¸Šä¸‹æ–‡ä¿¡æ¯,å¹¶ä½¿ç³»ç»Ÿèƒ½å¤Ÿæ‰§è¡Œæ¯”ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿæ›´å¤æ‚çš„æ¨ç†å’Œæ€»ç»“ä»»åŠ¡ã€‚å®ƒä»¬æå¤§åœ°å¢å¼ºäº†GraphRAGçš„çŸ¥è¯†è¡¨ç¤ºèƒ½åŠ›å’ŒæŸ¥è¯¢å¤„ç†èƒ½åŠ›ã€‚ 

###  ğŸ”¥ä»£ç æ£€ç´¢åŠŸèƒ½ 

> ä½¿ç”¨GraphRAGæ¥ä¸ºGitHubä¸Šçš„å¼€æºé¡¹ç›®æ„å»ºRAGç³»ç»Ÿä»¥ä¾¿å¿«é€Ÿæ£€ç´¢ä»£ç å…·æœ‰ä»¥ä¸‹å‡ ä¸ªä¸»è¦ä¼˜åŠ¿: 
> 
> ###  æé«˜æ£€ç´¢æ€§èƒ½å’Œå‡†ç¡®æ€§ 
> 
> GraphRAGé€šè¿‡ç»“åˆçŸ¥è¯†å›¾è°±å’Œå¤§å‹è¯­è¨€æ¨¡å‹,èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œè¿æ¥ä»£ç åº“ä¸­çš„å„ä¸ªç»„ä»¶ä¹‹é—´çš„å…³ç³»ã€‚è¿™ç§æ–¹æ³•å¯ä»¥: 
> 
>   * æ•æ‰ä»£ç ç»“æ„å’Œä¾èµ–å…³ç³»,æä¾›æ›´ç²¾å‡†çš„æ£€ç´¢ç»“æœ 
> 

>   * ç†è§£ä»£ç çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡,è€Œä¸ä»…ä»…æ˜¯åŸºäºå…³é”®è¯åŒ¹é… 
> 

>   * è¿æ¥åˆ†æ•£åœ¨ä¸åŒæ–‡ä»¶æˆ–æ¨¡å—ä¸­çš„ç›¸å…³ä»£ç ç‰‡æ®µ 
> 

> 
> ###  é™ä½è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ 
> 
> ä¸ä¼ ç»Ÿçš„RAGç³»ç»Ÿç›¸æ¯”,GraphRAGå¯ä»¥æ›´é«˜æ•ˆåœ°å­˜å‚¨å’Œæ£€ç´¢ä¿¡æ¯[3]: 
> 
>   * å‡å°‘å†—ä½™æ•°æ®å­˜å‚¨,å› ä¸ºå…³ç³»ä¿¡æ¯è¢«ç¼–ç åœ¨å›¾ç»“æ„ä¸­ 
> 

>   * åŠ å¿«æ£€ç´¢é€Ÿåº¦,é€šè¿‡å›¾éå†è€Œä¸æ˜¯å…¨æ–‡æœç´¢ 
> 

>   * é™ä½å¤„ç†å¤§å‹ä»£ç åº“æ‰€éœ€çš„è®¡ç®—èµ„æº 
> 

> 
> ###  GraphRAGçš„æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶æ›´å®¹æ˜“é€‚åº”ä¸åŒçš„ä»£ç åº“å’Œé¡¹ç›®ç»“æ„: 
> 
>   * å¯ä»¥è½»æ¾åœ°æ·»åŠ æ–°çš„ä»£ç ç»„ä»¶æˆ–æ›´æ–°ç°æœ‰å…³ç³» 
> 

>   * æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€å’Œæ¡†æ¶çš„é›†æˆ 
> 

> 
> ###  å¢å¼ºä»£ç ç†è§£å’Œæ–‡æ¡£ç”Ÿæˆ 
> 
> é€šè¿‡åˆ©ç”¨GraphRAGçš„çŸ¥è¯†å›¾è°±ç»“æ„,å¯ä»¥: 
> 
>   * è‡ªåŠ¨ç”Ÿæˆæ›´å…¨é¢å’Œå‡†ç¡®çš„ä»£ç æ–‡æ¡£ 
> 

>   * æä¾›ä»£ç ç»“æ„å’Œä¾èµ–å…³ç³»çš„å¯è§†åŒ– 
> 

>   * è¾…åŠ©å¼€å‘è€…å¿«é€Ÿç†è§£å¤æ‚çš„ä»£ç åº“ 
> 

> 
> ###  æ”¯æŒé«˜çº§æŸ¥è¯¢å’Œåˆ†æ 
> 
> GraphRAGå…è®¸è¿›è¡Œæ›´å¤æ‚å’Œè¯­ä¹‰åŒ–çš„ä»£ç æŸ¥è¯¢: 
> 
>   * å¯ä»¥æ‰§è¡ŒåŸºäºè·¯å¾„å’Œæ¨¡å¼çš„æœç´¢ 
> 

>   * æ”¯æŒè·¨æ–‡ä»¶å’Œæ¨¡å—çš„å…³ç³»æŸ¥è¯¢ 
> 

>   * èƒ½å¤Ÿå›ç­”å…³äºä»£ç ç»“æ„ã€è®¾è®¡æ¨¡å¼å’Œæœ€ä½³å®è·µçš„é—®é¢˜ 
> 

> 
> ä½¿ç”¨GraphRAGæ¥æ„å»ºGitHubé¡¹ç›®çš„RAGç³»ç»Ÿå¯ä»¥æ˜¾è‘—æé«˜ä»£ç æ£€ç´¢çš„æ•ˆç‡å’Œè´¨é‡,åŒæ—¶ä¸ºå¼€å‘è€…æä¾›æ›´æ·±å…¥çš„ä»£ç ç†è§£å’Œåˆ†æèƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¯ä»¥åŠ é€Ÿå¼€å‘è¿‡ç¨‹,è¿˜èƒ½ä¿ƒè¿›æ›´å¥½çš„ä»£ç é‡ç”¨å’ŒçŸ¥è¯†å…±äº«ã€‚ 

##  **ğŸ‘‰ğŸ‘‰ğŸ‘‰å¦‚æœ‰é—®é¢˜è¯·è”ç³»æˆ‘çš„å¾½ä¿¡ stoeng**

##  **ğŸ”¥ğŸ”¥ğŸ”¥æœ¬é¡¹ç›®ä»£ç ç”±AIè¶…å…ƒåŸŸé¢‘é“åˆ¶ä½œï¼Œè§‚çœ‹æ›´å¤šå¤§æ¨¡å‹å¾®è°ƒè§†é¢‘è¯·è®¿é—®æˆ‘çš„é¢‘é“â¬‡**

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„å“”å“©å“”å“©é¢‘é“ ](<https://space.bilibili.com/3493277319825652>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰** **[ æˆ‘çš„YouTubeé¢‘é“ ](<https://www.youtube.com/@AIsuperdomain>) **

###  **ğŸ‘‰ğŸ‘‰ğŸ‘‰æˆ‘çš„å¼€æºé¡¹ç›®** **[ https://github.com/win4r/AISuperDomain ](<https://github.com/win4r/AISuperDomain>) **
    
    
```
    claim_extraction:
      ## llm: override the global llm settings for this task
      ## parallelization: override the global parallelization settings for this task
      ## async_mode: override the global async_mode settings for this task
      enabled: true
      prompt: "prompts/claim_extraction.txt"
      description: "Any claims or facts that could be relevant to information discovery."
      max_gleanings: 1
```
    
    ##Just uncomment the enabled line in your settings.yaml file.
    ##I'll resolve the issue, but please reopen if this doesn't work
    
    
    pip install graphrag
    
```
    python -m graphrag.index --init  --root . 
    python -m graphrag.index --root . 
    ###æç¤ºä¼˜åŒ–
    python -m graphrag.prompt_tune --root . --no-entity-types
```
    
    GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE="prompts/entity_extraction.txt"
    
    GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE="prompts/community_report.txt"
    
    GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE="prompts/summarize_descriptions.txt"
    
    
    
    è®¾ç½®env
    GRAPHRAG_API_KEY=sk-proj-wDPaUDu1Iim
    
    
    
```
    export GRAPHRAG_API_KEY="sk-xggd2443fg"
    export GRAPHRAG_LLM_MODEL="gpt-3.5-turbo"
    export GRAPHRAG_EMBEDDING_MODEL="text-embedding-ada-002"
```
    
```
    export GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE="prompts/entity_extraction.txt"
    export GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE="prompts/community_report.txt"
    export GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE="prompts/summarize_descriptions.txt"
```
    
    
    
```
    python -m graphrag.query \
    --root . \
    --method local \
    "how to install crawl4ai?"
```
    
    
    ###settings.yaml
    
```
    encoding_model: cl100k_base
    skip_workflows: []
    llm:
      api_key: ${GRAPHRAG_API_KEY}
      type: openai_chat # or azure_openai_chat
      model: gpt-3.5-turbo-0125
      model_supports_json: true # recommended if this is available for your model.
      # max_tokens: 4000
      # request_timeout: 180.0
      # api_base: https://<instance>.openai.azure.com
      # api_version: 2024-02-15-preview
      # organization: <organization_id>
      # deployment_name: <azure_model_deployment_name>
      # tokens_per_minute: 150_000 # set a leaky bucket throttle
      # requests_per_minute: 10_000 # set a leaky bucket throttle
      # max_retries: 10
      # max_retry_wait: 10.0
      # sleep_on_rate_limit_recommendation: true # whether to sleep when azure suggests wait-times
      # concurrent_requests: 25 # the number of parallel inflight requests that may be made
```
    
```
    parallelization:
      stagger: 0.3
      # num_threads: 50 # the number of threads to use for parallel processing
```
    
    async_mode: threaded # or asyncio
    
```
    embeddings:
      ## parallelization: override the global parallelization settings for embeddings
      async_mode: threaded # or asyncio
      llm:
        api_key: ${GRAPHRAG_API_KEY}
        type: openai_embedding # or azure_openai_embedding
        model: text-embedding-3-small
        # api_base: https://<instance>.openai.azure.com
        # api_version: 2024-02-15-preview
        # organization: <organization_id>
        # deployment_name: <azure_model_deployment_name>
        # tokens_per_minute: 150_000 # set a leaky bucket throttle
        # requests_per_minute: 10_000 # set a leaky bucket throttle
        # max_retries: 10
        # max_retry_wait: 10.0
        # sleep_on_rate_limit_recommendation: true # whether to sleep when azure suggests wait-times
        # concurrent_requests: 25 # the number of parallel inflight requests that may be made
        # batch_size: 16 # the number of documents to send in a single request
        # batch_max_tokens: 8191 # the maximum number of tokens to send in a single request
        # target: required # or optional
```
      
    
    
```
    chunks:
      size: 300
      overlap: 100
      group_by_columns: [id] # by default, we don't allow chunks to cross documents
```
        
```
    input:
      type: file # or blob
      file_type: text # or csv
      base_dir: "input"
      file_encoding: utf-8
      file_pattern: ".*\\.txt$"
```
    
```
    cache:
      type: file # or blob
      base_dir: "cache"
      # connection_string: <azure_blob_storage_connection_string>
      # container_name: <azure_blob_storage_container_name>
```
    
```
    storage:
      type: file # or blob
      base_dir: "inputs/artifacts"
      # connection_string: <azure_blob_storage_connection_string>
      # container_name: <azure_blob_storage_container_name>
```
    
```
    reporting:
      type: file # or console, blob
      base_dir: "inputs/reports"
      # connection_string: <azure_blob_storage_connection_string>
      # container_name: <azure_blob_storage_container_name>
```
    
```
    entity_extraction:
      ## llm: override the global llm settings for this task
      ## parallelization: override the global parallelization settings for this task
      ## async_mode: override the global async_mode settings for this task
      prompt: "prompts/entity_extraction.txt"
      entity_types: [organization,person,geo,event]
      max_gleanings: 0
```
    
```
    summarize_descriptions:
      ## llm: override the global llm settings for this task
      ## parallelization: override the global parallelization settings for this task
      ## async_mode: override the global async_mode settings for this task
      prompt: "prompts/summarize_descriptions.txt"
      max_length: 500
```
    
```
    claim_extraction:
      ## llm: override the global llm settings for this task
      ## parallelization: override the global parallelization settings for this task
      ## async_mode: override the global async_mode settings for this task
      # enabled: true
      prompt: "prompts/claim_extraction.txt"
      description: "Any claims or facts that could be relevant to information discovery."
      max_gleanings: 0
```
    
```
    community_report:
      ## llm: override the global llm settings for this task
      ## parallelization: override the global parallelization settings for this task
      ## async_mode: override the global async_mode settings for this task
      prompt: "prompts/community_report.txt"
      max_length: 2000
      max_input_length: 8000
```
    
    cluster_graph:
      max_cluster_size: 10
    
```
    embed_graph:
      enabled: false # if true, will generate node2vec embeddings for nodes
      # num_walks: 10
      # walk_length: 40
      # window_size: 2
      # iterations: 3
      # random_seed: 597832
```
    
    umap:
      enabled: false # if true, will generate UMAP embeddings for nodes
    
```
    snapshots:
      graphml: false
      raw_entities: false
      top_level_nodes: false
```
    
```
    local_search:
      # text_unit_prop: 0.5
      # community_prop: 0.1
      # conversation_history_max_turns: 5
      # top_k_mapped_entities: 10
      # top_k_relationships: 10
      # max_tokens: 12000
```
    
```
    global_search:
      # max_tokens: 12000
      # data_max_tokens: 12000
      # map_max_tokens: 1000
      # reduce_max_tokens: 2000
      # concurrency: 32
```
    
    
    
```
    import os
    import asyncio
    import pandas as pd
    import tiktoken
    from rich import print
    from typing import List
```
    
```
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—å’Œç±»
    from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
    from graphrag.query.indexer_adapters import (
        read_indexer_covariates,
        read_indexer_entities,
        read_indexer_relationships,
        read_indexer_reports,
        read_indexer_text_units,
    )
    from graphrag.query.input.loaders.dfs import store_entity_semantic_embeddings
    from graphrag.query.llm.oai.chat_openai import ChatOpenAI
    from graphrag.query.llm.oai.embedding import OpenAIEmbedding
    from graphrag.query.llm.oai.typing import OpenaiApiType
    from graphrag.query.question_gen.local_gen import LocalQuestionGen
    from graphrag.query.structured_search.local_search.mixed_context import LocalSearchMixedContext
    from graphrag.query.structured_search.local_search.search import LocalSearch
    from graphrag.vector_stores.lancedb import LanceDBVectorStore
```
    
```
    # è®¾ç½®å¸¸é‡å’Œé…ç½®
    INPUT_DIR = "/Users/charlesqin/PycharmProjects/RAGCode/inputs/artifacts"
    LANCEDB_URI = f"{INPUT_DIR}/lancedb"
    COMMUNITY_REPORT_TABLE = "create_final_community_reports"
    ENTITY_TABLE = "create_final_nodes"
    ENTITY_EMBEDDING_TABLE = "create_final_entities"
    RELATIONSHIP_TABLE = "create_final_relationships"
    COVARIATE_TABLE = "create_final_covariates"
    TEXT_UNIT_TABLE = "create_final_text_units"
    COMMUNITY_LEVEL = 2
```
    
    
```
    async def setup_llm_and_embedder():
        """
        è®¾ç½®è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåµŒå…¥æ¨¡å‹
        """
        api_key = os.environ["GRAPHRAG_API_KEY"]
        llm_model = os.environ.get("GRAPHRAG_LLM_MODEL", "gpt-3.5-turbo")
        embedding_model = os.environ.get("GRAPHRAG_EMBEDDING_MODEL", "text-embedding-3-small")
```
    
```
        # åˆå§‹åŒ–ChatOpenAIå®ä¾‹
        llm = ChatOpenAI(
            api_key=api_key,
            model=llm_model,
            api_type=OpenaiApiType.OpenAI,
            max_retries=20,
        )
```
    
        # åˆå§‹åŒ–tokenç¼–ç å™¨
        token_encoder = tiktoken.get_encoding("cl100k_base")
    
```
        # åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹
        text_embedder = OpenAIEmbedding(
            api_key=api_key,
            api_base=None,
            api_type=OpenaiApiType.OpenAI,
            model=embedding_model,
            deployment_name=embedding_model,
            max_retries=20,
        )
```
    
        return llm, token_encoder, text_embedder
    
    
```
    async def load_context():
        """
        åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®ï¼ŒåŒ…æ‹¬å®ä½“ã€å…³ç³»ã€æŠ¥å‘Šã€æ–‡æœ¬å•å…ƒå’Œåå˜é‡
        """
        # è¯»å–å®ä½“æ•°æ®
        entity_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_TABLE}.parquet")
        entity_embedding_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet")
        entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)
```
    
```
        # è®¾ç½®å’ŒåŠ è½½å®ä½“æè¿°åµŒå…¥
        description_embedding_store = LanceDBVectorStore(collection_name="entity_description_embeddings")
        description_embedding_store.connect(db_uri=LANCEDB_URI)
        store_entity_semantic_embeddings(entities=entities, vectorstore=description_embedding_store)
```
    
```
        # è¯»å–å…³ç³»æ•°æ®
        relationship_df = pd.read_parquet(f"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet")
        relationships = read_indexer_relationships(relationship_df)
```
    
```
        # è¯»å–ç¤¾åŒºæŠ¥å‘Šæ•°æ®
        report_df = pd.read_parquet(f"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet")
        reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)
```
    
```
        # è¯»å–æ–‡æœ¬å•å…ƒæ•°æ®
        text_unit_df = pd.read_parquet(f"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet")
        text_units = read_indexer_text_units(text_unit_df)
```
    
```
        # è¯»å–å’Œå¤„ç†åå˜é‡æ•°æ®
        covariate_df = pd.read_parquet(f"{INPUT_DIR}/{COVARIATE_TABLE}.parquet")
        claims = read_indexer_covariates(covariate_df)
        print(f"Claim records: {len(claims)}")
        covariates = {"claims": claims}
```
    
        return entities, relationships, reports, text_units, description_embedding_store, covariates
    
    
```
    async def setup_search_engine(llm, token_encoder, text_embedder, entities, relationships, reports, text_units,
                                  description_embedding_store, covariates):
        """
        è®¾ç½®æœç´¢å¼•æ“ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡æ„å»ºå™¨å’Œæœç´¢å‚æ•°
        """
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡æ„å»ºå™¨
        context_builder = LocalSearchMixedContext(
            community_reports=reports,
            text_units=text_units,
            entities=entities,
            relationships=relationships,
            covariates=covariates,
            entity_text_embeddings=description_embedding_store,
            embedding_vectorstore_key=EntityVectorStoreKey.ID,
            text_embedder=text_embedder,
            token_encoder=token_encoder,
        )
```
    
```
        # è®¾ç½®æœ¬åœ°ä¸Šä¸‹æ–‡å‚æ•°
        local_context_params = {
            "text_unit_prop": 0.5,
            "community_prop": 0.1,
            "conversation_history_max_turns": 5,
            "conversation_history_user_turns_only": True,
            "top_k_mapped_entities": 10,
            "top_k_relationships": 10,
            "include_entity_rank": True,
            "include_relationship_weight": True,
            "include_community_rank": False,
            "return_candidate_context": False,
            "embedding_vectorstore_key": EntityVectorStoreKey.ID,
            "max_tokens": 12_000,
        }
```
    
```
        # è®¾ç½®è¯­è¨€æ¨¡å‹å‚æ•°
        llm_params = {
            "max_tokens": 2_000,
            "temperature": 0.0,
        }
```
    
```
        # åˆå§‹åŒ–æœ¬åœ°æœç´¢å¼•æ“
        search_engine = LocalSearch(
            llm=llm,
            context_builder=context_builder,
            token_encoder=token_encoder,
            llm_params=llm_params,
            context_builder_params=local_context_params,
            response_type="multiple paragraphs",
        )
```
    
        return search_engine, context_builder, llm_params, local_context_params
    
    
```
    async def run_search(search_engine, query: str):
        """
        æ‰§è¡Œæœç´¢æŸ¥è¯¢
        """
        result = await search_engine.asearch(query)
        return result
```
    
    
```
    async def generate_questions(question_generator, history: List[str]):
        """
        åŸºäºå†å²ç”Ÿæˆæ–°çš„é—®é¢˜
        """
        questions = await question_generator.agenerate(
            question_history=history, context_data=None, question_count=5
        )
        return questions
```
    
    
```
    async def main():
        """
        ä¸»å‡½æ•°ï¼Œè¿è¡Œæ•´ä¸ªæœç´¢å’Œé—®é¢˜ç”Ÿæˆæµç¨‹
        """
        try:
            # è®¾ç½®è¯­è¨€æ¨¡å‹å’ŒåµŒå…¥å™¨
            llm, token_encoder, text_embedder = await setup_llm_and_embedder()
```
    
            # åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®
            entities, relationships, reports, text_units, description_embedding_store, covariates = await load_context()
    
```
            # è®¾ç½®æœç´¢å¼•æ“
            search_engine, context_builder, llm_params, local_context_params = await setup_search_engine(
                llm, token_encoder, text_embedder, entities, relationships, reports, text_units,
                description_embedding_store, covariates
            )
```
    
```
            # è¿è¡Œæœç´¢ç¤ºä¾‹
            queries = [
                "how to take a screenshot of the page in crawl4ai?",
                "how to set a custom user agent in crawl4ai?",
                "tell me what is Extraction Strategies and show some examples in crawl4ai.",
            ]
```
    
```
            for query in queries:
                print(f"\n[bold]Query:[/bold] {query}")
                result = await run_search(search_engine, query)
                print(f"[bold]Response:[/bold]\n{result.response}")
                print("\n[bold]Context Data:[/bold]")
                print("Entities:")
                print(result.context_data["entities"].head())
                print("\nRelationships:")
                print(result.context_data["relationships"].head())
                print("\nReports:")
                print(result.context_data["reports"].head())
                print("\nSources:")
                print(result.context_data["sources"].head())
                if "claims" in result.context_data:
                    print("\nClaims:")
                    print(result.context_data["claims"].head())
```
    
```
            # é—®é¢˜ç”Ÿæˆ
            question_generator = LocalQuestionGen(
                llm=llm,
                context_builder=context_builder,
                token_encoder=token_encoder,
                llm_params=llm_params,
                context_builder_params=local_context_params,
            )
```
    
```
            question_history = [
                "how to take a screenshot of the page in crawl4ai?",
                "how to set a custom user agent in crawl4ai?",
                "tell me what is Extraction Strategies and show some examples in crawl4ai.",
            ]
            print("\n[bold]Generating questions based on history:[/bold]")
            print(f"History: {question_history}")
            candidate_questions = await generate_questions(question_generator, question_history)
            print("Generated questions:")
            for i, question in enumerate(candidate_questions.response, 1):
                print(f"{i}. {question}")
```
    
        except Exception as e:
            print(f"[bold red]An error occurred:[/bold red] {str(e)}")
    
    
```
    if __name__ == "__main__":
        # è¿è¡Œä¸»å‡½æ•°
        asyncio.run(main())
```

###  çˆ¬è™« 
    
    
    #pip install scrapy html2text bs4
    
```
    import scrapy
    from scrapy.crawler import CrawlerProcess
    from bs4 import BeautifulSoup
    import html2text
    import os
    import json
    from urllib.parse import urlparse
```
    
```
    class ContentFocusedSpider(scrapy.Spider):
        name = 'content_focused_spider'
        start_urls = ['https://crawl4ai.com/mkdocs/']
        allowed_domains = ['crawl4ai.com']
```
    
```
        def __init__(self, *args, **kwargs):
            super(ContentFocusedSpider, self).__init__(*args, **kwargs)
            self.h = html2text.HTML2Text()
            self.h.ignore_links = True
            self.h.ignore_images = True
            self.h.ignore_emphasis = True
            self.h.body_width = 0
            self.results = []
```
            
            os.makedirs('.data', exist_ok=True)
            os.makedirs('.data/markdown_files', exist_ok=True)
    
```
        def parse(self, response):
            # ä½¿ç”¨ BeautifulSoup æå–ä¸»è¦å†…å®¹
            soup = BeautifulSoup(response.text, 'html.parser')
```
            
```
            # ç§»é™¤å¯¼èˆªæ ã€ä¾§è¾¹æ ã€é¡µè„šç­‰å…ƒç´ 
            for elem in soup(['nav', 'header', 'footer', 'aside']):
                elem.decompose()
```
            
            # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')
            
```
            if main_content:
                content = str(main_content)
            else:
                content = str(soup.body)  # å¦‚æœæ‰¾ä¸åˆ°æ˜ç¡®çš„ä¸»è¦å†…å®¹ï¼Œä½¿ç”¨æ•´ä¸ª body
```
    
            # è½¬æ¢ä¸º Markdown
            markdown_content = self.h.handle(content)
    
```
            # ç”Ÿæˆæ–‡ä»¶åå¹¶ä¿å­˜
            parsed_url = urlparse(response.url)
            file_path = parsed_url.path.strip('/').replace('/', '_') or 'index'
            markdown_filename = f'.data/markdown_files/{file_path}.md'
```
            
            with open(markdown_filename, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
```
            result = {
                'url': response.url,
                'markdown_file': markdown_filename,
            }
            self.results.append(result)
```
            
```
            # ç»§ç»­çˆ¬å–å…¶ä»–é“¾æ¥
            for link in response.css('a::attr(href)').getall():
                yield response.follow(link, self.parse)
```
    
```
        def closed(self, reason):
            with open('.data/markdown_results.json', 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
```
            
```
            print(f"çˆ¬å–å®Œæˆã€‚æ€»å…±çˆ¬å–äº† {len(self.results)} ä¸ªé¡µé¢")
            print("ç»“æœå…ƒæ•°æ®ä¿å­˜åœ¨ .data/markdown_results.json")
            print("Markdown æ–‡ä»¶ä¿å­˜åœ¨ .data/markdown_files/ ç›®å½•ä¸‹")
```
    
```
    process = CrawlerProcess(settings={
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'ROBOTSTXT_OBEY': True,
        'CONCURRENT_REQUESTS': 1,
        'DOWNLOAD_DELAY': 2,
    })
```
    
    process.crawl(ContentFocusedSpider)
    process.start()

###  fastapi 
    
    
    #pip install fastapi uvicorn
    
```
    #æµ‹è¯•å‘½ä»¤
    #curl -X POST "http://localhost:8012/v1/completions" -H "Content-Type: application/json" -d '{"prompt": "how to take a screenshot of the page in crawl4ai?"}'
    #curl -X POST "http://localhost:8012/v1/question_generation" -H "Content-Type: application/json" -d '{"question_history": ["how to take a screenshot of the page in crawl4ai?", "how to set a custom user agent in crawl4ai?"], "question_count": 5}'
```
    
```
    import os
    import asyncio
    import time
    import pandas as pd
    import tiktoken
    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel
    from typing import List, Optional
```
    
```
    # å¯¼å…¥å¿…è¦çš„GraphRAGæ¨¡å—å’Œç±»
    from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
    from graphrag.query.indexer_adapters import (
        read_indexer_covariates,
        read_indexer_entities,
        read_indexer_relationships,
        read_indexer_reports,
        read_indexer_text_units,
    )
    from graphrag.query.input.loaders.dfs import store_entity_semantic_embeddings
    from graphrag.query.llm.oai.chat_openai import ChatOpenAI
    from graphrag.query.llm.oai.embedding import OpenAIEmbedding
    from graphrag.query.llm.oai.typing import OpenaiApiType
    from graphrag.query.question_gen.local_gen import LocalQuestionGen
    from graphrag.query.structured_search.local_search.mixed_context import LocalSearchMixedContext
    from graphrag.query.structured_search.local_search.search import LocalSearch
    from graphrag.vector_stores.lancedb import LanceDBVectorStore
```
    
    # åˆ›å»ºFastAPIåº”ç”¨
    app = FastAPI()
    
```
    # è®¾ç½®å¸¸é‡å’Œé…ç½®
    INPUT_DIR = "/Users/charlesqin/PycharmProjects/RAGCode/inputs/artifacts"
    LANCEDB_URI = f"{INPUT_DIR}/lancedb"
    COMMUNITY_REPORT_TABLE = "create_final_community_reports"
    ENTITY_TABLE = "create_final_nodes"
    ENTITY_EMBEDDING_TABLE = "create_final_entities"
    RELATIONSHIP_TABLE = "create_final_relationships"
    COVARIATE_TABLE = "create_final_covariates"
    TEXT_UNIT_TABLE = "create_final_text_units"
    COMMUNITY_LEVEL = 2
```
    
```
    # å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨æœç´¢å¼•æ“å’Œé—®é¢˜ç”Ÿæˆå™¨
    search_engine = None
    question_generator = None
```
    
    
```
    # å®šä¹‰APIè¯·æ±‚çš„æ•°æ®æ¨¡å‹
    class Query(BaseModel):
        prompt: str
        max_tokens: Optional[int] = 2000
        temperature: Optional[float] = 0.0
```
    
    
```
    class QuestionGenRequest(BaseModel):
        question_history: List[str]
        question_count: Optional[int] = 5
```
    
    
```
    async def setup_llm_and_embedder():
        """
        è®¾ç½®è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåµŒå…¥æ¨¡å‹
        """
        api_key = os.environ["GRAPHRAG_API_KEY"]
        llm_model = os.environ.get("GRAPHRAG_LLM_MODEL", "gpt-3.5-turbo")
        embedding_model = os.environ.get("GRAPHRAG_EMBEDDING_MODEL", "text-embedding-3-small")
```
    
```
        # åˆå§‹åŒ–ChatOpenAIå®ä¾‹
        llm = ChatOpenAI(
            api_key=api_key,
            model=llm_model,
            api_type=OpenaiApiType.OpenAI,
            max_retries=20,
        )
```
    
        # åˆå§‹åŒ–tokenç¼–ç å™¨
        token_encoder = tiktoken.get_encoding("cl100k_base")
    
```
        # åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹
        text_embedder = OpenAIEmbedding(
            api_key=api_key,
            api_base=None,
            api_type=OpenaiApiType.OpenAI,
            model=embedding_model,
            deployment_name=embedding_model,
            max_retries=20,
        )
```
    
        return llm, token_encoder, text_embedder
    
    
```
    async def load_context():
        """
        åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®ï¼ŒåŒ…æ‹¬å®ä½“ã€å…³ç³»ã€æŠ¥å‘Šã€æ–‡æœ¬å•å…ƒå’Œåå˜é‡
        """
        # è¯»å–å®ä½“æ•°æ®
        entity_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_TABLE}.parquet")
        entity_embedding_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet")
        entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)
```
    
```
        # è®¾ç½®å’ŒåŠ è½½å®ä½“æè¿°åµŒå…¥
        description_embedding_store = LanceDBVectorStore(collection_name="entity_description_embeddings")
        description_embedding_store.connect(db_uri=LANCEDB_URI)
        store_entity_semantic_embeddings(entities=entities, vectorstore=description_embedding_store)
```
    
```
        # è¯»å–å…³ç³»æ•°æ®
        relationship_df = pd.read_parquet(f"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet")
        relationships = read_indexer_relationships(relationship_df)
```
    
```
        # è¯»å–ç¤¾åŒºæŠ¥å‘Šæ•°æ®
        report_df = pd.read_parquet(f"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet")
        reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)
```
    
```
        # è¯»å–æ–‡æœ¬å•å…ƒæ•°æ®
        text_unit_df = pd.read_parquet(f"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet")
        text_units = read_indexer_text_units(text_unit_df)
```
    
```
        # è¯»å–å’Œå¤„ç†åå˜é‡æ•°æ®
        covariate_df = pd.read_parquet(f"{INPUT_DIR}/{COVARIATE_TABLE}.parquet")
        claims = read_indexer_covariates(covariate_df)
        print(f"Claim records: {len(claims)}")
        covariates = {"claims": claims}
```
    
        return entities, relationships, reports, text_units, description_embedding_store, covariates
    
    
```
    async def setup_search_engine(llm, token_encoder, text_embedder, entities, relationships, reports, text_units,
                                  description_embedding_store, covariates):
        """
        è®¾ç½®æœç´¢å¼•æ“ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡æ„å»ºå™¨å’Œæœç´¢å‚æ•°
        """
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡æ„å»ºå™¨
        context_builder = LocalSearchMixedContext(
            community_reports=reports,
            text_units=text_units,
            entities=entities,
            relationships=relationships,
            covariates=covariates,
            entity_text_embeddings=description_embedding_store,
            embedding_vectorstore_key=EntityVectorStoreKey.ID,
            text_embedder=text_embedder,
            token_encoder=token_encoder,
        )
```
    
```
        # è®¾ç½®æœ¬åœ°ä¸Šä¸‹æ–‡å‚æ•°
        local_context_params = {
            "text_unit_prop": 0.5,
            "community_prop": 0.1,
            "conversation_history_max_turns": 5,
            "conversation_history_user_turns_only": True,
            "top_k_mapped_entities": 10,
            "top_k_relationships": 10,
            "include_entity_rank": True,
            "include_relationship_weight": True,
            "include_community_rank": False,
            "return_candidate_context": False,
            "embedding_vectorstore_key": EntityVectorStoreKey.ID,
            "max_tokens": 12_000,
        }
```
    
```
        # è®¾ç½®è¯­è¨€æ¨¡å‹å‚æ•°
        llm_params = {
            "max_tokens": 2_000,
            "temperature": 0.0,
        }
```
    
```
        # åˆå§‹åŒ–æœ¬åœ°æœç´¢å¼•æ“
        search_engine = LocalSearch(
            llm=llm,
            context_builder=context_builder,
            token_encoder=token_encoder,
            llm_params=llm_params,
            context_builder_params=local_context_params,
            response_type="multiple paragraphs",
        )
```
    
        return search_engine, context_builder, llm_params, local_context_params
    
    
```
    @app.on_event("startup")
    async def startup_event():
        """
        åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–äº‹ä»¶
        """
        global search_engine, question_generator
```
    
        # è®¾ç½®è¯­è¨€æ¨¡å‹å’ŒåµŒå…¥å™¨
        llm, token_encoder, text_embedder = await setup_llm_and_embedder()
    
        # åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®
        entities, relationships, reports, text_units, description_embedding_store, covariates = await load_context()
    
```
        # è®¾ç½®æœç´¢å¼•æ“
        search_engine, context_builder, llm_params, local_context_params = await setup_search_engine(
            llm, token_encoder, text_embedder, entities, relationships, reports, text_units,
            description_embedding_store, covariates
        )
```
    
```
        # è®¾ç½®é—®é¢˜ç”Ÿæˆå™¨
        question_generator = LocalQuestionGen(
            llm=llm,
            context_builder=context_builder,
            token_encoder=token_encoder,
            llm_params=llm_params,
            context_builder_params=local_context_params,
        )
```
    
    
```
    @app.post("/v1/completions")
    async def create_completion(query: Query):
        """
        å¤„ç†æ–‡æœ¬è¡¥å…¨è¯·æ±‚çš„APIç«¯ç‚¹
        """
        if not search_engine:
            raise HTTPException(status_code=500, detail="æœç´¢å¼•æ“æœªåˆå§‹åŒ–")
```
    
```
        try:
            # æ‰§è¡Œæœç´¢
            result = await search_engine.asearch(query.prompt)
            # è¿”å›æ ¼å¼åŒ–çš„å“åº”
            return {
                "id": "cmpl-" + os.urandom(12).hex(),
                "object": "text_completion",
                "created": int(time.time()),
                "model": "graphrag-local-search",
                "choices": [
                    {
                        "text": result.response,
                        "index": 0,
                        "logprobs": None,
                        "finish_reason": "stop"
                    }
                ],
                "usage": {
                    "prompt_tokens": len(query.prompt.split()),
                    "completion_tokens": len(result.response.split()),
                    "total_tokens": len(query.prompt.split()) + len(result.response.split())
                }
            }
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
```
    
    
```
    @app.post("/v1/question_generation")
    async def generate_questions(request: QuestionGenRequest):
        """
        å¤„ç†é—®é¢˜ç”Ÿæˆè¯·æ±‚çš„APIç«¯ç‚¹
        """
        if not question_generator:
            raise HTTPException(status_code=500, detail="é—®é¢˜ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
```
    
```
        try:
            # ç”Ÿæˆå€™é€‰é—®é¢˜
            candidate_questions = await question_generator.agenerate(
                question_history=request.question_history,
                context_data=None,
                question_count=request.question_count
            )
            # è¿”å›æ ¼å¼åŒ–çš„å“åº”
            return {
                "id": "qgen-" + os.urandom(12).hex(),
                "object": "question_generation",
                "created": int(time.time()),
                "model": "graphrag-question-generator",
                "choices": [
                    {
                        "questions": candidate_questions.response,
                        "index": 0
                    }
                ]
            }
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
```
    
    
```
    if __name__ == "__main__":
        # ä½¿ç”¨uvicornè¿è¡ŒFastAPIåº”ç”¨
        import uvicorn
```
    
        uvicorn.run(app, host="0.0.0.0", port=8012)

###  chainlit 
    
    
```
    #pip install chainlit
    import os
    import aiohttp
    import chainlit as cl
    import logging
```
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢ Chainlit åŠ è½½ .env æ–‡ä»¶
    os.environ["CHAINLIT_AUTO_LOAD_DOTENV"] = "false"
    
```
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
```
    
    API_BASE_URL = "http://localhost:8012"  # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨äº†8012ç«¯å£ï¼Œè¯·ç¡®ä¿ä¸æ‚¨çš„APIç«¯å£ä¸€è‡´
    
    
```
    class CustomAsyncClient:
        def __init__(self, base_url: str):
            self.base_url = base_url
            self.chat = self.Chat(base_url)
```
    
```
        class Chat:
            def __init__(self, base_url: str):
                self.base_url = base_url
                self.completions = self.Completions(base_url)
```
    
```
            class Completions:
                def __init__(self, base_url: str):
                    self.base_url = base_url
```
    
```
                async def create(self, messages: list, **kwargs):
                    logger.info(f"Sending request to {self.base_url}/v1/completions")
                    try:
                        async with aiohttp.ClientSession() as session:
                            async with session.post(
                                    f"{self.base_url}/v1/completions",
                                    json={"prompt": messages[-1]["content"], **kwargs}
                            ) as response:
                                logger.info(f"Received response with status {response.status}")
                                if response.status == 200:
                                    data = await response.json()
                                    logger.info(f"Response data: {data}")
                                    return data
                                else:
                                    error_text = await response.text()
                                    logger.error(f"API request failed with status {response.status}: {error_text}")
                                    raise Exception(f"API request failed with status {response.status}: {error_text}")
                    except Exception as e:
                        logger.error(f"Error in create method: {str(e)}")
                        raise
```
    
    
    client = CustomAsyncClient(API_BASE_URL)
    
```
    settings = {
        "model": "graphrag-local-search",
        "temperature": 0,
    }
```
    
    
```
    @cl.on_message
    async def on_message(message: cl.Message):
        logger.info(f"Received message: {message.content}")
        try:
            # å‘é€"å¤„ç†ä¸­"çš„æ¶ˆæ¯
            processing_message = cl.Message(content="Processing your request...")
            await processing_message.send()
```
    
```
            response = await client.chat.completions.create(
                messages=[
                    {
                        "content": "You are a helpful bot based on GraphRAG",
                        "role": "system"
                    },
                    {
                        "content": message.content,
                        "role": "user"
                    }
                ],
                **settings
            )
```
    
            response_content = response['choices'][0]['text']
            logger.info(f"Sending response: {response_content}")
    
            # åˆ›å»ºæ–°æ¶ˆæ¯è€Œä¸æ˜¯æ›´æ–°æ—§æ¶ˆæ¯
            await cl.Message(content=response_content).send()
    
            # ç§»é™¤"å¤„ç†ä¸­"çš„æ¶ˆæ¯
            await processing_message.remove()
    
```
        except Exception as e:
            logger.error(f"Error in on_message: {str(e)}")
            error_message = f"An error occurred: {str(e)}"
            # åˆ›å»ºæ–°çš„é”™è¯¯æ¶ˆæ¯
            await cl.Message(content=error_message).send()
            # ç§»é™¤"å¤„ç†ä¸­"çš„æ¶ˆæ¯
            await processing_message.remove()
```
    
    
```
    @cl.on_chat_start
    async def on_chat_start():
        logger.info("New chat started")
        await cl.Message(content="Welcome! I'm your GraphRAG assistant. How can I help you today?").send()
```
    
    
```
    if __name__ == "__main__":
        logger.info("Starting Chainlit application")
        cl.run()
```
    
    
