---
layout: single  
title: "ğŸš€DeepSeekåˆæ”¾å¤§æ‹›ï¼è¿™ä¸ªOCRæ¨¡å‹è®©æ–‡æ¡£è¯†åˆ«æ•ˆç‡å€å¢ï¼æœ¬åœ°éƒ¨ç½²+å®¢è§‚å®æµ‹DeepSeek-OCRï¼OCRè¯†åˆ«å‡†ç¡®ç‡97%ï¼Œæ”¯æŒ100+è¯­è¨€ï¼Œæ¯å¤©å¤„ç†3300ä¸‡é¡µæ–‡æ¡£çš„å¼€æºå¤§æ¨¡å‹ï¼"  
sidebar:
  nav: "docs"
date: 2025-10-21 10:00:00 +0800  
categories: LLMs
tags: [DeepSeek-OCR, DeepSeek, OCR, VLM, multimodal, AI, PDF, markdown]
classes: wide  

author_profile: true  
---


å¦‚æœä½ ç»å¸¸éœ€è¦å¤„ç†å¤§é‡æ–‡æ¡£ï¼Œæˆ–è€…æ­£åœ¨ä¸ºAIæ¨¡å‹çš„é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›å‘æ„ï¼Œé‚£ä¹ˆè¿™ç¯‡æ–‡ç« ä½ ä¸€å®šè¦çœ‹å®Œã€‚DeepSeekæœ€è¿‘å‘å¸ƒçš„OCRæ¨¡å‹ï¼Œå¯èƒ½ä¼šå½»åº•æ”¹å˜æˆ‘ä»¬å¤„ç†æ–‡æ¡£çš„æ–¹å¼ã€‚

## ä¸€ã€ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦è¿™ä¸ªæ¨¡å‹ï¼Ÿ

è¯´èµ·OCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰ï¼Œå¤§å®¶åº”è¯¥éƒ½ä¸é™Œç”Ÿã€‚ä»æ‰«æçº¸è´¨æ–‡æ¡£åˆ°è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼ŒOCRæŠ€æœ¯å·²ç»æ·±å…¥åˆ°æˆ‘ä»¬å·¥ä½œç”Ÿæ´»çš„æ–¹æ–¹é¢é¢ã€‚ä½†æ˜¯ï¼Œä¼ ç»Ÿçš„OCRæŠ€æœ¯æœ‰ä¸ªè€å¤§éš¾é—®é¢˜â€”â€”**å¤„ç†é•¿æ–‡æ¡£æ—¶æ•ˆç‡ä½ä¸‹ï¼Œæˆæœ¬é«˜æ˜‚**ã€‚

æƒ³è±¡ä¸€ä¸‹è¿™ä¸ªåœºæ™¯ï¼šä½ éœ€è¦è®©AIåˆ†æä¸€ä»½100é¡µçš„ç ”ç©¶æŠ¥å‘Šï¼Œä¼ ç»Ÿæ–¹æ³•éœ€è¦å°†æ¯ä¸ªå­—ç¬¦éƒ½è½¬æ¢æˆæ•°å­—ä¿¡å·ï¼ˆtokenï¼‰ï¼Œä¸€ä»½é•¿æ–‡æ¡£å¯èƒ½éœ€è¦æˆåƒä¸Šä¸‡ä¸ªtokenã€‚è¿™ä¸ä»…ä¼šå¯¼è‡´å¤„ç†é€Ÿåº¦å˜æ…¢ï¼Œè¿˜ä¼šè®©æ˜¾å­˜å ç”¨æš´å¢ï¼Œæˆæœ¬ä¹Ÿéšä¹‹æ°´æ¶¨èˆ¹é«˜ã€‚

DeepSeekå›¢é˜Ÿæ˜¾ç„¶æ³¨æ„åˆ°äº†è¿™ä¸ªç—›ç‚¹ã€‚ä»–ä»¬æå‡ºäº†ä¸€ä¸ªè„‘æ´å¤§å¼€çš„æ€è·¯ï¼š**æ—¢ç„¶æ–‡å­—æœ¬èº«å°±åœ¨å›¾ç‰‡é‡Œï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥è®©AI"çœ‹"å›¾ç‰‡ï¼Œè€Œéé€å­—è¯†åˆ«å‘¢ï¼Ÿ**

> 
ğŸš€æœ¬ç¯‡ç¬”è®°æ‰€å¯¹åº”çš„è§†é¢‘ï¼š
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡å“”å“©å“”å“©è§‚çœ‹](https://www.bilibili.com/video/BV1SPWozJEos/)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ é€šè¿‡YouTubeè§‚çœ‹](https://youtu.be/9oICqbApvTg)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Subagentsè§†é¢‘](https://youtu.be/GjlkRcNNONo)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Gemini CLIè§†é¢‘](https://youtu.be/v41xKxZmygU)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Context Engineeringè§†é¢‘](https://youtu.be/oEZ7aN7jOEI)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ SuperClaudeè§†é¢‘](https://youtu.be/bMO13RNjvBk)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Claudiaè§†é¢‘](https://youtu.be/WIwW7V56wxE)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Task Masterè§†é¢‘](https://youtu.be/6dhOUJ_vnIY)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Zen MCPç¼–ç¨‹è§†é¢‘](https://youtu.be/2WgICfNzgZY)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Augmentç¼–ç¨‹è§†é¢‘](https://youtu.be/DbM3QZy5I6E)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ Serena MCPè§†é¢‘](https://youtu.be/DZ-gLebVnmg)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¼€æºé¡¹ç›®](https://github.com/win4r/AISuperDomain)
- [ğŸ‘‰ğŸ‘‰ğŸ‘‰ è¯·æˆ‘å–å’–å•¡](https://ko-fi.com/aila)
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æˆ‘çš„å¾®ä¿¡ï¼šstoeng
- ğŸ‘‰ğŸ‘‰ğŸ‘‰ æ‰¿æ¥å¤§æ¨¡å‹å¾®è°ƒã€RAGã€AIæ™ºèƒ½ä½“ã€AIç›¸å…³åº”ç”¨å¼€å‘ç­‰é¡¹ç›®ã€‚
> 
ğŸ”¥AIæ™ºèƒ½ä½“ç›¸å…³è§†é¢‘
- [AIæ™ºèƒ½ä½“è§†é¢‘ 1](https://youtu.be/vYm0brFoMwA) 
- [AIæ™ºèƒ½ä½“è§†é¢‘ 2](https://youtu.be/szTXELuaJos)  
- [AIæ™ºèƒ½ä½“è§†é¢‘ 3](https://youtu.be/szTXELuaJos)  
- [AIæ™ºèƒ½ä½“è§†é¢‘ 4](https://youtu.be/RxR3x_Uyq4c)  
- [AIæ™ºèƒ½ä½“è§†é¢‘ 5](https://youtu.be/IrTEDPnEVvU)  
- [AIæ™ºèƒ½ä½“è§†é¢‘ 6](https://youtu.be/q_IdxUGZsow)  



## äºŒã€DeepSeek-OCRçš„"é»‘ç§‘æŠ€"åœ¨å“ªé‡Œï¼Ÿ

### 1. å…‰å­¦ä¸Šä¸‹æ–‡å‹ç¼©ï¼šAIç•Œçš„"å‹ç¼©åŒ…"

DeepSeek-OCRæœ€æ ¸å¿ƒçš„åˆ›æ–°å«åš"å…‰å­¦ä¸Šä¸‹æ–‡å‹ç¼©"ï¼ˆOptical Context Compressionï¼‰ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯æŠŠæ–‡æ¡£å½“æˆä¸€å¼ å›¾ç‰‡ï¼Œç”¨è§†è§‰çš„æ–¹å¼æ¥å‹ç¼©å’Œç†è§£å…¶ä¸­çš„æ–‡å­—ä¿¡æ¯ã€‚

è¿™ç§æ–¹æ³•çš„å·§å¦™ä¹‹å¤„åœ¨äºï¼šä¸€å¼ åŒ…å«å¤§é‡æ–‡å­—çš„å›¾ç‰‡ï¼Œç”¨è§†è§‰tokenè¡¨ç¤ºæ—¶ï¼Œæ¯”ç”¨æ–‡æœ¬tokenè¦é«˜æ•ˆå¾—å¤šã€‚æ ¹æ®DeepSeekçš„å®éªŒæ•°æ®ï¼Œ**åœ¨10å€å‹ç¼©ç‡çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»èƒ½ä¿æŒ97%çš„è¯†åˆ«ç²¾åº¦**ï¼å³ä¾¿å°†å‹ç¼©ç‡æå‡åˆ°20å€ï¼Œç²¾åº¦ä¾ç„¶èƒ½ç»´æŒåœ¨60%å·¦å³ã€‚

è¿™æ˜¯ä»€ä¹ˆæ¦‚å¿µï¼Ÿä¼ ç»ŸOCRå¯èƒ½éœ€è¦ç”¨å‡ åƒä¸ªtokenæ‰èƒ½å¤„ç†å®Œçš„æ–‡æ¡£ï¼ŒDeepSeek-OCRåªéœ€è¦å‡ ç™¾ä¸ªç”šè‡³å‡ åä¸ªtokenå°±èƒ½æå®šã€‚

### 2. åŒæ ¸æ¶æ„ï¼šDeepEncoder + DeepSeek-3B-MoE

DeepSeek-OCRé‡‡ç”¨äº†ç²¾å¿ƒè®¾è®¡çš„åŒç»„ä»¶æ¶æ„ï¼š

**DeepEncoderï¼ˆè§†è§‰ç¼–ç å™¨ï¼‰**

è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„"çœ¼ç›"ï¼Œå‚æ•°é‡çº¦ä¸º380Mã€‚å®ƒçš„è®¾è®¡éå¸¸å·§å¦™ï¼Œèåˆäº†ä¸¤ç§ä¸åŒçš„è§†è§‰å¤„ç†èƒ½åŠ›ï¼š

- **SAMï¼ˆSegment Anything Modelï¼‰**ï¼šè´Ÿè´£å±€éƒ¨æ„ŸçŸ¥ï¼Œåƒæ˜¾å¾®é•œä¸€æ ·æ‰«æå›¾åƒçš„ç»†èŠ‚éƒ¨åˆ†
- **CLIP**ï¼šè´Ÿè´£å…¨å±€ç†è§£ï¼Œåƒé¸Ÿç°å›¾ä¸€æ ·æŠŠæ¡æ•´ä½“å¸ƒå±€å’Œä¸Šä¸‹æ–‡

ä¸¤è€…ä¹‹é—´è¿˜æ’å…¥äº†ä¸€ä¸ª16å€çš„å·ç§¯å‹ç¼©å™¨ã€‚ä¸€å¼ 1024Ã—1024çš„å›¾ç‰‡æœ€åˆä¼šè¢«åˆ†æˆ4096ä¸ªå°å—ï¼Œç»è¿‡SAMå¤„ç†åï¼Œå‹ç¼©å™¨å°†å…¶ç¼©å‡ä¸ºä»…256ä¸ªtokenï¼Œç„¶åå†é€å…¥CLIPè¿›è¡Œå…¨å±€åˆ†æã€‚è¿™ç§è®¾è®¡æ—¢ä¿è¯äº†ç»†èŠ‚è¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œåˆå¤§å¹…é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**DeepSeek-3B-MoEï¼ˆè§£ç å™¨ï¼‰**

è¿™æ˜¯ä¸€ä¸ª30äº¿å‚æ•°çš„æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMixture of Expertsï¼‰ï¼Œä½†æ¯æ¬¡å¤„ç†æ—¶åªæ¿€æ´»çº¦5.7äº¿ä¸ªå‚æ•°ã€‚å®ƒè´Ÿè´£æŠŠå‹ç¼©åçš„è§†è§‰ä¿¡æ¯è§£ç æˆæˆ‘ä»¬èƒ½ç†è§£çš„æ–‡å­—ã€‚

### 3. çµæ´»çš„åˆ†è¾¨ç‡æ¨¡å¼

DeepSeek-OCRæä¾›äº†äº”ç§ä¸åŒçš„å¤„ç†æ¨¡å¼ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯çš„éœ€æ±‚ï¼š

- **Tinyæ¨¡å¼**ï¼š512Ã—512åˆ†è¾¨ç‡ï¼Œä»…éœ€64ä¸ªè§†è§‰token
- **Smallæ¨¡å¼**ï¼š640Ã—640åˆ†è¾¨ç‡ï¼Œéœ€è¦100ä¸ªè§†è§‰token
- **Baseæ¨¡å¼**ï¼š1024Ã—1024åˆ†è¾¨ç‡ï¼Œéœ€è¦256ä¸ªè§†è§‰token
- **Largeæ¨¡å¼**ï¼š1280Ã—1280åˆ†è¾¨ç‡ï¼Œéœ€è¦400ä¸ªè§†è§‰token
- **Gundamæ¨¡å¼**ï¼šåŠ¨æ€åˆ†è¾¨ç‡ï¼Œç»“åˆå¤šä¸ªå±€éƒ¨è§†å›¾å’Œä¸€ä¸ªå…¨å±€è§†å›¾

ä½ å¯ä»¥æ ¹æ®æ–‡æ¡£çš„å¤æ‚ç¨‹åº¦å’Œå¯¹ç²¾åº¦çš„è¦æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„æ¨¡å¼ã€‚æ¯”å¦‚å¤„ç†ç®€å•çš„æ”¶æ®ï¼ŒTinyæ¨¡å¼å°±è¶³å¤Ÿäº†ï¼›è€Œé¢å¯¹å¤æ‚çš„æŠ€æœ¯è®ºæ–‡ï¼Œå¯èƒ½éœ€è¦ç”¨åˆ°Gundamæ¨¡å¼ã€‚

## ä¸‰ã€æ€§èƒ½åˆ°åº•æœ‰å¤šå¼ºï¼Ÿ

æ•°æ®æœ€æœ‰è¯´æœåŠ›ã€‚åœ¨å®é™…æµ‹è¯•ä¸­ï¼ŒDeepSeek-OCRçš„è¡¨ç°ç›¸å½“äº®çœ¼ï¼š

**1. æ•ˆç‡æƒŠäºº**

åœ¨FoxåŸºå‡†æµ‹è¯•ä¸­ï¼Œå½“æ–‡æœ¬tokenæ•°é‡åœ¨è§†è§‰tokençš„10å€ä»¥å†…æ—¶ï¼ŒDeepSeek-OCRèƒ½è¾¾åˆ°97%çš„è§£ç ç²¾åº¦ã€‚è€Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œ**ä¸€å—NVIDIA A100 GPUæ¯å¤©å¯ä»¥å¤„ç†è¶…è¿‡20ä¸‡é¡µæ–‡æ¡£**ï¼

**2. ä»¥å°‘èƒœå¤š**

åœ¨OmniDocBenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDeepSeek-OCRä»…ç”¨100ä¸ªè§†è§‰tokenå°±è¾¾åˆ°äº†ä¸GOT-OCR2.0ï¼ˆä½¿ç”¨256ä¸ªtokenï¼‰ç›¸å½“çš„æ€§èƒ½ã€‚ä¸éœ€è¦è¿‘7000ä¸ªtokençš„MinerU 2.0ç›¸æ¯”ï¼ŒDeepSeek-OCRåªéœ€ä¸åˆ°800ä¸ªtokenå°±èƒ½è¶…è¶Šå…¶è¡¨ç°ã€‚

**3. æ”¯æŒè¶…è¿‡100ç§è¯­è¨€**

æ— è®ºæ˜¯è‹±æ–‡ã€ä¸­æ–‡ï¼Œè¿˜æ˜¯å°è¯­ç§ï¼ŒDeepSeek-OCRéƒ½èƒ½è½»æ¾åº”å¯¹ã€‚è¿™å¯¹äºéœ€è¦å¤„ç†å¤šè¯­è¨€æ–‡æ¡£çš„åœºæ™¯æ¥è¯´ï¼Œç®€ç›´æ˜¯ç¦éŸ³ã€‚

## å››ã€å¯ä»¥ç”¨æ¥åšä»€ä¹ˆï¼Ÿ

DeepSeek-OCRçš„åº”ç”¨åœºæ™¯éå¸¸å¹¿æ³›ï¼š

### æ–‡æ¡£æ•°å­—åŒ–

å°†çº¸è´¨æ–‡æ¡£ã€PDFæ‰«æä»¶å¿«é€Ÿè½¬æ¢ä¸ºå¯ç¼–è¾‘çš„ç”µå­æ–‡æœ¬ï¼Œè€Œä¸”èƒ½ä¿ç•™åŸæœ‰çš„æ’ç‰ˆæ ¼å¼ã€‚å¯¹äºéœ€è¦æ‰¹é‡å¤„ç†å†å²æ¡£æ¡ˆã€åˆåŒæ–‡ä»¶çš„ä¼ä¸šæ¥è¯´ï¼Œè¿™èƒ½å¤§å¹…æå‡å·¥ä½œæ•ˆç‡ã€‚

### æ™ºèƒ½å¯¹è¯ç³»ç»Ÿçš„è®°å¿†ä¼˜åŒ–

DeepSeekå›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„åº”ç”¨æ€è·¯ï¼šç”¨è¿™ä¸ªæ¨¡å‹æ¥å‹ç¼©èŠå¤©æœºå™¨äººçš„å¯¹è¯å†å²ã€‚å°±åƒäººçš„è®°å¿†ä¼šéšç€æ—¶é—´æ·¡åŒ–ä¸€æ ·ï¼Œè¾ƒæ—©çš„å¯¹è¯å¯ä»¥ç”¨è¾ƒä½çš„åˆ†è¾¨ç‡å­˜å‚¨ï¼Œè®©AIèƒ½å¤Ÿåœ¨æœ‰é™çš„ç®—åŠ›ä¸‹å¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ã€‚

### æ•°æ®é›†æ„å»º

ç°ä»£AIæ¨¡å‹çš„è®­ç»ƒéœ€è¦æµ·é‡çš„æ–‡æœ¬æ•°æ®ã€‚DeepSeek-OCRå¯ä»¥ä»å„ç§æ–‡æ¡£ä¸­å¿«é€Ÿæå–æ–‡æœ¬ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜é«˜æ•ˆæ„å»ºè®­ç»ƒæ•°æ®é›†ã€‚

### å¤æ‚æ–‡æ¡£è§£æ

ä¸ä»…ä»…æ˜¯è¯†åˆ«æ–‡å­—ï¼ŒDeepSeek-OCRè¿˜èƒ½ç†è§£å’Œè§£æå›¾è¡¨ã€åŒ–å­¦åˆ†å­å¼ã€å‡ ä½•å›¾å½¢ç­‰å¤æ‚å†…å®¹ã€‚å®ƒå¯ä»¥å°†é‡‘èå›¾è¡¨è½¬æ¢æˆç»“æ„åŒ–æ•°æ®ï¼Œè‡ªåŠ¨ç”ŸæˆMarkdownè¡¨æ ¼å’Œå›¾å½¢æè¿°ã€‚

## äº”ã€å¦‚ä½•ä¸Šæ‰‹ä½¿ç”¨ï¼Ÿ

DeepSeekä¸€å¦‚æ—¢å¾€åœ°é€‰æ‹©äº†å¼€æºç­–ç•¥ï¼Œä»»ä½•äººéƒ½å¯ä»¥å…è´¹ä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚

### ç¯å¢ƒè¦æ±‚

- Python 3.12.9
- CUDA 11.8
- PyTorch 2.6.0
- Transformers 4.46.3

### å¿«é€Ÿå¼€å§‹

æ¨¡å‹å·²ç»æ‰˜ç®¡åœ¨Hugging Faceå¹³å°ä¸Šï¼Œä½ å¯ä»¥ç”¨å‡ è¡Œä»£ç å°±å¼€å§‹ä½¿ç”¨ï¼š

```python
from transformers import AutoModel, AutoTokenizer
import torch

model_name = 'deepseek-ai/DeepSeek-OCR'
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
model = model.eval().cuda().to(torch.bfloat16)

# å¯¹äºæ–‡æ¡£ï¼Œä½¿ç”¨è¿™ä¸ªæç¤ºè¯
prompt = "<image>\n<|grounding|>Convert the document to markdown."
# å¯¹äºä¸€èˆ¬å›¾ç‰‡
# prompt = "<image>\n<|grounding|>OCR this image."

res = model.infer(tokenizer, prompt=prompt, image_file='your_image.jpg')

```

### å¤šç§æç¤ºè¯æ”¯æŒ

DeepSeek-OCRæ”¯æŒå¤šç§åœºæ™¯çš„æç¤ºè¯ï¼š

- æ–‡æ¡£è½¬Markdownï¼š`<image>\n<|grounding|>Convert the document to markdown.`
- é€šç”¨OCRï¼š`<image>\n<|grounding|>OCR this image.`
- æ— å¸ƒå±€æå–ï¼š`<image>\nFree OCR.`
- å›¾è¡¨è§£æï¼š`<image>\nParse the figure.`
- å›¾åƒæè¿°ï¼š`<image>\nDescribe this image in detail.`
- æ–‡æœ¬å®šä½ï¼š`<image>\nLocate <|ref|>ç‰¹å®šæ–‡å­—<|/ref|> in the image.`

## å…­ã€èƒŒåçš„æ•…äº‹

DeepSeek-OCRçš„å‘å¸ƒå…¶å®ä¹Ÿåæ˜ äº†å½“å‰AIè¡Œä¸šçš„ä¸€äº›è¶‹åŠ¿ã€‚

ä»Šå¹´ï¼ŒDeepSeekçš„æ——èˆ°æ¨¡å‹R2å› ä¸ºç¡¬ä»¶æŒ‘æˆ˜ï¼ˆä¸»è¦ä¸ä¸­ç¾ç§‘æŠ€ç«äº‰æœ‰å…³ï¼‰è€Œè¢«æ— é™æœŸæ¨è¿Ÿã€‚ä½†è¿™å¹¶æ²¡æœ‰é˜»æ­¢DeepSeekç»§ç»­åˆ›æ–°çš„æ­¥ä¼ã€‚å‘å¸ƒDeepSeek-OCRï¼ŒæŸç§ç¨‹åº¦ä¸Šä¹Ÿæ˜¯ä¸€ç§æˆ˜ç•¥è°ƒæ•´â€”â€”é€šè¿‡ä¸“æ³¨äºé«˜æ•ˆã€å®ç”¨çš„å¼€æºå·¥å…·ï¼Œç»§ç»­ä¿æŒæŠ€æœ¯é¢†å…ˆå’Œç¤¾åŒºå½±å“åŠ›ã€‚

å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒDeepSeekä¸€è´¯ç§‰æŒçš„ç†å¿µå°±æ˜¯**æé«˜AIæ•ˆç‡ï¼Œé™ä½ä½¿ç”¨æˆæœ¬**ã€‚ä»å»å¹´åº•å‘å¸ƒçš„V3æ¨¡å‹ï¼Œåˆ°ä»Šå¹´2æœˆçš„R1æ¨¡å‹ï¼Œå†åˆ°ç°åœ¨çš„OCRæ¨¡å‹ï¼Œè¿™æ¡ä¸»çº¿ä¸€ç›´æ²¡æœ‰æ”¹å˜ã€‚åœ¨å…¨çƒAIç«èµ›æ„ˆæ¼”æ„ˆçƒˆçš„èƒŒæ™¯ä¸‹ï¼Œè¿™ç§åŠ¡å®çš„è·¯çº¿æ˜¾å¾—å°¤ä¸ºå¯è´µã€‚

## ä¸ƒã€æœªæ¥å±•æœ›

DeepSeek-OCRçš„å‡ºç°ï¼Œä¸ºAIå¤„ç†é•¿æ–‡æœ¬æä¾›äº†ä¸€æ¡å…¨æ–°çš„æ€è·¯ã€‚é€šè¿‡"è§†è§‰å‹ç¼©"è¿™ç§å·§å¦™çš„æ–¹æ³•ï¼Œå®ƒåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶å¤§å¹…é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

ä½†è¿™åªæ˜¯å¼€å§‹ã€‚éšç€æ¨¡å‹çš„è¿›ä¸€æ­¥ä¼˜åŒ–å’Œåº”ç”¨åœºæ™¯çš„æ‹“å±•ï¼Œæˆ‘ä»¬æœ‰ç†ç”±ç›¸ä¿¡ï¼Œæœªæ¥AIå¤„ç†æ–‡æ¡£ä¼šå˜å¾—æ›´åŠ é«˜æ•ˆã€æ™ºèƒ½ã€‚æˆ–è®¸æœ‰ä¸€å¤©ï¼Œå¤„ç†å‡ ç™¾é¡µçš„åˆåŒæ–‡ä»¶å°±åƒç¿»é˜…å‡ é¡µPPTä¸€æ ·è½»æ¾ã€‚

å¯¹äºå¼€å‘è€…å’Œç ”ç©¶è€…æ¥è¯´ï¼Œç°åœ¨å°±æ˜¯ä¸Šæ‰‹ä½“éªŒçš„æœ€å¥½æ—¶æœºã€‚æ¨¡å‹å·²ç»åœ¨GitHubå’ŒHugging Faceä¸Šå¼€æºï¼Œé…å¥—çš„æŠ€æœ¯æ–‡æ¡£ä¹Ÿå¾ˆå®Œå–„ã€‚æ— è®ºä½ æ˜¯æƒ³ç”¨å®ƒæ¥ä¼˜åŒ–è‡ªå·±çš„äº§å“ï¼Œè¿˜æ˜¯æƒ³æ·±å…¥ç ”ç©¶å…¶æŠ€æœ¯ç»†èŠ‚ï¼Œéƒ½èƒ½æ‰¾åˆ°åˆé€‚çš„åˆ‡å…¥ç‚¹ã€‚

AIæŠ€æœ¯çš„å‘å±•æ—¥æ–°æœˆå¼‚ï¼Œä½†çœŸæ­£èƒ½è½åœ°ã€èƒ½è§£å†³å®é™…é—®é¢˜çš„å·¥å…·æ‰æ˜¯æœ€æœ‰ä»·å€¼çš„ã€‚DeepSeek-OCRæ˜¾ç„¶å±äºåè€…ã€‚å¦‚æœä½ çš„å·¥ä½œæ¶‰åŠå¤§é‡æ–‡æ¡£å¤„ç†ï¼Œä¸å¦¨è¯•è¯•è¿™ä¸ªæ–°å·¥å…·ï¼Œè¯´ä¸å®šä¼šç»™ä½ å¸¦æ¥æ„å¤–çš„æƒŠå–œã€‚

---

**ç›¸å…³é“¾æ¥ï¼š**

- GitHubä»“åº“ï¼šhttps://github.com/deepseek-ai/DeepSeek-OCR
- Hugging Faceæ¨¡å‹ï¼šhttps://huggingface.co/deepseek-ai/DeepSeek-OCR
- æŠ€æœ¯è®ºæ–‡ï¼šå¯åœ¨GitHubä»“åº“ä¸­æ‰¾åˆ°

**å…³æ³¨æˆ‘ä»¬ï¼Œè·å–æ›´å¤šAIå‰æ²¿èµ„è®¯ï¼**

### ğŸ”¥å®Œæ•´å®‰è£…å‘½ä»¤å¦‚ä¸‹

```markdown
# å…‹éš†ä»“åº“
git clone https://github.com/deepseek-ai/DeepSeek-OCR.git

cd DeepSeek-OCR

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n deepseek-ocr python=3.12 -y

conda activate deepseek-ocr

pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118

pip install -r requirements.txt

pip install gradio

nvcc --version
which nvcc
find /usr -name nvcc 2>/dev/null
export CUDA_HOME=/usr/local/cuda-12.3
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
nvcc --version
pip install flash-attn==2.7.3 --no-build-isolation

# æ–°å»ºgradio_demo.pyæ–‡ä»¶å¹¶æ”¾å…¥ä¸‹é¢çš„Demoä»£ç 
nano gradio_demo.py

# è¿è¡Œè„šæœ¬
python gradio_demo.py

```

### ğŸ”¥Demoä»£ç 

```python
# ç¡®ä¿å®‰è£…äº†gradioï¼Œç”¨pip install gradio

import gradio as gr
from transformers import AutoModel, AutoTokenizer
import torch
import os
from PIL import Image
import tempfile
import shutil

# Global variables for model and tokenizer
model = None
tokenizer = None

def load_model():
    """Load the DeepSeek-OCR model and tokenizer"""
    global model, tokenizer
    
    if model is None:
        print("Loading DeepSeek-OCR model...")
        model_name = 'deepseek-ai/DeepSeek-OCR'
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        model = AutoModel.from_pretrained(
            model_name, 
            _attn_implementation='flash_attention_2',
            trust_remote_code=True, 
            use_safetensors=True
        )
        model = model.eval().cuda().to(torch.bfloat16)
        print("Model loaded successfully!")
    
    return model, tokenizer

def process_image(image, prompt_type, custom_prompt, model_size):
    """Process image with OCR"""
    try:
        # Load model if not already loaded
        model, tokenizer = load_model()
        
        # Create temporary directory for output
        temp_dir = tempfile.mkdtemp()
        
        # Save uploaded image temporarily
        temp_image_path = os.path.join(temp_dir, "input_image.jpg")
        if isinstance(image, str):
            shutil.copy(image, temp_image_path)
        else:
            image.save(temp_image_path)
        
        # Set prompt based on selection
        if prompt_type == "Free OCR":
            prompt = "<image>\nFree OCR. "
        elif prompt_type == "Markdown Conversion":
            prompt = "<image>\n<|grounding|>Convert the document to markdown. "
        elif prompt_type == "Custom":
            prompt = f"<image>\n{custom_prompt}"
        else:
            prompt = "<image>\nFree OCR. "
        
        # Set model size parameters
        size_configs = {
            "Tiny": {"base_size": 512, "image_size": 512, "crop_mode": False},
            "Small": {"base_size": 640, "image_size": 640, "crop_mode": False},
            "Base": {"base_size": 1024, "image_size": 1024, "crop_mode": False},
            "Large": {"base_size": 1280, "image_size": 1280, "crop_mode": False},
            "Gundam (Recommended)": {"base_size": 1024, "image_size": 640, "crop_mode": True}
        }
        
        config = size_configs[model_size]
        
        # Capture stdout to get the OCR results
        import sys
        from io import StringIO
        
        # Redirect stdout to capture print statements
        old_stdout = sys.stdout
        sys.stdout = captured_output = StringIO()
        
        try:
            # Run inference
            result = model.infer(
                tokenizer,
                prompt=prompt,
                image_file=temp_image_path,
                output_path=temp_dir,
                base_size=config["base_size"],
                image_size=config["image_size"],
                crop_mode=config["crop_mode"],
                save_results=True,
                test_compress=False
            )
        finally:
            # Restore stdout
            sys.stdout = old_stdout
        
        # Get captured output
        captured_text = captured_output.getvalue()
        
        # Try to read from saved text file if it exists
        ocr_text = ""
        for filename in os.listdir(temp_dir):
            if filename.endswith('.txt'):
                with open(os.path.join(temp_dir, filename), 'r', encoding='utf-8') as f:
                    ocr_text += f.read() + "\n"
        
        # If we found text in files, use that; otherwise use captured output
        if ocr_text.strip():
            final_result = ocr_text.strip()
        elif captured_text.strip():
            # Parse the captured output to extract actual OCR text
            # Remove detection boxes and reference tags
            lines = captured_text.split('\n')
            clean_lines = []
            for line in lines:
                # Skip lines with detection boxes and reference tags
                if '<|ref|>' in line or '<|det|>' in line or '<|/ref|>' in line or '<|/det|>' in line:
                    # Extract text between tags
                    import re
                    # Pattern to match text between </ref|> and <|det|>
                    text_match = re.search(r'<\|/ref\|>(.*?)<\|det\|>', line)
                    if text_match:
                        clean_lines.append(text_match.group(1).strip())
                elif line.startswith('=====') or 'BASE:' in line or 'PATCHES:' in line or line.startswith('image:') or line.startswith('other:'):
                    continue
                elif line.strip():
                    clean_lines.append(line.strip())
            
            final_result = '\n'.join(clean_lines)
        elif isinstance(result, str):
            final_result = result
        else:
            final_result = str(result) if result else "No text detected in image."
        
        # Clean up temporary directory
        shutil.rmtree(temp_dir)
        
        return final_result if final_result.strip() else "No text detected in image."
    
    except Exception as e:
        import traceback
        return f"Error: {str(e)}\n\nTraceback:\n{traceback.format_exc()}\n\nPlease make sure you have a CUDA-enabled GPU and all dependencies installed."

def create_demo():
    """Create Gradio interface"""
    
    with gr.Blocks(title="DeepSeek-OCR Demo", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # ğŸ” DeepSeek-OCR Demo
            
            Upload an image containing text, documents, charts, or tables to extract text using DeepSeek-OCR.
            
            **Features:**
            - Free OCR for general text extraction
            - Markdown conversion for document structure
            - Multiple model sizes for different accuracy/speed tradeoffs
            - Support for various document types
            """
        )
        
        with gr.Row():
            with gr.Column(scale=1):
                # Input section
                gr.Markdown("### ğŸ“¤ Input")
                image_input = gr.Image(
                    label="Upload Image",
                    type="pil",
                    sources=["upload", "clipboard"]
                )
                
                gr.Markdown("### âš™ï¸ Settings")
                
                prompt_type = gr.Radio(
                    choices=["Free OCR", "Markdown Conversion", "Custom"],
                    value="Markdown Conversion",
                    label="Prompt Type",
                    info="Choose the type of OCR processing"
                )
                
                custom_prompt = gr.Textbox(
                    label="Custom Prompt (if selected)",
                    placeholder="Enter your custom prompt here...",
                    lines=2,
                    visible=False
                )
                
                model_size = gr.Radio(
                    choices=[
                        "Tiny",
                        "Small", 
                        "Base",
                        "Large",
                        "Gundam (Recommended)"
                    ],
                    value="Gundam (Recommended)",
                    label="Model Size",
                    info="Larger models are more accurate but slower"
                )
                
                process_btn = gr.Button("ğŸš€ Process Image", variant="primary", size="lg")
                
                gr.Markdown(
                    """
                    ### ğŸ’¡ Tips
                    - **Gundam** mode works best for most documents
                    - Use **Markdown Conversion** for structured documents
                    - **Free OCR** for simple text extraction
                    - Higher resolution images give better results
                    """
                )
            
            with gr.Column(scale=1):
                # Output section
                gr.Markdown("### ğŸ“„ Results")
                output_text = gr.Textbox(
                    label="Extracted Text",
                    lines=20,
                    max_lines=30,
                    show_copy_button=True
                )
                
                gr.Markdown(
                    """
                    ### ğŸ“¥ Export
                    You can copy the results using the copy button above.
                    """
                )
        
        # Show/hide custom prompt based on selection
        def update_prompt_visibility(choice):
            return gr.update(visible=(choice == "Custom"))
        
        prompt_type.change(
            fn=update_prompt_visibility,
            inputs=[prompt_type],
            outputs=[custom_prompt]
        )
        
        # Process button click
        process_btn.click(
            fn=process_image,
            inputs=[image_input, prompt_type, custom_prompt, model_size],
            outputs=[output_text]
        )
        
        # Add examples
        gr.Markdown("### ğŸ“š Example Images")
        gr.Examples(
            examples=[
                ["example_document.jpg", "Markdown Conversion", "", "Gundam (Recommended)"],
                ["example_receipt.jpg", "Free OCR", "", "Small"],
            ],
            inputs=[image_input, prompt_type, custom_prompt, model_size],
            outputs=[output_text],
            fn=process_image,
            cache_examples=False,
        )
        
        gr.Markdown(
            """
            ---
            ### â„¹ï¸ About
            
            This demo uses [DeepSeek-OCR](https://huggingface.co/deepseek-ai/DeepSeek-OCR) for optical character recognition.
            
            **Model Sizes Explained:**
            - **Tiny**: Fastest, lowest accuracy (512x512)
            - **Small**: Fast, good for simple documents (640x640)
            - **Base**: Balanced performance (1024x1024)
            - **Large**: High accuracy, slower (1280x1280)
            - **Gundam**: Best balance with crop mode (1024x640 with cropping)
            
            **Note:** First run will download the model (~several GB). Requires CUDA-enabled GPU.
            """
        )
    
    return demo

if __name__ == "__main__":
    # Set CUDA device
    os.environ["CUDA_VISIBLE_DEVICES"] = '0'
    
    # Create and launch demo
    demo = create_demo()
    demo.launch(
        server_name="0.0.0.0",  # Allow external access
        server_port=7860,
        share=False,  # Set to True to create a public link
        debug=True
    )
```